{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperparameter_tuning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfogarty/machine-learning-intro-workshop/blob/master/notebooks/hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LDujGFixTPuK"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "From [Hyperparameter Tuning](https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624) by [Tara Boyle](https://taraboyle.me/data-science/) in [towardsdatascience.com](https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624)\n",
        "\n",
        "Updated by [John Fogarty](https://github.com/jfogarty) for Python 3.6 and [Base2 MLI](https://github.com/base2solutions/mli) and [colab](https://colab.research.google.com) standalone evaluation.\n",
        "\n",
        "### NOTE! This is a Colab ONLY notebook which loads data via Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EnZkt8ymUQIm"
      },
      "source": [
        "[Kaggle’s](https://www.kaggle.com/c/dont-overfit-ii) Don’t Overfit II competition presents an interesting problem. We have 20,000 rows of continuous variables, with only 250 of them belonging to the training set.\n",
        "The challenge is not to overfit.\n",
        "\n",
        "With such a small dataset — and even smaller training set, this can be a difficult task!\n",
        "In this article, we’ll explore hyperparameter optimization as a means of preventing overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xPxUadYaUy4I"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "[Wikipedia states](https://en.wikipedia.org/wiki/Hyperparameter_optimization) that “hyperparameter tuning is choosing a set of optimal hyperparameters for a learning algorithm”. So what is a [hyperparameter](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning))?\n",
        "\n",
        "> *A hyperparameter is a parameter whose value is set before the learning process begins.\n",
        "Some examples of hyperparameters include penalty in logistic regression and loss in stochastic gradient descent.*\n",
        "\n",
        "In [sklearn](https://scikit-learn.org/stable/modules/grid_search.html#grid-search), hyperparameters are passed in as arguments to the constructor of the model classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MDUPmbyrVTPT"
      },
      "source": [
        "## Tuning Strategies\n",
        "\n",
        "We will explore two different methods for optimizing hyperparameters:\n",
        "\n",
        "- **Grid Search**\n",
        "\n",
        "- **Random Search**\n",
        "\n",
        "We’ll begin by preparing the data and trying several different models with their default hyperparameters. From these we’ll select the top two performing methods for hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VwLdIxGNVp77",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(27)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wnPtpiAFVTcR"
      },
      "source": [
        "## Download Datasets from Google Drive\n",
        "\n",
        "### NOTE! This will ask you to login to your Google Account \n",
        "\n",
        "You can use any Google account, this is asking you whether you are willing to use your account to access the remote Google Drive and and to create a local drive on this virtual machine instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hDS4esRhkAZ1",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ND-b8sKfi4hm",
        "colab": {}
      },
      "source": [
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "def googledrive_load(file_id, tofile):\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    with open(tofile, 'wb') as fout:\n",
        "      downloader = MediaIoBaseDownload(fout, request)\n",
        "      done = False\n",
        "      while done is False:\n",
        "          _, done = downloader.next_chunk()\n",
        "    print(f'- Downloaded {tofile}.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i7ChsJQXg2i8",
        "outputId": "08f7b56f-0ff1-4939-832f-7c86efed3b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "FILE_ID_TRAIN = '1ldAeovBgrWo3oiXlQaeuSX97xLb_r962'\n",
        "TMPDATA = './tmpData'\n",
        "if not os.path.exists(TMPDATA) : os.makedirs(TMPDATA)\n",
        "trainFile = os.path.join(TMPDATA, 'train.csv')\n",
        "googledrive_load(FILE_ID_TRAIN, trainFile)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Downloaded ./tmpData/train.csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KX9sYfKLmAoc",
        "outputId": "39e1337a-7388-4c36-991c-c79a14b2d014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "FILE_ID_TEST = '1YTe-lgXh8lUtz56Ftw10u-7ucoI8fW1o'\n",
        "testFile = os.path.join(TMPDATA, 'test.csv')\n",
        "googledrive_load(FILE_ID_TEST, testFile)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Downloaded ./tmpData/test.csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ffup5CEn8iA",
        "outputId": "ff9e22a9-33ee-440a-a098-0e2bc8c3f950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "FILE_ID_SUBMISSION = '1dIxeOb6U_96Ky7Hg_cWbZeTqq7gDkybH'\n",
        "submissionFile = os.path.join(TMPDATA, 'submission.csv')\n",
        "googledrive_load(FILE_ID_SUBMISSION, submissionFile)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Downloaded ./tmpData/submission.csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bj_zdQq7XaXj",
        "colab": {}
      },
      "source": [
        "# setting up default plotting parameters\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20.0, 7.0]\n",
        "plt.rcParams.update({'font.size': 22,})\n",
        "\n",
        "sns.set_palette('viridis')\n",
        "sns.set_style('white')\n",
        "sns.set_context('talk', font_scale=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IfQhU40rWE9a",
        "outputId": "c3558c68-76ab-4033-de7d-312e932a92ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "\n",
        "train = pd.read_csv(trainFile)\n",
        "test = pd.read_csv(testFile)\n",
        "\n",
        "print('Train Shape: ', train.shape)\n",
        "print('Test Shape: ', test.shape)\n",
        "\n",
        "train.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Shape:  (250, 302)\n",
            "Test Shape:  (19750, 301)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.098</td>\n",
              "      <td>2.165</td>\n",
              "      <td>0.681</td>\n",
              "      <td>-0.614</td>\n",
              "      <td>1.309</td>\n",
              "      <td>-0.455</td>\n",
              "      <td>-0.236</td>\n",
              "      <td>0.276</td>\n",
              "      <td>-2.246</td>\n",
              "      <td>1.825</td>\n",
              "      <td>-0.912</td>\n",
              "      <td>-0.107</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.826</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.177</td>\n",
              "      <td>-0.673</td>\n",
              "      <td>-0.503</td>\n",
              "      <td>1.864</td>\n",
              "      <td>0.410</td>\n",
              "      <td>-1.927</td>\n",
              "      <td>0.102</td>\n",
              "      <td>-0.931</td>\n",
              "      <td>1.763</td>\n",
              "      <td>1.449</td>\n",
              "      <td>-1.097</td>\n",
              "      <td>-0.686</td>\n",
              "      <td>-0.250</td>\n",
              "      <td>-1.859</td>\n",
              "      <td>1.125</td>\n",
              "      <td>1.009</td>\n",
              "      <td>-2.296</td>\n",
              "      <td>0.385</td>\n",
              "      <td>-0.876</td>\n",
              "      <td>1.528</td>\n",
              "      <td>-0.144</td>\n",
              "      <td>-1.078</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.681</td>\n",
              "      <td>1.250</td>\n",
              "      <td>-0.565</td>\n",
              "      <td>-1.318</td>\n",
              "      <td>-0.923</td>\n",
              "      <td>0.075</td>\n",
              "      <td>-0.704</td>\n",
              "      <td>2.457</td>\n",
              "      <td>0.771</td>\n",
              "      <td>-0.460</td>\n",
              "      <td>0.569</td>\n",
              "      <td>-1.320</td>\n",
              "      <td>-1.516</td>\n",
              "      <td>-2.145</td>\n",
              "      <td>-1.120</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.820</td>\n",
              "      <td>-1.049</td>\n",
              "      <td>-1.125</td>\n",
              "      <td>0.484</td>\n",
              "      <td>0.617</td>\n",
              "      <td>1.253</td>\n",
              "      <td>1.248</td>\n",
              "      <td>0.504</td>\n",
              "      <td>-0.802</td>\n",
              "      <td>-0.896</td>\n",
              "      <td>-1.793</td>\n",
              "      <td>-0.284</td>\n",
              "      <td>-0.601</td>\n",
              "      <td>0.569</td>\n",
              "      <td>0.867</td>\n",
              "      <td>1.347</td>\n",
              "      <td>0.504</td>\n",
              "      <td>-0.649</td>\n",
              "      <td>0.672</td>\n",
              "      <td>-2.097</td>\n",
              "      <td>1.051</td>\n",
              "      <td>-0.414</td>\n",
              "      <td>1.038</td>\n",
              "      <td>-1.065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.081</td>\n",
              "      <td>-0.973</td>\n",
              "      <td>-0.383</td>\n",
              "      <td>0.326</td>\n",
              "      <td>-0.428</td>\n",
              "      <td>0.317</td>\n",
              "      <td>1.172</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.291</td>\n",
              "      <td>2.907</td>\n",
              "      <td>1.085</td>\n",
              "      <td>2.144</td>\n",
              "      <td>1.540</td>\n",
              "      <td>0.584</td>\n",
              "      <td>1.133</td>\n",
              "      <td>1.098</td>\n",
              "      <td>-0.237</td>\n",
              "      <td>-0.498</td>\n",
              "      <td>0.283</td>\n",
              "      <td>-1.100</td>\n",
              "      <td>-0.417</td>\n",
              "      <td>1.382</td>\n",
              "      <td>-0.515</td>\n",
              "      <td>-1.519</td>\n",
              "      <td>0.619</td>\n",
              "      <td>-0.128</td>\n",
              "      <td>0.866</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>1.238</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>0.269</td>\n",
              "      <td>-0.390</td>\n",
              "      <td>-2.721</td>\n",
              "      <td>1.659</td>\n",
              "      <td>0.106</td>\n",
              "      <td>-0.121</td>\n",
              "      <td>1.719</td>\n",
              "      <td>...</td>\n",
              "      <td>0.971</td>\n",
              "      <td>-1.489</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.917</td>\n",
              "      <td>-0.094</td>\n",
              "      <td>-1.407</td>\n",
              "      <td>0.887</td>\n",
              "      <td>-0.104</td>\n",
              "      <td>-0.583</td>\n",
              "      <td>1.267</td>\n",
              "      <td>-1.667</td>\n",
              "      <td>-2.771</td>\n",
              "      <td>-0.516</td>\n",
              "      <td>1.312</td>\n",
              "      <td>0.491</td>\n",
              "      <td>0.932</td>\n",
              "      <td>2.064</td>\n",
              "      <td>0.422</td>\n",
              "      <td>1.215</td>\n",
              "      <td>2.012</td>\n",
              "      <td>0.043</td>\n",
              "      <td>-0.307</td>\n",
              "      <td>-0.059</td>\n",
              "      <td>1.121</td>\n",
              "      <td>1.333</td>\n",
              "      <td>0.211</td>\n",
              "      <td>1.753</td>\n",
              "      <td>0.053</td>\n",
              "      <td>1.274</td>\n",
              "      <td>-0.612</td>\n",
              "      <td>-0.165</td>\n",
              "      <td>-1.695</td>\n",
              "      <td>-1.257</td>\n",
              "      <td>1.359</td>\n",
              "      <td>-0.808</td>\n",
              "      <td>-1.624</td>\n",
              "      <td>-0.458</td>\n",
              "      <td>-1.099</td>\n",
              "      <td>-0.936</td>\n",
              "      <td>0.973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.523</td>\n",
              "      <td>-0.089</td>\n",
              "      <td>-0.348</td>\n",
              "      <td>0.148</td>\n",
              "      <td>-0.022</td>\n",
              "      <td>0.404</td>\n",
              "      <td>-0.023</td>\n",
              "      <td>-0.172</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.459</td>\n",
              "      <td>0.478</td>\n",
              "      <td>-0.425</td>\n",
              "      <td>0.352</td>\n",
              "      <td>1.095</td>\n",
              "      <td>0.300</td>\n",
              "      <td>-1.044</td>\n",
              "      <td>0.270</td>\n",
              "      <td>-1.038</td>\n",
              "      <td>0.144</td>\n",
              "      <td>-1.658</td>\n",
              "      <td>-0.946</td>\n",
              "      <td>0.633</td>\n",
              "      <td>-0.772</td>\n",
              "      <td>1.786</td>\n",
              "      <td>0.136</td>\n",
              "      <td>-0.103</td>\n",
              "      <td>-1.223</td>\n",
              "      <td>2.273</td>\n",
              "      <td>0.055</td>\n",
              "      <td>-2.032</td>\n",
              "      <td>-0.452</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.924</td>\n",
              "      <td>-0.692</td>\n",
              "      <td>-0.067</td>\n",
              "      <td>-0.917</td>\n",
              "      <td>1.896</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>1.074</td>\n",
              "      <td>-0.748</td>\n",
              "      <td>1.086</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>-0.931</td>\n",
              "      <td>0.432</td>\n",
              "      <td>1.345</td>\n",
              "      <td>-0.491</td>\n",
              "      <td>-1.602</td>\n",
              "      <td>-0.727</td>\n",
              "      <td>0.346</td>\n",
              "      <td>0.780</td>\n",
              "      <td>-0.527</td>\n",
              "      <td>-1.122</td>\n",
              "      <td>-0.208</td>\n",
              "      <td>-0.730</td>\n",
              "      <td>-0.302</td>\n",
              "      <td>2.535</td>\n",
              "      <td>-1.045</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.020</td>\n",
              "      <td>1.373</td>\n",
              "      <td>0.456</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>1.381</td>\n",
              "      <td>1.843</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.263</td>\n",
              "      <td>-1.222</td>\n",
              "      <td>0.726</td>\n",
              "      <td>1.444</td>\n",
              "      <td>-1.165</td>\n",
              "      <td>-1.544</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.800</td>\n",
              "      <td>-1.211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.067</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>0.392</td>\n",
              "      <td>-1.637</td>\n",
              "      <td>-0.446</td>\n",
              "      <td>-0.725</td>\n",
              "      <td>-1.035</td>\n",
              "      <td>0.834</td>\n",
              "      <td>0.503</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.335</td>\n",
              "      <td>-1.148</td>\n",
              "      <td>0.067</td>\n",
              "      <td>-1.010</td>\n",
              "      <td>1.048</td>\n",
              "      <td>-1.442</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.836</td>\n",
              "      <td>-0.326</td>\n",
              "      <td>0.716</td>\n",
              "      <td>-0.764</td>\n",
              "      <td>0.248</td>\n",
              "      <td>-1.308</td>\n",
              "      <td>2.127</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.296</td>\n",
              "      <td>-0.808</td>\n",
              "      <td>1.854</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.999</td>\n",
              "      <td>-1.171</td>\n",
              "      <td>2.798</td>\n",
              "      <td>0.394</td>\n",
              "      <td>-1.048</td>\n",
              "      <td>1.078</td>\n",
              "      <td>0.401</td>\n",
              "      <td>-0.486</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.083</td>\n",
              "      <td>-0.831</td>\n",
              "      <td>1.251</td>\n",
              "      <td>-0.206</td>\n",
              "      <td>-0.933</td>\n",
              "      <td>-1.215</td>\n",
              "      <td>0.281</td>\n",
              "      <td>0.512</td>\n",
              "      <td>-0.424</td>\n",
              "      <td>0.769</td>\n",
              "      <td>0.223</td>\n",
              "      <td>-0.710</td>\n",
              "      <td>2.725</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.845</td>\n",
              "      <td>-1.226</td>\n",
              "      <td>1.527</td>\n",
              "      <td>-1.701</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.150</td>\n",
              "      <td>1.864</td>\n",
              "      <td>0.322</td>\n",
              "      <td>-0.214</td>\n",
              "      <td>1.282</td>\n",
              "      <td>0.408</td>\n",
              "      <td>-0.910</td>\n",
              "      <td>1.020</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>-1.574</td>\n",
              "      <td>-1.618</td>\n",
              "      <td>-0.404</td>\n",
              "      <td>0.640</td>\n",
              "      <td>-0.595</td>\n",
              "      <td>-0.966</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.467</td>\n",
              "      <td>-0.562</td>\n",
              "      <td>-0.254</td>\n",
              "      <td>-0.533</td>\n",
              "      <td>0.238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.347</td>\n",
              "      <td>-0.831</td>\n",
              "      <td>0.511</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>1.225</td>\n",
              "      <td>1.594</td>\n",
              "      <td>0.585</td>\n",
              "      <td>1.509</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>2.198</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.453</td>\n",
              "      <td>0.494</td>\n",
              "      <td>1.478</td>\n",
              "      <td>-1.412</td>\n",
              "      <td>0.270</td>\n",
              "      <td>-1.312</td>\n",
              "      <td>-0.322</td>\n",
              "      <td>-0.688</td>\n",
              "      <td>-0.198</td>\n",
              "      <td>-0.285</td>\n",
              "      <td>1.042</td>\n",
              "      <td>-0.315</td>\n",
              "      <td>-0.478</td>\n",
              "      <td>0.024</td>\n",
              "      <td>-0.190</td>\n",
              "      <td>1.656</td>\n",
              "      <td>-0.469</td>\n",
              "      <td>-1.437</td>\n",
              "      <td>-0.581</td>\n",
              "      <td>-0.308</td>\n",
              "      <td>-0.837</td>\n",
              "      <td>-1.739</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.336</td>\n",
              "      <td>-1.102</td>\n",
              "      <td>2.371</td>\n",
              "      <td>0.554</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.050</td>\n",
              "      <td>-0.347</td>\n",
              "      <td>0.904</td>\n",
              "      <td>-1.324</td>\n",
              "      <td>-0.849</td>\n",
              "      <td>3.432</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.416</td>\n",
              "      <td>0.174</td>\n",
              "      <td>-1.517</td>\n",
              "      <td>-0.337</td>\n",
              "      <td>0.055</td>\n",
              "      <td>-0.464</td>\n",
              "      <td>0.014</td>\n",
              "      <td>-1.073</td>\n",
              "      <td>0.325</td>\n",
              "      <td>-0.523</td>\n",
              "      <td>-0.692</td>\n",
              "      <td>0.190</td>\n",
              "      <td>-0.883</td>\n",
              "      <td>-1.830</td>\n",
              "      <td>1.408</td>\n",
              "      <td>2.319</td>\n",
              "      <td>1.704</td>\n",
              "      <td>-0.723</td>\n",
              "      <td>1.014</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.096</td>\n",
              "      <td>-0.775</td>\n",
              "      <td>1.845</td>\n",
              "      <td>0.898</td>\n",
              "      <td>0.134</td>\n",
              "      <td>2.415</td>\n",
              "      <td>-0.996</td>\n",
              "      <td>-1.006</td>\n",
              "      <td>1.378</td>\n",
              "      <td>1.246</td>\n",
              "      <td>1.478</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.253</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 302 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  target      0      1      2  ...    295    296    297    298    299\n",
              "0   0     1.0 -0.098  2.165  0.681  ... -2.097  1.051 -0.414  1.038 -1.065\n",
              "1   1     0.0  1.081 -0.973 -0.383  ... -1.624 -0.458 -1.099 -0.936  0.973\n",
              "2   2     1.0 -0.523 -0.089 -0.348  ... -1.165 -1.544  0.004  0.800 -1.211\n",
              "3   3     1.0  0.067 -0.021  0.392  ...  0.467 -0.562 -0.254 -0.533  0.238\n",
              "4   4     1.0  2.347 -0.831  0.511  ...  1.378  1.246  1.478  0.428  0.253\n",
              "\n",
              "[5 rows x 302 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ShuYAMsyVTue"
      },
      "source": [
        "Prepare the data sets for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2-7iWdWsXmKz",
        "colab": {}
      },
      "source": [
        "# prepare for modeling\n",
        "X_train = train.drop(['id', 'target'], axis=1)\n",
        "y_train = train['target']\n",
        "\n",
        "X_test = test.drop(['id'], axis=1)\n",
        "\n",
        "# scaling data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6Oh_WzTYVTzW"
      },
      "source": [
        "## Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mIyKp8W9XocV",
        "colab": {}
      },
      "source": [
        "# define models\n",
        "ridge = linear_model.Ridge()\n",
        "lasso = linear_model.Lasso()\n",
        "elastic = linear_model.ElasticNet()\n",
        "lasso_lars = linear_model.LassoLars()\n",
        "bayesian_ridge = linear_model.BayesianRidge()\n",
        "logistic = linear_model.LogisticRegression(solver='liblinear')\n",
        "sgd = linear_model.SGDClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eIciWM6eXtJd"
      },
      "source": [
        "Here we select seven common traditional machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O8L8OslwUMru",
        "colab": {}
      },
      "source": [
        "models = [ridge, lasso, elastic, lasso_lars, bayesian_ridge, logistic, sgd]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0EzvLtYVXxI2"
      },
      "source": [
        "### Get Basic Metrics\n",
        "\n",
        "We then find the mean cross validation score and standard deviation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z_gWxEKeXx52",
        "colab": {}
      },
      "source": [
        "# function to get cross validation scores\n",
        "def get_cv_scores(model):\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "    print('CV Mean: ', np.mean(scores))\n",
        "    print('STD: ', np.std(scores))\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mApTAseoX2PR",
        "outputId": "02e28f6b-dcb8-4781-d1bd-b3394aab98d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "# loop through list of models\n",
        "for model in models:\n",
        "    print(model)\n",
        "    get_cv_scores(model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
            "      normalize=False, random_state=None, solver='auto', tol=0.001)\n",
            "CV Mean:  0.6759762475523124\n",
            "STD:  0.1170461756924883\n",
            "\n",
            "\n",
            "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
            "      normalize=False, positive=False, precompute=False, random_state=None,\n",
            "      selection='cyclic', tol=0.0001, warm_start=False)\n",
            "CV Mean:  0.5\n",
            "STD:  0.0\n",
            "\n",
            "\n",
            "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
            "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
            "CV Mean:  0.5\n",
            "STD:  0.0\n",
            "\n",
            "\n",
            "LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,\n",
            "          fit_path=True, max_iter=500, normalize=True, positive=False,\n",
            "          precompute='auto', verbose=False)\n",
            "CV Mean:  0.5\n",
            "STD:  0.0\n",
            "\n",
            "\n",
            "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
            "              fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
            "              normalize=False, tol=0.001, verbose=False)\n",
            "CV Mean:  0.688224616492365\n",
            "STD:  0.13183095412112777\n",
            "\n",
            "\n",
            "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "CV Mean:  0.7447916666666667\n",
            "STD:  0.053735373404660246\n",
            "\n",
            "\n",
            "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
            "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
            "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "CV Mean:  0.7322916666666667\n",
            "STD:  0.031488289024318425\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WwhV28zvX-LV"
      },
      "source": [
        "From this we can see our best performing models out of the box are logistic regression and stochastic gradient descent. Let's see if we can optimize these models with hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UV-jZ6MmX-S7"
      },
      "source": [
        "## Logistic Regression and Grid Search\n",
        "\n",
        "Grid search is a traditional way to perform hyperparameter optimization. It works by searching exhaustively through a specified subset of hyperparameters.\n",
        "Using sklearn’s **[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)**, we first define our grid of parameters to search over and then run the grid search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UuyDDYSdYEIl",
        "outputId": "09d4afd4-e642-4b8a-fd01-d01f68712a6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "penalty = ['l1', 'l2']\n",
        "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
        "solver = ['liblinear', 'saga']\n",
        "\n",
        "param_grid = dict(penalty=penalty,\n",
        "                  C=C,\n",
        "                  class_weight=class_weight,\n",
        "                  solver=solver)\n",
        "\n",
        "grid = GridSearchCV(estimator=logistic, param_grid=param_grid, scoring='roc_auc', verbose=1, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "print('Best Score: ', grid_result.best_score_)\n",
        "print('Best Params: ', grid_result.best_params_)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 128 candidates, totalling 384 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 268 tasks      | elapsed:   11.2s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Score:  0.7901274633123689\n",
            "Best Params:  {'C': 1, 'class_weight': {1: 0.6, 0: 0.4}, 'penalty': 'l1', 'solver': 'liblinear'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 384 out of 384 | elapsed:   17.3s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fws--KGDYIQ_"
      },
      "source": [
        "We improved our cross validation score from 0.744 to 0.789!\n",
        "\n",
        "The benefit of grid search is that it is guaranteed to find the optimal combination of parameters supplied. The drawback is that it can be very time consuming and computationally expensive.\n",
        "\n",
        "We can combat this with random search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_DR63HsrYMGf",
        "outputId": "2d48271b-b7a8-4a61-fb7a-e92b2fc5df06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "logistic = linear_model.LogisticRegression(C=1, class_weight={1:0.6, 0:0.4}, penalty='l1', solver='liblinear')\n",
        "get_cv_scores(logistic)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV Mean:  0.8166666666666667\n",
            "STD:  0.0398716352734963\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P3Eu_FAoYP20",
        "colab": {}
      },
      "source": [
        "predictions = logistic.fit(X_train, y_train).predict_proba(X_test)\n",
        "#### score 0.828 on public leaderboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AYIOgdTGYToR",
        "outputId": "693ba108-3d5b-4316-af1d-bc4ac7de9b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "submission = pd.read_csv(submissionFile)\n",
        "submission['target'] = predictions\n",
        "#submission.to_csv('submission.csv', index=False)\n",
        "submission.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>250</td>\n",
              "      <td>0.232936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>251</td>\n",
              "      <td>0.529461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>252</td>\n",
              "      <td>0.222784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>253</td>\n",
              "      <td>0.001155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>254</td>\n",
              "      <td>0.132049</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id    target\n",
              "0  250  0.232936\n",
              "1  251  0.529461\n",
              "2  252  0.222784\n",
              "3  253  0.001155\n",
              "4  254  0.132049"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WRxdH-gKYYv3"
      },
      "source": [
        "## Stochastic Gradient Descent and Random Search\n",
        "Random search is a random (obviously) search over specified parameter values.\n",
        "\n",
        "### Random Search\n",
        "\n",
        "Random search differs from grid search mainly in that it searches the specified subset of hyperparameters randomly instead of exhaustively. The major benefit being decreased processing time.\n",
        "\n",
        "There is a tradeoff to decreased processing time, however. We aren’t guaranteed to find the optimal combination of hyperparameters.\n",
        "\n",
        "Let’s give random search a try with sklearn’s **[RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)**. Very similar to grid search above, we define the hyperparameters to search over before running the search.\n",
        "An important additional parameter to specify here is n_iter. This specifies the number of combinations to randomly try.\n",
        "\n",
        "- Selecting too low of a number will decrease our chance of finding the best combination. \n",
        "\n",
        "- Selecting too large of a number will increase our processing time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RjqSs6ghYam4",
        "outputId": "e8eb983e-134a-4f44-976d-1856e76869a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "loss = ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']\n",
        "penalty = ['l1', 'l2', 'elasticnet']\n",
        "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']\n",
        "class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
        "eta0 = [1, 10, 100]\n",
        "\n",
        "param_distributions = dict(loss=loss,\n",
        "                           penalty=penalty,\n",
        "                           alpha=alpha,\n",
        "                           learning_rate=learning_rate,\n",
        "                           class_weight=class_weight,\n",
        "                           eta0=eta0)\n",
        "\n",
        "random = RandomizedSearchCV(estimator=sgd, param_distributions=param_distributions, scoring='roc_auc', verbose=1, n_jobs=-1, n_iter=1000)\n",
        "random_result = random.fit(X_train, y_train)\n",
        "\n",
        "print('Best Score: ', random_result.best_score_)\n",
        "print('Best Params: ', random_result.best_params_)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:   14.0s\n",
            "[Parallel(n_jobs=-1)]: Done 2303 tasks      | elapsed:   46.4s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Score:  0.8044072676450035\n",
            "Best Params:  {'penalty': 'l1', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 100, 'class_weight': {1: 0.7, 0: 0.3}, 'alpha': 0.1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed:  1.0min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TQbz9-1xYj4D",
        "outputId": "c4de9711-cbad-4c09-befe-a8e0b57471d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "sgd = linear_model.SGDClassifier(alpha=0.1,\n",
        "                                 class_weight={1:0.7, 0:0.3},\n",
        "                                 eta0=100,\n",
        "                                 learning_rate='optimal',\n",
        "                                 loss='log',\n",
        "                                 penalty='elasticnet')\n",
        "get_cv_scores(sgd)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV Mean:  0.7777777777777777\n",
            "STD:  0.040148859776561824\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vXwk625VYm0N",
        "colab": {}
      },
      "source": [
        "predictions = sgd.fit(X_train, y_train).predict_proba(X_test)\n",
        "#### score 0.790 on public leaderboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pIbpaVYkYnss"
      },
      "source": [
        "Here we improved the cross validation score from 0.733 to 0.780!\n",
        "Conclusion\n",
        "Here we explored two methods for hyperparameter turning and saw improvement in model performance.\n",
        "While this is an important step in modeling, it is by no means the only way to improve performance.\n",
        "In future articles we will explore other means to prevent overfitting including feature selection and ensembling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGbLUXzz3klv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0103786d-ccd5-428e-fd17-41f4d701ed77"
      },
      "source": [
        "predictions[:,1].mean()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8031943629509815"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F1jfSWkPbeMJ"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Here we explored two methods for hyperparameter turning and saw improvement in model performance.\n",
        "\n",
        "While this is an important step in modeling, it is by no means the only way to improve performance.\n",
        "\n",
        "In future articles we will explore other means to prevent overfitting including feature selection and ensembling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E61FtcVxYn0v",
        "outputId": "1b93bb86-5167-48b9-a545-40a3d6811811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "submission = pd.read_csv(submissionFile)\n",
        "submission['target'] = predictions\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "submission.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>250</td>\n",
              "      <td>0.179365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>251</td>\n",
              "      <td>0.199650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>252</td>\n",
              "      <td>0.200808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>253</td>\n",
              "      <td>0.144066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>254</td>\n",
              "      <td>0.209169</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id    target\n",
              "0  250  0.179365\n",
              "1  251  0.199650\n",
              "2  252  0.200808\n",
              "3  253  0.144066\n",
              "4  254  0.209169"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wVOAS8V0Yqi3"
      },
      "source": [
        "So since this is used in a Kaggle Competition, the results are saved into a submission CSV dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cpUlhl-LYvN3"
      },
      "source": [
        "### End of notebook."
      ]
    }
  ]
}