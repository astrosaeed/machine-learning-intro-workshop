{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_workflow_with_diabetes_dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfogarty/machine-learning-intro-workshop/blob/master/notebooks/ml_workflow_with_diabetes_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rTXivsi4dGFG"
      },
      "source": [
        "# Machine Learning Workflow - Diabetes Data Set\n",
        "\n",
        "- From [Machine Learning Workflow on Diabetes Data](https://towardsdatascience.com/machine-learning-workflow-on-diabetes-data-part-01-573864fcc6b8) on [towardsdatascience.com](https://towardsdatascience.com) by [Lahiru Liyanapathirana](https://towardsdatascience.com/@lahiru.tjay)\n",
        "\n",
        "> *“Machine learning in a medical setting can help enhance medical diagnosis dramatically.”*\n",
        "\n",
        "Updated by [John Fogarty](https://github.com/jfogarty) for Python 3.6 and [Base2 MLI](https://github.com/base2solutions/mli) and [colab](https://colab.research.google.com) standalone evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8GI9qFYpPDM",
        "colab_type": "text"
      },
      "source": [
        "## What's a Workflow?\n",
        "\n",
        "Every machine learning project goes through variations of this cycle.  \n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/ml-workflow.png?raw=1\" />\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wRI6TWasgJ3u"
      },
      "source": [
        "This notebook will portray how data related to diabetes can be leveraged to predict if a person has diabetes or not. More specifically, this notebook will focus on how machine learning can be utilized to predict diseases such as diabetes. By the end of this notebook you will be able to understand concepts like data exploration, data cleansing, feature selection, model selection, model evaluation and apply them in a practical way.\n",
        "\n",
        "## What is Diabetes ?\n",
        "\n",
        "Diabetes is a disease which occurs when the blood glucose level becomes high, which ultimately leads to other health problems such as heart diseases, kidney disease, etc. Diabetes is caused mainly due to the consumption of highly processed foods, bad consumption habits, etc. According to [WHO](http://www.who.int/mediacentre/factsheets/fs312/en/), the number of people with diabetes has been increasing over the years."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ObYNJSoHdHKe"
      },
      "source": [
        "## Data Set Information:\n",
        "\n",
        "This dataset describes the medical records for Pima Indians and whether or not each patient will have an onset of diabetes within five years.\n",
        "\n",
        "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective is to predict based on diagnostic measurements whether a patient has diabetes.\n",
        "\n",
        "**pima-indians-diabetes.csv** 767 rows x 9 columns\n",
        "\n",
        "\n",
        "\n",
        "#### Inspiration:\n",
        "\n",
        "- Some values are not in the range where they are supposed to be, should be treated as missing values.\n",
        "- What kind of method is better to use to fill this type of missing value? What will further classification be like?\n",
        "- Are there any sub-groups significantly more likely to have diabetes?\n",
        "\n",
        "#### Sources\n",
        "\n",
        "- [Description of Diabetes dataset](https://www.kaggle.com/kumargh/pimaindiansdiabetescsv#pima-indians-diabetes.csv) on [Kaggle](https://www.kaggle.com/).\n",
        "- [Description of Diabetes dataset (UCI)](https://data.world/data-society/pima-indians-diabetes-database) in \n",
        "- [Description of Diabetes dataset (UCI)](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes) in \n",
        " [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)\n",
        "\n",
        "#### Fields:\n",
        "\n",
        "Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
        "\n",
        "- Pregnancies: Number of times pregnant\n",
        "- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "- BloodPressure: Diastolic blood pressure (mm Hg)\n",
        "- SkinThickness: Triceps skin fold thickness (mm)\n",
        "- Insulin: 2-Hour serum insulin (mu U/ml)\n",
        "- BMI: Body mass index (weight in kg/(height in m)^2)\n",
        "- DiabetesPedigreeFunction: Diabetes pedigree function\n",
        "- Age: Age (years)\n",
        "- Outcome: Class variable: (1:tested positive for diabetes, 0: tested negative for diabetes)\n",
        "\n",
        "### Feature ranges:\n",
        "\n",
        "<figure>\n",
        "  <img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/wf_diabetes_set.png?raw=1\" align=”right”/>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IJ8lAsQfgV2V"
      },
      "source": [
        "### Prerequisites\n",
        "\n",
        "- Python 3.+\n",
        "- Anaconda (Scikit Learn, Numpy, Pandas, Matplotlib, Seaborn)\n",
        "- Jupyter Notebook\n",
        "- Basic understanding of supervised machine learning methods : specifically classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fhW2tiSsgcCl"
      },
      "source": [
        "## Phase 0 — Data Preparation\n",
        "\n",
        "As a Data Scientist the most tedious task which we encounter is the acquiring and the preparation of a data set. Even though there is an abundance of data in this era, it is still hard to find a suitable data set which suits the problem you are trying to tackle. If there aren’t any suitable data sets to be found, you might have to create your own.\n",
        "\n",
        "In this tutorial we aren’t going to create our own data set, instead we will be using an existing data set called the **“Pima Indians Diabetes Database”** provided by the **UCI Machine Learning Repository** (a famous repository for machine learning data sets). We will be performing the machine learning workflow with the Diabetes Data set described above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GIVwl1XRiqN7"
      },
      "source": [
        "## Phase 1 — Data Exploration\n",
        "\n",
        "When encountered with a data set, first we should analyse and **“get to know”** the data set. This step is necessary to familiarize with the data, to gain some understanding about the potential features and to see if data cleaning is needed. This [EDA (Exploratory Data Analysis)](https://en.wikipedia.org/wiki/Exploratory_data_analysis) is a key data science component of most machine learning applications.\n",
        "\n",
        "First we will import the necessary libraries and import our data set to the Jupyter notebook. We can observe the mentioned columns in the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dM4bWq3gvMPb",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "class Context:\n",
        "    DATA = 'https://raw.githubusercontent.com/jfogarty/machine-learning-intro-workshop/master/data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TVC6G15hvbkH",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "diabetes = pd.read_csv(Context.DATA + 'diabetes.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "97bd7lbsjVRF"
      },
      "source": [
        "> Important: It should be noted that the above data set contains only limited features, where as in reality numerous features come into play.\n",
        "\n",
        "We can examine the data set using the [pandas DataFrame ‘head‘](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DOi-frnnvoTC",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "diabetes.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P7CacBdQjl2d"
      },
      "source": [
        "We can find the dimensions of the data set using the [pandas DataFrame ‘shape’](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html?highlight=shape#pandas.DataFrame.shape) attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ef9yhi2iwFzC",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "print(f\"Diabetes data set dimensions : {diabetes.shape}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5gTgOCwaj5Zm"
      },
      "source": [
        "We can observe that the data set contains 768 rows and 9 columns. **‘Outcome’** is the column which we are going to predict, which says if the patient is diabetic or not. 1 means the person is diabetic and 0 means the person is not. We can identify that out of the 768 persons, 500 are labeled as 0 (non diabetic) and 268 as 1 (diabetic)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aJThRYzSkg62",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "diabetes.groupby('Outcome').size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "96ezxzJOj5ch"
      },
      "source": [
        "### Visualization\n",
        "\n",
        "Visualization of data is an imperative aspect of data science. It helps to understand data and also to explain the data to another person. Python has several interesting visualization libraries such as Matplotlib, Seaborn, etc.\n",
        "In this tutorial we will use pandas visualization, which is built on top of matplotlib, to find the data distribution of the features.\n",
        "\n",
        "We can use the [pandas DataFrameGroupBy ‘hist‘](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.hist.html?highlight=hist#pandas.core.groupby.DataFrameGroupBy.hist) method to draw histograms for the two responses separately. (The images are not displayed here.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sUzcfiE8kuFO",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "diabetes.groupby('Outcome').hist(figsize=(9, 9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pcQqleyMj5f1"
      },
      "source": [
        "## Phase 2 — Data Cleaning\n",
        "\n",
        "Next phase of the machine learning work flow is the data cleaning. Considered to be one of the crucial steps of the work flow, because it can make or break the model. There is a saying in machine learning **“Better data beats fancier algorithms”**, which suggests better data gives you better resulting models.\n",
        "There are several factors to consider in the data cleaning process.\n",
        "\n",
        "1. Duplicate or irrelevant observations.\n",
        "\n",
        "2. Bad labeling of data, same category occurring multiple times.\n",
        "\n",
        "3. Missing or null data points.\n",
        "\n",
        "4. Unexpected outliers.\n",
        "\n",
        "> Sorry, we won’t be discussing data cleaning procedure in detail in this tutorial.\n",
        "\n",
        "Since we are using a standard data set, we can safely assume that factors 1, 2 are already dealt with. Unexpected outliers are either useful or potentially harmful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LZZtghMjl26w"
      },
      "source": [
        "## Missing or Null Data points\n",
        "\n",
        "We can find any missing or null data points of the data set (if there are any) using the following pandas [isnull](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.isnull.html?highlight=isnull#pandas.Index.isnull) and [isna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isna.html?highlight=isna#pandas.DataFrame.isna) functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cP3gFz67mLK4",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "diabetes.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PR4LLzNmmPvP",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "diabetes.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IltTwnFzmZI3"
      },
      "source": [
        "## Unexpected Outliers\n",
        "\n",
        "When analyzing the histogram we can identify that there are some outliers in some columns. We will further analyse those outliers and determine what we can do about them.\n",
        "\n",
        "**Blood pressure** : By observing the data we can see that there are 0 values for blood pressure. And it is evident that the readings of the data set seem wrong because a living person cannot have diastolic blood pressure of zero. By observing the data we can see 35 counts where the value is 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wYHtc971ml8w",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "print(f\"- Total entries with 0 Blood Pressure: {diabetes[diabetes.BloodPressure == 0].shape[0]}\")\n",
        "Total :  35\n",
        "diabetes[diabetes.BloodPressure == 0].groupby('Outcome')['Age'].count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X_F8LqCDmZMN"
      },
      "source": [
        "**Plasma glucose levels** : Even after fasting glucose level would not be as low as zero. Therefore zero is an invalid reading. By observing the data we can see **5 counts where the value is 0**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WSQAGQgOm_I5",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "print(f\"- Total entries with 0 Glucose levels: {diabetes[diabetes.Glucose == 0].shape[0]}\")\n",
        "print(diabetes[diabetes.Glucose == 0].groupby('Outcome')['Age'].count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_PWqHz09mZQB"
      },
      "source": [
        "**Skin Fold Thickness** : For normal people skin fold thickness can’t be less than 10 mm let alone zero. Total **count where skin fold is 0 : 227**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w-0W_n8Wnb0_",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "print(f\"- Total entries with 0 Skin fold thickness: {diabetes[diabetes.SkinThickness == 0].shape[0]}\")\n",
        "print(diabetes[diabetes.SkinThickness  == 0].groupby('Outcome')['Age'].count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NlwCfCgYmZxO"
      },
      "source": [
        "**BMI** : Should not be 0 or close to zero unless the person is really underweight which could be life threatening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cd2Y-bZbmYXX",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "print(f\"- Total entries with 0 Body mass: {diabetes[diabetes.BMI == 0].shape[0]}\")\n",
        "print(diabetes[diabetes.BMI == 0].groupby('Outcome')['Age'].count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EoOUMhvcn78C"
      },
      "source": [
        "**Insulin** : In a rare situation a person can have zero insulin but by observing the data, we can find that there is a **total of 374 counts**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ICeuzYWgoIJi",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "print(f\"- Total entries with 0 Insulin: {diabetes[diabetes.Insulin == 0].shape[0]}\")\n",
        "print(diabetes[diabetes.Insulin  == 0].groupby('Outcome')['Age'].count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kFad8U9PpBny"
      },
      "source": [
        "#### Shortcut\n",
        "\n",
        "You could just as well have written:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NpkFH140oWCA",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "for f in ['BloodPressure', 'Glucose', 'SkinThickness', 'BMI', 'Insulin']:\n",
        "    print(f\"\\n- Total entries with 0 {f}: {diabetes[diabetes[f] == 0].shape[0]}\")\n",
        "    print(diabetes[diabetes[f] == 0].groupby('Outcome')['Age'].count())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jXWyEzz1pagf"
      },
      "source": [
        "### Handling Invalid Data\n",
        "\n",
        "Here are several ways to handle invalid data values :\n",
        "\n",
        "1. Ignore/remove these cases : This is not actually possible in most cases because that would mean losing valuable information. And in this case “skin thickness” and “insulin” columns have a lot of invalid points. But it might work for “BMI”, “glucose”, and “blood pressure” data points.\n",
        "\n",
        "2. Put average/mean values : This might work for some data sets, but in our case putting a mean value to the blood pressure column would send a wrong signal to the model.\n",
        "\n",
        "3. Avoid using features : It is possible to not use the features with a lot of invalid values for the model. This may work for “skin thickness” but it's hard to predict that.\n",
        "\n",
        "4. Add a new column (\"skin thickness is valid\") that indicates whether this feature is valid for this row.\n",
        "\n",
        "By the end of the data cleaning process we have come to the conclusion that this given data set is incomplete. Since this is a demonstration for machine learning we will proceed with the given data with some minor adjustments.\n",
        "\n",
        "We will remove the rows which the “BloodPressure”, “BMI” and “Glucose” are zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CKeAu0uwpsVg",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "diabetes_mod = diabetes[(diabetes.BloodPressure != 0) & (diabetes.BMI != 0) & (diabetes.Glucose != 0)]\n",
        "print(diabetes_mod.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UnGretzgp2E5"
      },
      "source": [
        "## Phase 3 — Feature Engineering\n",
        "\n",
        "Feature engineering is the process of transforming the gathered data into features that better represent the problem that we are trying to solve, thereby improving the performance and accuracy of the model.\n",
        "\n",
        "Feature engineering can create more input features from the existing features and also combine several features to produce more intuitive features to feed to the model.\n",
        "\n",
        "> *“Feature engineering enables to highlight the important features and facilitate to bring domain expertise on the problem to the table. It also allows to avoid overfitting the model despite providing many input features.”*\n",
        "\n",
        "The domain of the problem we are trying to tackle requires lots of related features. Since the data set is already provided, and by examining the data we can’t further create or dismiss any data at this point. In the data set we have the following features.\n",
        "\n",
        "> **‘Pregnancies’, ‘Glucose’, ‘Blood Pressure’, ‘Skin Thickness’, ‘Insulin’, ‘BMI’, ‘Diabetes Pedigree Function’, ‘Age’**\n",
        "\n",
        "By a crude observation we can say that the ‘Skin Thickness’ is not an indicator of diabetes. But we can’t deny the fact that it is unusable at this point.\n",
        "\n",
        "Therefore we will use all the features available. We separate the data set into features and the response that we are going to predict. We will assign the features to the **$X$ variable** and the response to the **$y$ variable**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D2B6jh_-qQXV",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "feature_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
        "X = diabetes_mod[feature_names]\n",
        "y = diabetes_mod.Outcome"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dc5etFA-qWD5"
      },
      "source": [
        "Generally feature engineering is performed before selecting the model. However for this tutorial we follow a different approach. Initially we will be utilizing all the features provided in the data set to the model, we will revisit feature engineering to discuss feature importance on the selected model.\n",
        "\n",
        "[This article](https://elitedatascience.com/feature-engineering-best-practices) gives a very nice explanation about [Feature Engineering](https://en.wikipedia.org/wiki/Feature_engineering)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8FN7U14lqumc"
      },
      "source": [
        "## Phase 4 — Model Selection\n",
        "\n",
        "Model selection or algorithm selection phase is the most exciting and the heart of machine learning. It is the phase where we select the model which performs best for the data set at hand.\n",
        "\n",
        "First we will be calculating the **“Classification Accuracy (Testing Accuracy)”** of a given set of classification models with their default parameters to determine which model performs better with the diabetes data set.\n",
        "\n",
        "We will import the necessary libraries to the notebook. We import 7 [sklearn] classifiers namely:  \n",
        "- K-Nearest Neighbors\n",
        "- Support Vector Classifier\n",
        "- Logistic Regression\n",
        "- Gaussian Naive Bayes\n",
        "- Random Forest\n",
        "- Gradient Boost\n",
        "\n",
        "These are our contenders for the best classifier.\n",
        "\n",
        "Look at the classifer comparisons at the end of this notebook for more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bbi6RY9dspeM",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BnGOF45Osr1C"
      },
      "source": [
        "We will initialize the classifier models with their default parameters and add them to a model list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f0R2LDB7svKw",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "models = []\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('SVC', SVC()))\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('DT', DecisionTreeClassifier()))\n",
        "models.append(('GNB', GaussianNB()))\n",
        "models.append(('RF', RandomForestClassifier()))\n",
        "models.append(('GB', GradientBoostingClassifier()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2a0vWl3wsysT"
      },
      "source": [
        "> Generally models are trained with Scikit learn with something like:\n",
        "- `knn = KNeighborsClassifier()`\n",
        "- `knn.fit(X_train, y_train)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KpQSWbv7tI9t"
      },
      "source": [
        "## Evaluation Methods\n",
        "\n",
        "It is a general practice to avoid training and testing on the same data. The reasons are that, the goal of the model is to predict **out-of-sample data**, and the model could be overly complex leading to **overfitting**. To avoid the aforementioned problems, there are two precautions.\n",
        "\n",
        "- **1. Train/Test Split**\n",
        "\n",
        "- **2. K-Fold Cross Validation**\n",
        "\n",
        "We will import *“train_test_split”* for **train/test split** and *“cross_val_score”* for **k-fold cross validation**. \n",
        "\n",
        "*“accuracy_score”* is to evaluate the accuracy of the model in the train/test split method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bAQ3gwz0tk01",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D8hEvLakugP7"
      },
      "source": [
        "We will use the above mentioned methods to find the best performing base models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p1fdZvPqtwqK"
      },
      "source": [
        "## Evaluate with Train/Test Split\n",
        "\n",
        "This method splits the data set into two portions : a **training set** and a **test set**. \n",
        "\n",
        "The **training set** is used to train the model. And the **test set** is used to test the model, and evaluate the accuracy.\n",
        "\n",
        "- **Cons** : Provides a **high-variance estimate** of out-of-sample accuracy\n",
        "\n",
        "- **Pros** : But, train/test split is still useful because of its **flexibility and speed**\n",
        "\n",
        "<figure>\n",
        "  <center>\n",
        "  <img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/wf_1.png?raw=1\"/>\n",
        "  </center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WejxK1RluVuV"
      },
      "source": [
        "### Train/Test Split with Scikit Learn:\n",
        "\n",
        "Next we can split the features and responses into train and test portions. We stratify ( process where each response class should be represented with equal proportions in each of the portions) the samples. [See train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3r9OFDZnulM7",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = diabetes_mod.Outcome, random_state=0)\n",
        "X_train.info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "78B-e7aywh9N"
      },
      "source": [
        "Then we fit each model in a loop and calculate the accuracy of the respective model using the *“accuracy_score”*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sFEhLzFhwiqq",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "names = []\n",
        "scores = []\n",
        "for name, model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    scores.append(accuracy_score(y_test, y_pred))\n",
        "    names.append(name)\n",
        "tr_split = pd.DataFrame({'Name': names, 'Score': scores})\n",
        "print(tr_split)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dBvw3LDSwi6U"
      },
      "source": [
        "## Evaluate with K-Fold Cross Validation\n",
        "\n",
        "This method splits the data set into **K equal partitions** (“folds”), then uses 1 fold as the **test set** and the union of the other folds as the **training set**.\n",
        "\n",
        "Next the model is tested for accuracy. The process will follow the above steps K times, using a different fold as the testing set each time. The **average testing accuracy** of the process is the testing accuracy.\n",
        "\n",
        "- **Cons** : Much slower than Train/Test split\n",
        "\n",
        "- **Pros** : More accurate estimate of out-of-sample accuracy. More “efficient” use of data (every observation is used for both training and testing)\n",
        "\n",
        "<figure>\n",
        "  <center>\n",
        "  <img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/wf_2.png?raw=1\"/>\n",
        "  </center>\n",
        "</figure>\n",
        "\n",
        "> ***This is the preferred method when computation capability is not scarce. We will be using this method from here on out.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fVu3wKPUx7lU"
      },
      "source": [
        "### K-Fold Cross Validation with Scikit Learn :\n",
        "\n",
        "We will move forward with [K-Fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) cross validation as it is more accurate and use the data efficiently. We will train the models using 10 fold cross validation and calculate the mean accuracy of the models. *“cross_val_score”* provides its own training and accuracy calculation interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XRQDdx85xvfd",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "names = []\n",
        "scores = []\n",
        "for name, model in models:   \n",
        "    kfold = KFold(n_splits=10, random_state=10) \n",
        "    score = cross_val_score(model, X, y, cv=kfold, scoring='accuracy').mean()\n",
        "    \n",
        "    names.append(name)\n",
        "    scores.append(score)\n",
        "kf_cross_val = pd.DataFrame({'Name': names, 'Score': scores})\n",
        "print(kf_cross_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xQ1DYwjayvb2"
      },
      "source": [
        "### Grapically Compare the Accuracy Scores\n",
        "\n",
        "We can plot the accuracy scores using seaborn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uhklqqiay7CE",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "axis = sns.barplot(x = 'Name', y = 'Score', data = kf_cross_val)\n",
        "axis.set(xlabel='Classifier', ylabel='Accuracy')\n",
        "for p in axis.patches:\n",
        "    height = p.get_height()\n",
        "    axis.text(p.get_x() + p.get_width()/2, height + 0.005, '{:1.4f}'.format(height), ha=\"center\") \n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tn97QbwwzDa2"
      },
      "source": [
        "We can see the **Logistic Regression**, **Gaussian Naive Bayes**, **Random Forest** and **Gradient Boosting** have performed better than the rest.\n",
        "\n",
        " From the base level we can observe that the Logistic Regression performs better than the other algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yK_9B3n-zSOX"
      },
      "source": [
        "### *At the baseline Logistic Regression managed to achieve a classification accuracy of about 77%, although sometimes Gradient Boosting will be better. This will be selected as the prime candidate for the next phases*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2WaTzP5RzyMC"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook we discussed the basic machine learning workflow steps such as data exploration, data cleaning steps, feature engineering basics and model selection using Scikit Learn library. Next, I will be discussing more about feature engineering, and hyperparameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vqr4tHtRr_-E"
      },
      "source": [
        "# Bonus Section! Classifier comparisons\n",
        "\n",
        "A comparison of several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets.\n",
        "\n",
        "Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers.\n",
        "\n",
        "The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.\n",
        "\n",
        "- [Classifier comparison](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) from the [scikit-learn docs](https://scikit-learn.org)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X1h86BJKj4hF",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "def evaluate_all_classifiers():\n",
        "    h = .02  # step size in the mesh\n",
        "\n",
        "    names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
        "            \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
        "            \"Naive Bayes\", \"QDA\"]\n",
        "\n",
        "    classifiers = [\n",
        "        KNeighborsClassifier(3),\n",
        "        SVC(kernel=\"linear\", C=0.025),\n",
        "        SVC(gamma=2, C=1),\n",
        "        GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
        "        DecisionTreeClassifier(max_depth=5),\n",
        "        RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "        MLPClassifier(alpha=1, max_iter=1000),\n",
        "        AdaBoostClassifier(),\n",
        "        GaussianNB(),\n",
        "        QuadraticDiscriminantAnalysis()]\n",
        "\n",
        "    X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
        "                            random_state=1, n_clusters_per_class=1)\n",
        "    rng = np.random.RandomState(2)\n",
        "    X += 2 * rng.uniform(size=X.shape)\n",
        "    linearly_separable = (X, y)\n",
        "\n",
        "    datasets = [make_moons(noise=0.3, random_state=0),\n",
        "                make_circles(noise=0.2, factor=0.5, random_state=1),\n",
        "                linearly_separable\n",
        "                ]\n",
        "\n",
        "    figure = plt.figure(figsize=(27, 9))\n",
        "    i = 1\n",
        "    # iterate over datasets\n",
        "    for ds_cnt, ds in enumerate(datasets):\n",
        "        # preprocess dataset, split into training and test part\n",
        "        X, y = ds\n",
        "        X = StandardScaler().fit_transform(X)\n",
        "        X_train, X_test, y_train, y_test = \\\n",
        "            train_test_split(X, y, test_size=.4, random_state=42)\n",
        "\n",
        "        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                            np.arange(y_min, y_max, h))\n",
        "\n",
        "        # just plot the dataset first\n",
        "        cm = plt.cm.RdBu\n",
        "        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
        "        if ds_cnt == 0:\n",
        "            ax.set_title(\"Input data\")\n",
        "        # Plot the training points\n",
        "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
        "                edgecolors='k')\n",
        "        # Plot the testing points\n",
        "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
        "                edgecolors='k')\n",
        "        ax.set_xlim(xx.min(), xx.max())\n",
        "        ax.set_ylim(yy.min(), yy.max())\n",
        "        ax.set_xticks(())\n",
        "        ax.set_yticks(())\n",
        "        i += 1\n",
        "\n",
        "        # iterate over classifiers\n",
        "        for name, clf in zip(names, classifiers):\n",
        "            ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
        "            clf.fit(X_train, y_train)\n",
        "            score = clf.score(X_test, y_test)\n",
        "\n",
        "            # Plot the decision boundary. For that, we will assign a color to each\n",
        "            # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "            if hasattr(clf, \"decision_function\"):\n",
        "                Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "            else:\n",
        "                Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "\n",
        "            # Put the result into a color plot\n",
        "            Z = Z.reshape(xx.shape)\n",
        "            ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
        "\n",
        "            # Plot the training points\n",
        "            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
        "                    edgecolors='k')\n",
        "            # Plot the testing points\n",
        "            ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
        "                    edgecolors='k', alpha=0.6)\n",
        "\n",
        "            ax.set_xlim(xx.min(), xx.max())\n",
        "            ax.set_ylim(yy.min(), yy.max())\n",
        "            ax.set_xticks(())\n",
        "            ax.set_yticks(())\n",
        "            if ds_cnt == 0:\n",
        "                ax.set_title(name)\n",
        "            ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
        "                    size=15, horizontalalignment='right')\n",
        "            i += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9gmnUEhSwNIG",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "evaluate_all_classifiers()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB-gzEqIvgNC",
        "colab_type": "text"
      },
      "source": [
        "Note that the Linear SVM performed a bit worse than chance in the second row.\n",
        "\n",
        "Note that for the 1st and 2nd rows, several classifiers might do better if the data was represented in terms of polar coordinates instead of cartesian coordinates.  Once again, better data (or data representations) can be better than fancier algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Ac2X7VRj24d"
      },
      "source": [
        "### End of notebook."
      ]
    }
  ]
}