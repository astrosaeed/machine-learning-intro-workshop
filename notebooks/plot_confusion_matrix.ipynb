{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "plot_precision_recall.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfogarty/machine-learning-intro-workshop/blob/master/notebooks/plot_confusion_matrix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XLrZUYOJVES0"
      },
      "source": [
        "# Plotting Confusion Matrices\n",
        "\n",
        "- From [Demystifying ‘Confusion Matrix’ Confusion](https://towardsdatascience.com/demystifying-confusion-matrix-confusion-9e82201592fd) by [salrite](https://towardsdatascience.com/@salrite?source=post_page-----9e82201592fd----------------------) in [towardsdatascience.co](https://towardsdatascience.co)\n",
        "- and From [Confiusion Matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) in [scikit-learn.org](https://scikit-learn.org/stable/) documentation site.\n",
        "\n",
        "Updated by [John Fogarty](https://github.com/jfogarty) for Python 3.6 and [Base2 MLI](https://github.com/base2solutions/mli) and [colab](https://colab.research.google.com) standalone evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kXahgbN4VES6",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbC42pjLGUgD",
        "colab_type": "text"
      },
      "source": [
        "# Confusion Matrix\n",
        "\n",
        "\n",
        "In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix,[5] is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).[2] The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another).\n",
        "\n",
        "We will use the [UCI Bank Note Authentication Dataset](https://archive.ics.uci.edu/ml/datasets/banknote+authentication) for demystifying the confusion behind Confusion Matrix. We will predict and evaluate our model, and along the way develop our conceptual understanding. Also will be providing the links to further reading wherever required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1valzYbG5_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import os\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euerK-2PLWQA",
        "colab_type": "text"
      },
      "source": [
        "## Understanding the Data\n",
        "\n",
        "The Dataset contains properties of the wavelet transformed image of $400x400$ pixels of a BankNote, and can be found here. It is recommended for reader to download the dataset and follow along. Further for reference, you can find the Kaggle Notebook [here](https://www.kaggle.com/ritesaluja/binaryclassification-and-modelevaluation-example/).\n",
        "\n",
        "<figure>\n",
        "  <center>\n",
        "    <img src=\"https://miro.medium.com/max/288/1*BAAk374bKlraxnJvV3_hyg.png\" width=\"60%\" >\n",
        "    <figcaption>Confusion Matrix for Binary Classification</figcaption>\n",
        "  </center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6tnbJ10MMSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "URL='https://datahub.io/machine-learning/banknote-authentication/r/banknote-authentication.csv'\n",
        "URL='https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt'\n",
        "import os\n",
        "TMPDATA = './tmpData'\n",
        "DATAFILE = os.path.join(TMPDATA, 'banknote-authentication.csv')\n",
        "if not os.path.exists(TMPDATA) : os.makedirs(TMPDATA)\n",
        "!curl $URL -o $DATAFILE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEXGhLt6HKNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colnames = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n",
        "df = pd.read_csv(DATAFILE, names=colnames)\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8f7Ycu1HLLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Class is our Target Label, with zero indicating the Bank Note is Forged and 1 Indicating it is Legit\n",
        "df['class'].value_counts() #to check if the data is equally balanced between the two classes for prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsuKkl6OLC4R",
        "colab_type": "text"
      },
      "source": [
        "## Building the Model\n",
        "\n",
        "Splitting the Data into Training and Test Set, Train is on which we will be training our model and the evaluation will be performed on the Test set, we are skipping the Validation set here for simplicity and lack of sufficient data. In general the data is divided into three sets Train, Test and Validation, read more [here](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiPtn01YHOQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining features and target variable\n",
        "y = df['class']\n",
        "X = df.drop(columns = ['class'])\n",
        "\n",
        "#splitting the data into train and test set \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOIdBz_nK-pt",
        "colab_type": "text"
      },
      "source": [
        "Next, we will make a simple Logistic Regression Model for our Prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJyleYe8HSjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predicting using Logistic Regression for Binary classification \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "LR = LogisticRegression()\n",
        "LR.fit(X_train,y_train) #fitting the model \n",
        "y_pred = LR.predict(X_test) #prediction "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88MC7n-1KhGk",
        "colab_type": "text"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Let’s plot the most confusing Confusion Matrix? Just Kidding, Lets have a simple Confusion Matrix (Scikit-learn documentation used for the below code).\n",
        "\n",
        "<figure>\n",
        "  <center>\n",
        "    <img src=\"https://miro.medium.com/max/288/1*BAAk374bKlraxnJvV3_hyg.png\" width=\"60%\" >\n",
        "    <figcaption>Confusion Matrix for Binary Classification</figcaption>\n",
        "  </center>\n",
        "</figure>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar5LY8hPHVi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluation \n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=['Forged','Authorized'],\n",
        "                      title='Confusion matrix, without normalization')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITlwyZFHu47R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(cnf_matrix, classes=['Forged','Authorized'],\n",
        "                      title='Confusion matrix, with normalization',\n",
        "                      normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86Z4iX39HaaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "print(\"True Negatives: \",tn)\n",
        "print(\"False Positives: \",fp)\n",
        "print(\"False Negatives: \",fn)\n",
        "print(\"True Positives: \",tp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODkY2R3aKZHC",
        "colab_type": "text"
      },
      "source": [
        "## How Accurate is our Model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYd_ZQ8pHeqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Accuracy (%) \n",
        "Accuracy = (tn+tp)*100/(tp+tn+fp+fn) \n",
        "print(\"Accuracy {:0.2f}%\".format(Accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_oF6ppoJ0-T",
        "colab_type": "text"
      },
      "source": [
        "### Does Accuracy matter?\n",
        "\n",
        "Not always, it may not be the right measure at times, especially if your Target class is not balanced (data is skewed). Then you may consider additional metrics like Precision, Recall, F score (combined metric), but before diving in lets take a step back and understand the terms that form the basis for these."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYJRQyMfJ1Lx",
        "colab_type": "text"
      },
      "source": [
        "### Some Basic Terms\n",
        "\n",
        "- **True Positive** — Label which was predicted Positive (in our scenario Authenticated Bank Notes) and is actually Positive (i.e. belong to Positive ‘Authorized’ Class).\n",
        "\n",
        "- **True Negative** — Label which was predicted Negative (in our scenario Forged Bank Notes) and is actually Negative (i.e. belong to Negative ‘Forged’ Class).\n",
        "\n",
        "- **False Positive** — Label which was predicted as Positive, but is actually Negative, or in simple words the Note wrongly predicted as Authentic by our Model, but is actually Forged. In Hypothesis Testing it is also known as Type 1 error or the incorrect rejection of Null Hypothesis, refer this to read more about Hypothesis testing.\n",
        "\n",
        "- **False Negatives** — Labels which was predicted as Negative, but is actually Positive (Authentic Note predicted as Forged). It is also known as Type 2 error, which leads to the failure in rejection of Null Hypothesis.\n",
        "\n",
        "Now lets look at most common evaluation metrics every Machine Learning Practitioner should know!\n",
        "\n",
        "<figure>\n",
        "  <center>\n",
        "    <img src=\"https://miro.medium.com/max/243/1*B0UVzVktFU9uQ1t6_hfN-A.png\" width=\"50%\" >\n",
        "  </center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SriAM40SJZjn",
        "colab_type": "text"
      },
      "source": [
        "## Metrics beyond Accuracy\n",
        "\n",
        "### Precision\n",
        "\n",
        "It is the ‘Exactness’, ability of the model to return only relevant instances. If your use case/problem statement involves minimizing the False Positives, i.e. in current scenario if you don’t want the Forged Notes to be labelled as Authentic by the Model then Precision is something you need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ysWeR-aHhik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Precision \n",
        "Precision = tp/(tp+fp) \n",
        "print(\"Precision {:0.2f}\".format(Precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4dfGoZtJWRU",
        "colab_type": "text"
      },
      "source": [
        "### Recall\n",
        "\n",
        "It is the ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity. In the current scenario if your focus is to have the least False Negatives i.e. you don’t Authentic Notes to be wrongly classified as Forged then Recall can come to your rescue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_ih1rvwHk4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Recall \n",
        "Recall = tp/(tp+fn) \n",
        "print(\"Recall {:0.2f}\".format(Recall))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajnXpxPQJG7l",
        "colab_type": "text"
      },
      "source": [
        "### F1 Measure\n",
        "\n",
        "Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from $0$ to $1$. F1 Score reaches its best value at 1 (perfect precision & recall) and worst at $0$, read more [here](https://stats.stackexchange.com/questions/49226/how-to-interpret-f-measure-values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wjncJhlHoEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#F1 Score\n",
        "f1 = (2*Precision*Recall)/(Precision + Recall)\n",
        "print(\"F1 Score {:0.2f}\".format(f1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ6loN_KIYNy",
        "colab_type": "text"
      },
      "source": [
        "### F-beta Measure\n",
        "\n",
        "It is the general form of F measure — Beta $0.5$ & $2$ are usually used as measures, $0.5$ indicates the Inclination towards Precision whereas $2$ favors Recall giving it twice the weightage compared to precision.\n",
        "\n",
        "<figure>\n",
        "  <center>\n",
        "    <img src=\"https://miro.medium.com/max/610/1*5HgVbqXE3tejNUNI3tnoNg.png\" width=\"50%\" >\n",
        "  </center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC5qOvJIHq_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fbeta score\n",
        "def fbeta(precision, recall, beta):\n",
        "    return ((1+pow(beta,2))*precision*recall)/(pow(beta,2)*precision + recall)\n",
        "            \n",
        "f2 = fbeta(Precision, Recall, 2)\n",
        "f0_5 = fbeta(Precision, Recall, 0.5)\n",
        "\n",
        "print(\"F2 {:0.2f}\".format(f2))\n",
        "print(\"\\nF0.5 {:0.2f}\".format(f0_5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY7yUtbWIPSa",
        "colab_type": "text"
      },
      "source": [
        "### Specificity\n",
        "\n",
        "It is also referred to as ‘True Negative Rate’ (Proportion of actual negatives that are correctly identified), i.e. more True Negatives the data hold the higher its Specificity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks0q3HyxHuJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Specificity \n",
        "Specificity = tn/(tn+fp)\n",
        "print(\"Specificity {:0.2f}\".format(Specificity))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMXt9S_8H4G-",
        "colab_type": "text"
      },
      "source": [
        "## ROC (Receiver Operating Characteristic curve)\n",
        "\n",
        "The plot of ‘True Positive Rate’ (Sensitivity/Recall) against the ‘False Positive Rate’ (1-Specificity) at different classification thresholds.\n",
        "\n",
        "The area under the **ROC curve** (AuC) measures the entire two-dimensional area underneath the curve. It is a measure of how well a parameter can distinguish between two diagnostic groups. Often used as a measure of quality of the classification models.\n",
        "A random classifier has an area under the curve of $0.5$, while **AUC** for a perfect classifier is equal to $1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD-JBTojtoAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install scikit-plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ5k19ikHw5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ROC\n",
        "import scikitplot as skplt #to make things easy\n",
        "y_pred_proba = LR.predict_proba(X_test)\n",
        "skplt.metrics.plot_roc_curve(y_test, y_pred_proba)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3gYFlSUQVETA"
      },
      "source": [
        "# Beyond True and False\n",
        "\n",
        "Example of confusion matrix usage to evaluate the quality of the output of a classifier on the iris data set. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. The higher the diagonal values of the confusion matrix the better, indicating many correct predictions.\n",
        "\n",
        "The figures show the confusion matrix with and without normalization by class support size (number of elements in each class). \n",
        "\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <center>\n",
        "      <img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_001.png\" />\n",
        "  <figurecaption></figurecaption>  \n",
        "    </center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "This kind of normalization can be interesting in case of class imbalance to have a more visual interpretation of which class is being misclassified.\n",
        "\n",
        "\n",
        "<figure>\n",
        "  <center>\n",
        "      <img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_002.png\" />\n",
        "  <figurecaption></figurecaption>  \n",
        "    </center>\n",
        "</figure>\n",
        "\n",
        "Here the results are not as good as they could be as our choice for the regularization parameter C was not the best. In real life applications this parameter is usually chosen using [Tuning the hyper-parameters of an estimator](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "adGe6nA_VETG"
      },
      "source": [
        "## Three Class Confusion Matrix\n",
        "\n",
        "This example from using the classic Iris dataset illustrates a non-binary matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jsc83q0sVETJ",
        "colab": {}
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "# import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Split the data into a training set and a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# Run classifier, using a model that is too regularized (C too low) to see\n",
        "# the impact on the results\n",
        "classifier = svm.SVC(kernel='linear', C=0.01)\n",
        "y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ETqBrxTzVEUf"
      },
      "source": [
        "### End of notebook."
      ]
    }
  ]
}