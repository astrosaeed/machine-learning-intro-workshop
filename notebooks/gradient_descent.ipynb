{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Gradient-descent.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H0VzvfBxFWBQ"
      },
      "source": [
        "# An introduction to Gradient Descent\n",
        "\n",
        "- From [Implementation of gradient-descent-in-python](https://medium.com/coinmonks/implementation-of-gradient-descent-in-python-a43f160ec521) at [medium.com](https://medium.com) by Deepak Battini.\n",
        "- And [Implement Gradient Descent in Python](https://towardsdatascience.com/implement-gradient-descent-in-python-9b93ed7108d1) at [TowardDatascience.com](https://towardsdatascience.com) by Rohan Joseph.\n",
        "- And [A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) at [machinelearningmastery.com](https://machinelearningmastery.com) by  Jason Brownlee\n",
        "\n",
        "Updated by [John Fogarty](https://github.com/jfogarty) for Python 3.6 and [Base2 MLI](https://github.com/base2solutions/mli) and [colab](https://colab.research.google.com) standalone evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmjInBsK3Jfu",
        "colab_type": "text"
      },
      "source": [
        "Every machine learning engineer is always looking to improve their model’s performance. This is where optimization, one of the most important fields in machine learning, comes in. Optimization allows us to select the best parameters, associated with the machine learning algorithm or method we are using, for our problem case. There are several types of optimization algorithms. Perhaps the most popular one is the Gradient Descent optimization algorithm. The first encounter of Gradient Descent for many machine learning engineers is in their introduction to neural networks. In this tutorial, we will teach you how to implement Gradient Descent from scratch in Python. \n",
        "\n",
        "But first, what exactly is Gradient Descent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7myUH-b3Jfv",
        "colab_type": "text"
      },
      "source": [
        "## What is Gradient Descent?\n",
        "\n",
        "It is an optimization algorithm to find the minimum of a function. We start with a random point on the function and move in the negative direction of the gradient of the function to reach the local/global minima.\n",
        "\n",
        "<figure>\n",
        "  <br><center>\n",
        "    <img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/homer-descending.gif?raw=1\" />\n",
        "    <figcaption>Homer descending!</figcaption>     \n",
        "  </center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar0iQ3lW3Jfw",
        "colab_type": "text"
      },
      "source": [
        "Gradient descent is an optimization algorithm that helps machine learning models converge at a minimum value through repeated steps. Essentially, gradient descent is used to minimize a function by finding the value that gives the lowest output of that function. Often times, this function is usually a loss function. Loss functions measure how bad our model performs compared to actual occurrences. Hence, it only makes sense that we should reduce this loss. One way to do this is via gradient descent.\n",
        "\n",
        "A simple gradient descent algorithm is as follows:\n",
        "\n",
        "1. Obtain a function to minimize F(x)\n",
        "1. Initialize a value x from which to start the descent or optimization from\n",
        "1. Specify a **learning rate** that will determine how much of a step to descend by or how quickly you converge to the minimum value\n",
        "1. Obtain the derivative of that value x (the descent)\n",
        "1. Proceed to descend by the derivative of that value multiplied by the learning rate\n",
        "1. Update the value of x with the new value descended to\n",
        "1. Check your stop condition to see whether to stop\n",
        "1. If condition satisfied, stop. If not, proceed to step 4 with the new x value and keep repeating algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmu6JM1V3Jfx",
        "colab_type": "text"
      },
      "source": [
        "### Heres an Example by hand :\n",
        "\n",
        "Question : Find the local minima of the function: $y=(x+5)^2$ starting from the point $x=3$\n",
        "    \n",
        "<figure>\n",
        "  <br><center>\n",
        "    <img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/graph_x+5_squared.png?raw=1\" />\n",
        "  </center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gnj63qY3Jfy",
        "colab_type": "text"
      },
      "source": [
        "### Solution\n",
        "\n",
        "We know the answer just by looking at the graph. $y = (x+5)^2$ reaches its minimum value when $x = -5$ (i.e when $x=-5$, $y=0$). Hence $x=-5$ is the \n",
        "local and global minima of the function.\n",
        "\n",
        "Now, let’s see how to obtain the same numerically using gradient descent.\n",
        "\n",
        "- Step 1 : Initialize x =3. \n",
        "$$x_0 = 3$$\n",
        "Then, find the gradient of the function, \n",
        "$$\n",
        "  \\frac{dy}{dx} = \\frac{dy}{dx}(x+5)^2 \n",
        "                = 2(x+5) \n",
        "$$.\n",
        "- Step 3 : Move in the direction of the negative of the gradient (Why?). But wait, how much to move? For that, we require a learning rate. Let us assume the learning rate → 0.01 (this is often called $\\alpha$)\n",
        "\n",
        "$$\n",
        "\\alpha = 0.01\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYRdSqF43Jfy",
        "colab_type": "text"
      },
      "source": [
        "- Step 4 : Let’s perform some iterations of gradient descent:\n",
        "\n",
        "<figure>\n",
        "  <br><center>\n",
        "    <img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/gradient_descent_by_hand.png?raw=1\" />\n",
        "  </center>\n",
        "</figure>\n",
        "\n",
        "- Step 5 : We can observe that the X value is slowly decreasing and should converge to -5 (the local minimum). However, how many iterations should we perform?\n",
        "\n",
        "Let us set a precision variable in our algorithm which calculates the difference between two consecutive “$x$” values.\n",
        "\n",
        "If the difference between $x$ values from 2 consecutive iterations is lesser than the precision we set, stop the algorithm!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3NxGue83Jfz",
        "colab_type": "text"
      },
      "source": [
        "# Now for some Python \n",
        "\n",
        "**Usage NOTE!** Use `Shift+Enter` to step through this notebook, executing the code as you go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsllVvRg3Jf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f  = lambda x: (x + 5)**2  # The function we are trying to minimize (notice we don't actually use it!)\n",
        "df = lambda x: 2*(x+5)     # Gradient of our function\n",
        "\n",
        "rate      = 0.01  # Learning rate\n",
        "precision = 0.000001 # This tells us when to stop the algorithm\n",
        "previous_step_size = 1 # Initialize at 1\n",
        "max_iters = 10000 # Maximum number of iterations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfT4OCbW3Jf3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "29a5c900-e3a9-4f3f-968c-4e3191954a2d"
      },
      "source": [
        "cur_x     = 3 # The algorithm starts at x=3\n",
        "iters = 0 # Iteration counter\n",
        "while previous_step_size > precision and iters < max_iters:\n",
        "    prev_x = cur_x # Store current x value in prev_x\n",
        "    cur_x = cur_x - rate * df(prev_x) # Grad descent\n",
        "    previous_step_size = abs(cur_x - prev_x) # Change in x\n",
        "    iters = iters+1 # Iteration count\n",
        "    print(\"Iteration\",iters,\"\\nX value is\",cur_x) # Print iterations"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1 \n",
            "X value is 2.84\n",
            "Iteration 2 \n",
            "X value is 2.6832\n",
            "Iteration 3 \n",
            "X value is 2.529536\n",
            "Iteration 4 \n",
            "X value is 2.37894528\n",
            "Iteration 5 \n",
            "X value is 2.2313663744\n",
            "Iteration 6 \n",
            "X value is 2.0867390469119997\n",
            "Iteration 7 \n",
            "X value is 1.9450042659737599\n",
            "Iteration 8 \n",
            "X value is 1.8061041806542846\n",
            "Iteration 9 \n",
            "X value is 1.669982097041199\n",
            "Iteration 10 \n",
            "X value is 1.5365824551003748\n",
            "Iteration 11 \n",
            "X value is 1.4058508059983674\n",
            "Iteration 12 \n",
            "X value is 1.2777337898784\n",
            "Iteration 13 \n",
            "X value is 1.152179114080832\n",
            "Iteration 14 \n",
            "X value is 1.0291355317992152\n",
            "Iteration 15 \n",
            "X value is 0.9085528211632309\n",
            "Iteration 16 \n",
            "X value is 0.7903817647399662\n",
            "Iteration 17 \n",
            "X value is 0.6745741294451669\n",
            "Iteration 18 \n",
            "X value is 0.5610826468562635\n",
            "Iteration 19 \n",
            "X value is 0.44986099391913825\n",
            "Iteration 20 \n",
            "X value is 0.3408637740407555\n",
            "Iteration 21 \n",
            "X value is 0.23404649855994042\n",
            "Iteration 22 \n",
            "X value is 0.1293655685887416\n",
            "Iteration 23 \n",
            "X value is 0.026778257216966764\n",
            "Iteration 24 \n",
            "X value is -0.07375730792737258\n",
            "Iteration 25 \n",
            "X value is -0.1722821617688251\n",
            "Iteration 26 \n",
            "X value is -0.2688365185334486\n",
            "Iteration 27 \n",
            "X value is -0.36345978816277963\n",
            "Iteration 28 \n",
            "X value is -0.45619059239952403\n",
            "Iteration 29 \n",
            "X value is -0.5470667805515336\n",
            "Iteration 30 \n",
            "X value is -0.6361254449405029\n",
            "Iteration 31 \n",
            "X value is -0.7234029360416929\n",
            "Iteration 32 \n",
            "X value is -0.8089348773208591\n",
            "Iteration 33 \n",
            "X value is -0.8927561797744419\n",
            "Iteration 34 \n",
            "X value is -0.9749010561789531\n",
            "Iteration 35 \n",
            "X value is -1.055403035055374\n",
            "Iteration 36 \n",
            "X value is -1.1342949743542665\n",
            "Iteration 37 \n",
            "X value is -1.2116090748671813\n",
            "Iteration 38 \n",
            "X value is -1.2873768933698377\n",
            "Iteration 39 \n",
            "X value is -1.361629355502441\n",
            "Iteration 40 \n",
            "X value is -1.4343967683923922\n",
            "Iteration 41 \n",
            "X value is -1.5057088330245443\n",
            "Iteration 42 \n",
            "X value is -1.5755946563640535\n",
            "Iteration 43 \n",
            "X value is -1.6440827632367725\n",
            "Iteration 44 \n",
            "X value is -1.711201107972037\n",
            "Iteration 45 \n",
            "X value is -1.7769770858125964\n",
            "Iteration 46 \n",
            "X value is -1.8414375440963444\n",
            "Iteration 47 \n",
            "X value is -1.9046087932144176\n",
            "Iteration 48 \n",
            "X value is -1.9665166173501292\n",
            "Iteration 49 \n",
            "X value is -2.0271862850031264\n",
            "Iteration 50 \n",
            "X value is -2.0866425593030637\n",
            "Iteration 51 \n",
            "X value is -2.1449097081170025\n",
            "Iteration 52 \n",
            "X value is -2.2020115139546625\n",
            "Iteration 53 \n",
            "X value is -2.257971283675569\n",
            "Iteration 54 \n",
            "X value is -2.312811858002058\n",
            "Iteration 55 \n",
            "X value is -2.3665556208420164\n",
            "Iteration 56 \n",
            "X value is -2.419224508425176\n",
            "Iteration 57 \n",
            "X value is -2.4708400182566725\n",
            "Iteration 58 \n",
            "X value is -2.521423217891539\n",
            "Iteration 59 \n",
            "X value is -2.570994753533708\n",
            "Iteration 60 \n",
            "X value is -2.619574858463034\n",
            "Iteration 61 \n",
            "X value is -2.667183361293773\n",
            "Iteration 62 \n",
            "X value is -2.713839694067898\n",
            "Iteration 63 \n",
            "X value is -2.75956290018654\n",
            "Iteration 64 \n",
            "X value is -2.804371642182809\n",
            "Iteration 65 \n",
            "X value is -2.8482842093391527\n",
            "Iteration 66 \n",
            "X value is -2.8913185251523696\n",
            "Iteration 67 \n",
            "X value is -2.9334921546493224\n",
            "Iteration 68 \n",
            "X value is -2.974822311556336\n",
            "Iteration 69 \n",
            "X value is -3.015325865325209\n",
            "Iteration 70 \n",
            "X value is -3.055019348018705\n",
            "Iteration 71 \n",
            "X value is -3.093918961058331\n",
            "Iteration 72 \n",
            "X value is -3.1320405818371646\n",
            "Iteration 73 \n",
            "X value is -3.1693997702004215\n",
            "Iteration 74 \n",
            "X value is -3.206011774796413\n",
            "Iteration 75 \n",
            "X value is -3.2418915393004846\n",
            "Iteration 76 \n",
            "X value is -3.277053708514475\n",
            "Iteration 77 \n",
            "X value is -3.3115126343441856\n",
            "Iteration 78 \n",
            "X value is -3.345282381657302\n",
            "Iteration 79 \n",
            "X value is -3.378376734024156\n",
            "Iteration 80 \n",
            "X value is -3.4108091993436727\n",
            "Iteration 81 \n",
            "X value is -3.4425930153567994\n",
            "Iteration 82 \n",
            "X value is -3.4737411550496633\n",
            "Iteration 83 \n",
            "X value is -3.50426633194867\n",
            "Iteration 84 \n",
            "X value is -3.534181005309697\n",
            "Iteration 85 \n",
            "X value is -3.563497385203503\n",
            "Iteration 86 \n",
            "X value is -3.5922274374994325\n",
            "Iteration 87 \n",
            "X value is -3.620382888749444\n",
            "Iteration 88 \n",
            "X value is -3.6479752309744553\n",
            "Iteration 89 \n",
            "X value is -3.675015726354966\n",
            "Iteration 90 \n",
            "X value is -3.7015154118278666\n",
            "Iteration 91 \n",
            "X value is -3.7274851035913095\n",
            "Iteration 92 \n",
            "X value is -3.7529354015194833\n",
            "Iteration 93 \n",
            "X value is -3.7778766934890937\n",
            "Iteration 94 \n",
            "X value is -3.8023191596193118\n",
            "Iteration 95 \n",
            "X value is -3.8262727764269258\n",
            "Iteration 96 \n",
            "X value is -3.8497473208983872\n",
            "Iteration 97 \n",
            "X value is -3.8727523744804193\n",
            "Iteration 98 \n",
            "X value is -3.895297326990811\n",
            "Iteration 99 \n",
            "X value is -3.917391380450995\n",
            "Iteration 100 \n",
            "X value is -3.939043552841975\n",
            "Iteration 101 \n",
            "X value is -3.9602626817851356\n",
            "Iteration 102 \n",
            "X value is -3.981057428149433\n",
            "Iteration 103 \n",
            "X value is -4.001436279586445\n",
            "Iteration 104 \n",
            "X value is -4.021407553994716\n",
            "Iteration 105 \n",
            "X value is -4.040979402914822\n",
            "Iteration 106 \n",
            "X value is -4.060159814856525\n",
            "Iteration 107 \n",
            "X value is -4.078956618559395\n",
            "Iteration 108 \n",
            "X value is -4.097377486188207\n",
            "Iteration 109 \n",
            "X value is -4.115429936464443\n",
            "Iteration 110 \n",
            "X value is -4.133121337735154\n",
            "Iteration 111 \n",
            "X value is -4.150458910980451\n",
            "Iteration 112 \n",
            "X value is -4.167449732760842\n",
            "Iteration 113 \n",
            "X value is -4.1841007381056246\n",
            "Iteration 114 \n",
            "X value is -4.200418723343512\n",
            "Iteration 115 \n",
            "X value is -4.216410348876642\n",
            "Iteration 116 \n",
            "X value is -4.2320821418991095\n",
            "Iteration 117 \n",
            "X value is -4.247440499061128\n",
            "Iteration 118 \n",
            "X value is -4.262491689079905\n",
            "Iteration 119 \n",
            "X value is -4.277241855298307\n",
            "Iteration 120 \n",
            "X value is -4.291697018192341\n",
            "Iteration 121 \n",
            "X value is -4.305863077828494\n",
            "Iteration 122 \n",
            "X value is -4.319745816271924\n",
            "Iteration 123 \n",
            "X value is -4.333350899946486\n",
            "Iteration 124 \n",
            "X value is -4.3466838819475555\n",
            "Iteration 125 \n",
            "X value is -4.359750204308605\n",
            "Iteration 126 \n",
            "X value is -4.372555200222433\n",
            "Iteration 127 \n",
            "X value is -4.385104096217984\n",
            "Iteration 128 \n",
            "X value is -4.3974020142936245\n",
            "Iteration 129 \n",
            "X value is -4.409453974007752\n",
            "Iteration 130 \n",
            "X value is -4.421264894527597\n",
            "Iteration 131 \n",
            "X value is -4.432839596637045\n",
            "Iteration 132 \n",
            "X value is -4.444182804704305\n",
            "Iteration 133 \n",
            "X value is -4.4552991486102185\n",
            "Iteration 134 \n",
            "X value is -4.466193165638014\n",
            "Iteration 135 \n",
            "X value is -4.4768693023252535\n",
            "Iteration 136 \n",
            "X value is -4.487331916278748\n",
            "Iteration 137 \n",
            "X value is -4.497585277953173\n",
            "Iteration 138 \n",
            "X value is -4.50763357239411\n",
            "Iteration 139 \n",
            "X value is -4.517480900946228\n",
            "Iteration 140 \n",
            "X value is -4.527131282927304\n",
            "Iteration 141 \n",
            "X value is -4.536588657268758\n",
            "Iteration 142 \n",
            "X value is -4.545856884123382\n",
            "Iteration 143 \n",
            "X value is -4.5549397464409145\n",
            "Iteration 144 \n",
            "X value is -4.563840951512097\n",
            "Iteration 145 \n",
            "X value is -4.572564132481855\n",
            "Iteration 146 \n",
            "X value is -4.581112849832218\n",
            "Iteration 147 \n",
            "X value is -4.589490592835574\n",
            "Iteration 148 \n",
            "X value is -4.597700780978863\n",
            "Iteration 149 \n",
            "X value is -4.605746765359285\n",
            "Iteration 150 \n",
            "X value is -4.6136318300521\n",
            "Iteration 151 \n",
            "X value is -4.621359193451058\n",
            "Iteration 152 \n",
            "X value is -4.628932009582036\n",
            "Iteration 153 \n",
            "X value is -4.636353369390395\n",
            "Iteration 154 \n",
            "X value is -4.643626302002588\n",
            "Iteration 155 \n",
            "X value is -4.650753775962536\n",
            "Iteration 156 \n",
            "X value is -4.657738700443285\n",
            "Iteration 157 \n",
            "X value is -4.664583926434419\n",
            "Iteration 158 \n",
            "X value is -4.671292247905731\n",
            "Iteration 159 \n",
            "X value is -4.6778664029476165\n",
            "Iteration 160 \n",
            "X value is -4.684309074888664\n",
            "Iteration 161 \n",
            "X value is -4.6906228933908904\n",
            "Iteration 162 \n",
            "X value is -4.696810435523073\n",
            "Iteration 163 \n",
            "X value is -4.702874226812612\n",
            "Iteration 164 \n",
            "X value is -4.708816742276359\n",
            "Iteration 165 \n",
            "X value is -4.714640407430832\n",
            "Iteration 166 \n",
            "X value is -4.720347599282215\n",
            "Iteration 167 \n",
            "X value is -4.725940647296571\n",
            "Iteration 168 \n",
            "X value is -4.731421834350639\n",
            "Iteration 169 \n",
            "X value is -4.736793397663627\n",
            "Iteration 170 \n",
            "X value is -4.742057529710355\n",
            "Iteration 171 \n",
            "X value is -4.747216379116147\n",
            "Iteration 172 \n",
            "X value is -4.752272051533824\n",
            "Iteration 173 \n",
            "X value is -4.757226610503148\n",
            "Iteration 174 \n",
            "X value is -4.762082078293084\n",
            "Iteration 175 \n",
            "X value is -4.766840436727223\n",
            "Iteration 176 \n",
            "X value is -4.771503627992678\n",
            "Iteration 177 \n",
            "X value is -4.776073555432824\n",
            "Iteration 178 \n",
            "X value is -4.780552084324168\n",
            "Iteration 179 \n",
            "X value is -4.784941042637685\n",
            "Iteration 180 \n",
            "X value is -4.7892422217849315\n",
            "Iteration 181 \n",
            "X value is -4.793457377349233\n",
            "Iteration 182 \n",
            "X value is -4.7975882298022485\n",
            "Iteration 183 \n",
            "X value is -4.801636465206204\n",
            "Iteration 184 \n",
            "X value is -4.805603735902079\n",
            "Iteration 185 \n",
            "X value is -4.809491661184038\n",
            "Iteration 186 \n",
            "X value is -4.813301827960357\n",
            "Iteration 187 \n",
            "X value is -4.81703579140115\n",
            "Iteration 188 \n",
            "X value is -4.820695075573127\n",
            "Iteration 189 \n",
            "X value is -4.824281174061665\n",
            "Iteration 190 \n",
            "X value is -4.827795550580431\n",
            "Iteration 191 \n",
            "X value is -4.831239639568823\n",
            "Iteration 192 \n",
            "X value is -4.834614846777447\n",
            "Iteration 193 \n",
            "X value is -4.837922549841898\n",
            "Iteration 194 \n",
            "X value is -4.84116409884506\n",
            "Iteration 195 \n",
            "X value is -4.844340816868159\n",
            "Iteration 196 \n",
            "X value is -4.847454000530796\n",
            "Iteration 197 \n",
            "X value is -4.85050492052018\n",
            "Iteration 198 \n",
            "X value is -4.853494822109776\n",
            "Iteration 199 \n",
            "X value is -4.85642492566758\n",
            "Iteration 200 \n",
            "X value is -4.859296427154229\n",
            "Iteration 201 \n",
            "X value is -4.862110498611145\n",
            "Iteration 202 \n",
            "X value is -4.864868288638922\n",
            "Iteration 203 \n",
            "X value is -4.867570922866143\n",
            "Iteration 204 \n",
            "X value is -4.87021950440882\n",
            "Iteration 205 \n",
            "X value is -4.872815114320644\n",
            "Iteration 206 \n",
            "X value is -4.875358812034231\n",
            "Iteration 207 \n",
            "X value is -4.877851635793546\n",
            "Iteration 208 \n",
            "X value is -4.880294603077676\n",
            "Iteration 209 \n",
            "X value is -4.882688711016122\n",
            "Iteration 210 \n",
            "X value is -4.8850349367958\n",
            "Iteration 211 \n",
            "X value is -4.887334238059884\n",
            "Iteration 212 \n",
            "X value is -4.8895875532986866\n",
            "Iteration 213 \n",
            "X value is -4.891795802232712\n",
            "Iteration 214 \n",
            "X value is -4.893959886188058\n",
            "Iteration 215 \n",
            "X value is -4.896080688464297\n",
            "Iteration 216 \n",
            "X value is -4.898159074695011\n",
            "Iteration 217 \n",
            "X value is -4.9001958932011105\n",
            "Iteration 218 \n",
            "X value is -4.902191975337089\n",
            "Iteration 219 \n",
            "X value is -4.904148135830347\n",
            "Iteration 220 \n",
            "X value is -4.90606517311374\n",
            "Iteration 221 \n",
            "X value is -4.907943869651465\n",
            "Iteration 222 \n",
            "X value is -4.909784992258436\n",
            "Iteration 223 \n",
            "X value is -4.911589292413267\n",
            "Iteration 224 \n",
            "X value is -4.913357506565002\n",
            "Iteration 225 \n",
            "X value is -4.915090356433702\n",
            "Iteration 226 \n",
            "X value is -4.9167885493050285\n",
            "Iteration 227 \n",
            "X value is -4.918452778318928\n",
            "Iteration 228 \n",
            "X value is -4.920083722752549\n",
            "Iteration 229 \n",
            "X value is -4.921682048297498\n",
            "Iteration 230 \n",
            "X value is -4.923248407331548\n",
            "Iteration 231 \n",
            "X value is -4.9247834391849175\n",
            "Iteration 232 \n",
            "X value is -4.926287770401219\n",
            "Iteration 233 \n",
            "X value is -4.927762014993195\n",
            "Iteration 234 \n",
            "X value is -4.929206774693331\n",
            "Iteration 235 \n",
            "X value is -4.930622639199464\n",
            "Iteration 236 \n",
            "X value is -4.932010186415474\n",
            "Iteration 237 \n",
            "X value is -4.933369982687164\n",
            "Iteration 238 \n",
            "X value is -4.934702583033421\n",
            "Iteration 239 \n",
            "X value is -4.936008531372753\n",
            "Iteration 240 \n",
            "X value is -4.937288360745298\n",
            "Iteration 241 \n",
            "X value is -4.938542593530392\n",
            "Iteration 242 \n",
            "X value is -4.939771741659784\n",
            "Iteration 243 \n",
            "X value is -4.940976306826588\n",
            "Iteration 244 \n",
            "X value is -4.942156780690056\n",
            "Iteration 245 \n",
            "X value is -4.943313645076255\n",
            "Iteration 246 \n",
            "X value is -4.94444737217473\n",
            "Iteration 247 \n",
            "X value is -4.945558424731236\n",
            "Iteration 248 \n",
            "X value is -4.946647256236611\n",
            "Iteration 249 \n",
            "X value is -4.947714311111879\n",
            "Iteration 250 \n",
            "X value is -4.9487600248896415\n",
            "Iteration 251 \n",
            "X value is -4.949784824391848\n",
            "Iteration 252 \n",
            "X value is -4.950789127904011\n",
            "Iteration 253 \n",
            "X value is -4.951773345345931\n",
            "Iteration 254 \n",
            "X value is -4.952737878439012\n",
            "Iteration 255 \n",
            "X value is -4.953683120870232\n",
            "Iteration 256 \n",
            "X value is -4.954609458452827\n",
            "Iteration 257 \n",
            "X value is -4.955517269283771\n",
            "Iteration 258 \n",
            "X value is -4.956406923898095\n",
            "Iteration 259 \n",
            "X value is -4.957278785420133\n",
            "Iteration 260 \n",
            "X value is -4.958133209711731\n",
            "Iteration 261 \n",
            "X value is -4.958970545517496\n",
            "Iteration 262 \n",
            "X value is -4.959791134607146\n",
            "Iteration 263 \n",
            "X value is -4.960595311915003\n",
            "Iteration 264 \n",
            "X value is -4.9613834056767026\n",
            "Iteration 265 \n",
            "X value is -4.962155737563169\n",
            "Iteration 266 \n",
            "X value is -4.962912622811905\n",
            "Iteration 267 \n",
            "X value is -4.963654370355667\n",
            "Iteration 268 \n",
            "X value is -4.964381282948554\n",
            "Iteration 269 \n",
            "X value is -4.965093657289583\n",
            "Iteration 270 \n",
            "X value is -4.965791784143791\n",
            "Iteration 271 \n",
            "X value is -4.966475948460915\n",
            "Iteration 272 \n",
            "X value is -4.967146429491697\n",
            "Iteration 273 \n",
            "X value is -4.967803500901863\n",
            "Iteration 274 \n",
            "X value is -4.968447430883826\n",
            "Iteration 275 \n",
            "X value is -4.969078482266149\n",
            "Iteration 276 \n",
            "X value is -4.969696912620826\n",
            "Iteration 277 \n",
            "X value is -4.970302974368409\n",
            "Iteration 278 \n",
            "X value is -4.970896914881041\n",
            "Iteration 279 \n",
            "X value is -4.97147897658342\n",
            "Iteration 280 \n",
            "X value is -4.972049397051752\n",
            "Iteration 281 \n",
            "X value is -4.972608409110717\n",
            "Iteration 282 \n",
            "X value is -4.973156240928502\n",
            "Iteration 283 \n",
            "X value is -4.973693116109932\n",
            "Iteration 284 \n",
            "X value is -4.974219253787734\n",
            "Iteration 285 \n",
            "X value is -4.974734868711979\n",
            "Iteration 286 \n",
            "X value is -4.975240171337739\n",
            "Iteration 287 \n",
            "X value is -4.975735367910985\n",
            "Iteration 288 \n",
            "X value is -4.976220660552765\n",
            "Iteration 289 \n",
            "X value is -4.976696247341709\n",
            "Iteration 290 \n",
            "X value is -4.977162322394875\n",
            "Iteration 291 \n",
            "X value is -4.977619075946977\n",
            "Iteration 292 \n",
            "X value is -4.978066694428038\n",
            "Iteration 293 \n",
            "X value is -4.978505360539477\n",
            "Iteration 294 \n",
            "X value is -4.978935253328687\n",
            "Iteration 295 \n",
            "X value is -4.979356548262113\n",
            "Iteration 296 \n",
            "X value is -4.979769417296871\n",
            "Iteration 297 \n",
            "X value is -4.980174028950934\n",
            "Iteration 298 \n",
            "X value is -4.980570548371915\n",
            "Iteration 299 \n",
            "X value is -4.980959137404477\n",
            "Iteration 300 \n",
            "X value is -4.981339954656387\n",
            "Iteration 301 \n",
            "X value is -4.981713155563259\n",
            "Iteration 302 \n",
            "X value is -4.982078892451994\n",
            "Iteration 303 \n",
            "X value is -4.9824373146029535\n",
            "Iteration 304 \n",
            "X value is -4.982788568310895\n",
            "Iteration 305 \n",
            "X value is -4.983132796944677\n",
            "Iteration 306 \n",
            "X value is -4.983470141005784\n",
            "Iteration 307 \n",
            "X value is -4.983800738185668\n",
            "Iteration 308 \n",
            "X value is -4.984124723421955\n",
            "Iteration 309 \n",
            "X value is -4.984442228953515\n",
            "Iteration 310 \n",
            "X value is -4.984753384374445\n",
            "Iteration 311 \n",
            "X value is -4.985058316686956\n",
            "Iteration 312 \n",
            "X value is -4.9853571503532175\n",
            "Iteration 313 \n",
            "X value is -4.985650007346153\n",
            "Iteration 314 \n",
            "X value is -4.9859370071992295\n",
            "Iteration 315 \n",
            "X value is -4.986218267055245\n",
            "Iteration 316 \n",
            "X value is -4.98649390171414\n",
            "Iteration 317 \n",
            "X value is -4.986764023679857\n",
            "Iteration 318 \n",
            "X value is -4.98702874320626\n",
            "Iteration 319 \n",
            "X value is -4.987288168342134\n",
            "Iteration 320 \n",
            "X value is -4.987542404975292\n",
            "Iteration 321 \n",
            "X value is -4.987791556875786\n",
            "Iteration 322 \n",
            "X value is -4.98803572573827\n",
            "Iteration 323 \n",
            "X value is -4.988275011223505\n",
            "Iteration 324 \n",
            "X value is -4.988509510999035\n",
            "Iteration 325 \n",
            "X value is -4.988739320779054\n",
            "Iteration 326 \n",
            "X value is -4.988964534363473\n",
            "Iteration 327 \n",
            "X value is -4.989185243676204\n",
            "Iteration 328 \n",
            "X value is -4.98940153880268\n",
            "Iteration 329 \n",
            "X value is -4.989613508026626\n",
            "Iteration 330 \n",
            "X value is -4.989821237866094\n",
            "Iteration 331 \n",
            "X value is -4.990024813108772\n",
            "Iteration 332 \n",
            "X value is -4.9902243168465965\n",
            "Iteration 333 \n",
            "X value is -4.990419830509665\n",
            "Iteration 334 \n",
            "X value is -4.990611433899471\n",
            "Iteration 335 \n",
            "X value is -4.990799205221482\n",
            "Iteration 336 \n",
            "X value is -4.990983221117052\n",
            "Iteration 337 \n",
            "X value is -4.991163556694711\n",
            "Iteration 338 \n",
            "X value is -4.991340285560817\n",
            "Iteration 339 \n",
            "X value is -4.9915134798496\n",
            "Iteration 340 \n",
            "X value is -4.991683210252608\n",
            "Iteration 341 \n",
            "X value is -4.991849546047556\n",
            "Iteration 342 \n",
            "X value is -4.992012555126605\n",
            "Iteration 343 \n",
            "X value is -4.992172304024073\n",
            "Iteration 344 \n",
            "X value is -4.992328857943591\n",
            "Iteration 345 \n",
            "X value is -4.99248228078472\n",
            "Iteration 346 \n",
            "X value is -4.992632635169025\n",
            "Iteration 347 \n",
            "X value is -4.9927799824656445\n",
            "Iteration 348 \n",
            "X value is -4.992924382816332\n",
            "Iteration 349 \n",
            "X value is -4.993065895160005\n",
            "Iteration 350 \n",
            "X value is -4.993204577256805\n",
            "Iteration 351 \n",
            "X value is -4.993340485711669\n",
            "Iteration 352 \n",
            "X value is -4.993473675997436\n",
            "Iteration 353 \n",
            "X value is -4.993604202477487\n",
            "Iteration 354 \n",
            "X value is -4.993732118427937\n",
            "Iteration 355 \n",
            "X value is -4.993857476059379\n",
            "Iteration 356 \n",
            "X value is -4.993980326538191\n",
            "Iteration 357 \n",
            "X value is -4.9941007200074266\n",
            "Iteration 358 \n",
            "X value is -4.994218705607278\n",
            "Iteration 359 \n",
            "X value is -4.994334331495133\n",
            "Iteration 360 \n",
            "X value is -4.994447644865231\n",
            "Iteration 361 \n",
            "X value is -4.994558691967926\n",
            "Iteration 362 \n",
            "X value is -4.994667518128567\n",
            "Iteration 363 \n",
            "X value is -4.994774167765996\n",
            "Iteration 364 \n",
            "X value is -4.9948786844106765\n",
            "Iteration 365 \n",
            "X value is -4.994981110722463\n",
            "Iteration 366 \n",
            "X value is -4.995081488508014\n",
            "Iteration 367 \n",
            "X value is -4.995179858737854\n",
            "Iteration 368 \n",
            "X value is -4.995276261563097\n",
            "Iteration 369 \n",
            "X value is -4.995370736331835\n",
            "Iteration 370 \n",
            "X value is -4.9954633216051985\n",
            "Iteration 371 \n",
            "X value is -4.995554055173095\n",
            "Iteration 372 \n",
            "X value is -4.995642974069633\n",
            "Iteration 373 \n",
            "X value is -4.99573011458824\n",
            "Iteration 374 \n",
            "X value is -4.995815512296476\n",
            "Iteration 375 \n",
            "X value is -4.995899202050547\n",
            "Iteration 376 \n",
            "X value is -4.995981218009535\n",
            "Iteration 377 \n",
            "X value is -4.996061593649345\n",
            "Iteration 378 \n",
            "X value is -4.996140361776358\n",
            "Iteration 379 \n",
            "X value is -4.996217554540831\n",
            "Iteration 380 \n",
            "X value is -4.996293203450014\n",
            "Iteration 381 \n",
            "X value is -4.996367339381013\n",
            "Iteration 382 \n",
            "X value is -4.996439992593393\n",
            "Iteration 383 \n",
            "X value is -4.996511192741525\n",
            "Iteration 384 \n",
            "X value is -4.996580968886694\n",
            "Iteration 385 \n",
            "X value is -4.99664934950896\n",
            "Iteration 386 \n",
            "X value is -4.9967163625187805\n",
            "Iteration 387 \n",
            "X value is -4.996782035268405\n",
            "Iteration 388 \n",
            "X value is -4.996846394563037\n",
            "Iteration 389 \n",
            "X value is -4.996909466671776\n",
            "Iteration 390 \n",
            "X value is -4.996971277338341\n",
            "Iteration 391 \n",
            "X value is -4.997031851791574\n",
            "Iteration 392 \n",
            "X value is -4.997091214755742\n",
            "Iteration 393 \n",
            "X value is -4.997149390460628\n",
            "Iteration 394 \n",
            "X value is -4.997206402651415\n",
            "Iteration 395 \n",
            "X value is -4.997262274598387\n",
            "Iteration 396 \n",
            "X value is -4.997317029106419\n",
            "Iteration 397 \n",
            "X value is -4.997370688524291\n",
            "Iteration 398 \n",
            "X value is -4.997423274753805\n",
            "Iteration 399 \n",
            "X value is -4.997474809258729\n",
            "Iteration 400 \n",
            "X value is -4.997525313073554\n",
            "Iteration 401 \n",
            "X value is -4.997574806812083\n",
            "Iteration 402 \n",
            "X value is -4.997623310675841\n",
            "Iteration 403 \n",
            "X value is -4.997670844462324\n",
            "Iteration 404 \n",
            "X value is -4.997717427573078\n",
            "Iteration 405 \n",
            "X value is -4.997763079021617\n",
            "Iteration 406 \n",
            "X value is -4.997807817441185\n",
            "Iteration 407 \n",
            "X value is -4.997851661092361\n",
            "Iteration 408 \n",
            "X value is -4.997894627870514\n",
            "Iteration 409 \n",
            "X value is -4.997936735313104\n",
            "Iteration 410 \n",
            "X value is -4.9979780006068415\n",
            "Iteration 411 \n",
            "X value is -4.998018440594705\n",
            "Iteration 412 \n",
            "X value is -4.998058071782811\n",
            "Iteration 413 \n",
            "X value is -4.998096910347155\n",
            "Iteration 414 \n",
            "X value is -4.998134972140212\n",
            "Iteration 415 \n",
            "X value is -4.998172272697408\n",
            "Iteration 416 \n",
            "X value is -4.9982088272434595\n",
            "Iteration 417 \n",
            "X value is -4.998244650698591\n",
            "Iteration 418 \n",
            "X value is -4.998279757684619\n",
            "Iteration 419 \n",
            "X value is -4.998314162530927\n",
            "Iteration 420 \n",
            "X value is -4.998347879280309\n",
            "Iteration 421 \n",
            "X value is -4.998380921694703\n",
            "Iteration 422 \n",
            "X value is -4.998413303260809\n",
            "Iteration 423 \n",
            "X value is -4.998445037195593\n",
            "Iteration 424 \n",
            "X value is -4.998476136451681\n",
            "Iteration 425 \n",
            "X value is -4.998506613722648\n",
            "Iteration 426 \n",
            "X value is -4.998536481448195\n",
            "Iteration 427 \n",
            "X value is -4.998565751819231\n",
            "Iteration 428 \n",
            "X value is -4.998594436782846\n",
            "Iteration 429 \n",
            "X value is -4.998622548047189\n",
            "Iteration 430 \n",
            "X value is -4.998650097086245\n",
            "Iteration 431 \n",
            "X value is -4.9986770951445205\n",
            "Iteration 432 \n",
            "X value is -4.99870355324163\n",
            "Iteration 433 \n",
            "X value is -4.998729482176797\n",
            "Iteration 434 \n",
            "X value is -4.998754892533261\n",
            "Iteration 435 \n",
            "X value is -4.998779794682596\n",
            "Iteration 436 \n",
            "X value is -4.998804198788944\n",
            "Iteration 437 \n",
            "X value is -4.998828114813166\n",
            "Iteration 438 \n",
            "X value is -4.998851552516903\n",
            "Iteration 439 \n",
            "X value is -4.998874521466565\n",
            "Iteration 440 \n",
            "X value is -4.998897031037234\n",
            "Iteration 441 \n",
            "X value is -4.998919090416489\n",
            "Iteration 442 \n",
            "X value is -4.99894070860816\n",
            "Iteration 443 \n",
            "X value is -4.998961894435997\n",
            "Iteration 444 \n",
            "X value is -4.998982656547277\n",
            "Iteration 445 \n",
            "X value is -4.999003003416331\n",
            "Iteration 446 \n",
            "X value is -4.999022943348004\n",
            "Iteration 447 \n",
            "X value is -4.999042484481044\n",
            "Iteration 448 \n",
            "X value is -4.999061634791423\n",
            "Iteration 449 \n",
            "X value is -4.999080402095594\n",
            "Iteration 450 \n",
            "X value is -4.999098794053682\n",
            "Iteration 451 \n",
            "X value is -4.999116818172609\n",
            "Iteration 452 \n",
            "X value is -4.999134481809157\n",
            "Iteration 453 \n",
            "X value is -4.999151792172974\n",
            "Iteration 454 \n",
            "X value is -4.999168756329515\n",
            "Iteration 455 \n",
            "X value is -4.999185381202924\n",
            "Iteration 456 \n",
            "X value is -4.999201673578866\n",
            "Iteration 457 \n",
            "X value is -4.999217640107289\n",
            "Iteration 458 \n",
            "X value is -4.999233287305143\n",
            "Iteration 459 \n",
            "X value is -4.9992486215590395\n",
            "Iteration 460 \n",
            "X value is -4.999263649127859\n",
            "Iteration 461 \n",
            "X value is -4.999278376145302\n",
            "Iteration 462 \n",
            "X value is -4.999292808622396\n",
            "Iteration 463 \n",
            "X value is -4.999306952449948\n",
            "Iteration 464 \n",
            "X value is -4.999320813400949\n",
            "Iteration 465 \n",
            "X value is -4.99933439713293\n",
            "Iteration 466 \n",
            "X value is -4.999347709190272\n",
            "Iteration 467 \n",
            "X value is -4.9993607550064665\n",
            "Iteration 468 \n",
            "X value is -4.999373539906337\n",
            "Iteration 469 \n",
            "X value is -4.99938606910821\n",
            "Iteration 470 \n",
            "X value is -4.9993983477260455\n",
            "Iteration 471 \n",
            "X value is -4.999410380771525\n",
            "Iteration 472 \n",
            "X value is -4.999422173156094\n",
            "Iteration 473 \n",
            "X value is -4.9994337296929725\n",
            "Iteration 474 \n",
            "X value is -4.999445055099113\n",
            "Iteration 475 \n",
            "X value is -4.999456153997131\n",
            "Iteration 476 \n",
            "X value is -4.999467030917188\n",
            "Iteration 477 \n",
            "X value is -4.9994776902988445\n",
            "Iteration 478 \n",
            "X value is -4.999488136492867\n",
            "Iteration 479 \n",
            "X value is -4.99949837376301\n",
            "Iteration 480 \n",
            "X value is -4.99950840628775\n",
            "Iteration 481 \n",
            "X value is -4.999518238161995\n",
            "Iteration 482 \n",
            "X value is -4.999527873398756\n",
            "Iteration 483 \n",
            "X value is -4.99953731593078\n",
            "Iteration 484 \n",
            "X value is -4.999546569612165\n",
            "Iteration 485 \n",
            "X value is -4.999555638219921\n",
            "Iteration 486 \n",
            "X value is -4.999564525455523\n",
            "Iteration 487 \n",
            "X value is -4.999573234946412\n",
            "Iteration 488 \n",
            "X value is -4.9995817702474845\n",
            "Iteration 489 \n",
            "X value is -4.999590134842535\n",
            "Iteration 490 \n",
            "X value is -4.999598332145684\n",
            "Iteration 491 \n",
            "X value is -4.99960636550277\n",
            "Iteration 492 \n",
            "X value is -4.999614238192715\n",
            "Iteration 493 \n",
            "X value is -4.999621953428861\n",
            "Iteration 494 \n",
            "X value is -4.999629514360284\n",
            "Iteration 495 \n",
            "X value is -4.999636924073078\n",
            "Iteration 496 \n",
            "X value is -4.999644185591617\n",
            "Iteration 497 \n",
            "X value is -4.999651301879784\n",
            "Iteration 498 \n",
            "X value is -4.999658275842188\n",
            "Iteration 499 \n",
            "X value is -4.999665110325345\n",
            "Iteration 500 \n",
            "X value is -4.999671808118838\n",
            "Iteration 501 \n",
            "X value is -4.9996783719564615\n",
            "Iteration 502 \n",
            "X value is -4.999684804517332\n",
            "Iteration 503 \n",
            "X value is -4.999691108426985\n",
            "Iteration 504 \n",
            "X value is -4.999697286258446\n",
            "Iteration 505 \n",
            "X value is -4.9997033405332765\n",
            "Iteration 506 \n",
            "X value is -4.999709273722611\n",
            "Iteration 507 \n",
            "X value is -4.999715088248159\n",
            "Iteration 508 \n",
            "X value is -4.999720786483196\n",
            "Iteration 509 \n",
            "X value is -4.999726370753532\n",
            "Iteration 510 \n",
            "X value is -4.999731843338461\n",
            "Iteration 511 \n",
            "X value is -4.999737206471692\n",
            "Iteration 512 \n",
            "X value is -4.999742462342258\n",
            "Iteration 513 \n",
            "X value is -4.999747613095413\n",
            "Iteration 514 \n",
            "X value is -4.999752660833504\n",
            "Iteration 515 \n",
            "X value is -4.999757607616834\n",
            "Iteration 516 \n",
            "X value is -4.999762455464498\n",
            "Iteration 517 \n",
            "X value is -4.999767206355208\n",
            "Iteration 518 \n",
            "X value is -4.999771862228104\n",
            "Iteration 519 \n",
            "X value is -4.999776424983542\n",
            "Iteration 520 \n",
            "X value is -4.9997808964838715\n",
            "Iteration 521 \n",
            "X value is -4.999785278554194\n",
            "Iteration 522 \n",
            "X value is -4.9997895729831106\n",
            "Iteration 523 \n",
            "X value is -4.999793781523448\n",
            "Iteration 524 \n",
            "X value is -4.999797905892979\n",
            "Iteration 525 \n",
            "X value is -4.999801947775119\n",
            "Iteration 526 \n",
            "X value is -4.999805908819617\n",
            "Iteration 527 \n",
            "X value is -4.999809790643225\n",
            "Iteration 528 \n",
            "X value is -4.99981359483036\n",
            "Iteration 529 \n",
            "X value is -4.999817322933753\n",
            "Iteration 530 \n",
            "X value is -4.999820976475077\n",
            "Iteration 531 \n",
            "X value is -4.999824556945576\n",
            "Iteration 532 \n",
            "X value is -4.999828065806665\n",
            "Iteration 533 \n",
            "X value is -4.9998315044905315\n",
            "Iteration 534 \n",
            "X value is -4.999834874400721\n",
            "Iteration 535 \n",
            "X value is -4.999838176912706\n",
            "Iteration 536 \n",
            "X value is -4.999841413374452\n",
            "Iteration 537 \n",
            "X value is -4.999844585106963\n",
            "Iteration 538 \n",
            "X value is -4.999847693404824\n",
            "Iteration 539 \n",
            "X value is -4.999850739536727\n",
            "Iteration 540 \n",
            "X value is -4.999853724745993\n",
            "Iteration 541 \n",
            "X value is -4.999856650251073\n",
            "Iteration 542 \n",
            "X value is -4.999859517246051\n",
            "Iteration 543 \n",
            "X value is -4.99986232690113\n",
            "Iteration 544 \n",
            "X value is -4.999865080363108\n",
            "Iteration 545 \n",
            "X value is -4.999867778755846\n",
            "Iteration 546 \n",
            "X value is -4.999870423180729\n",
            "Iteration 547 \n",
            "X value is -4.999873014717115\n",
            "Iteration 548 \n",
            "X value is -4.999875554422772\n",
            "Iteration 549 \n",
            "X value is -4.999878043334316\n",
            "Iteration 550 \n",
            "X value is -4.99988048246763\n",
            "Iteration 551 \n",
            "X value is -4.999882872818278\n",
            "Iteration 552 \n",
            "X value is -4.999885215361912\n",
            "Iteration 553 \n",
            "X value is -4.999887511054674\n",
            "Iteration 554 \n",
            "X value is -4.999889760833581\n",
            "Iteration 555 \n",
            "X value is -4.999891965616909\n",
            "Iteration 556 \n",
            "X value is -4.999894126304571\n",
            "Iteration 557 \n",
            "X value is -4.999896243778479\n",
            "Iteration 558 \n",
            "X value is -4.999898318902909\n",
            "Iteration 559 \n",
            "X value is -4.999900352524851\n",
            "Iteration 560 \n",
            "X value is -4.9999023454743545\n",
            "Iteration 561 \n",
            "X value is -4.999904298564868\n",
            "Iteration 562 \n",
            "X value is -4.9999062125935705\n",
            "Iteration 563 \n",
            "X value is -4.999908088341699\n",
            "Iteration 564 \n",
            "X value is -4.9999099265748645\n",
            "Iteration 565 \n",
            "X value is -4.999911728043367\n",
            "Iteration 566 \n",
            "X value is -4.9999134934825\n",
            "Iteration 567 \n",
            "X value is -4.99991522361285\n",
            "Iteration 568 \n",
            "X value is -4.999916919140593\n",
            "Iteration 569 \n",
            "X value is -4.999918580757781\n",
            "Iteration 570 \n",
            "X value is -4.999920209142625\n",
            "Iteration 571 \n",
            "X value is -4.999921804959773\n",
            "Iteration 572 \n",
            "X value is -4.9999233688605775\n",
            "Iteration 573 \n",
            "X value is -4.999924901483366\n",
            "Iteration 574 \n",
            "X value is -4.999926403453699\n",
            "Iteration 575 \n",
            "X value is -4.999927875384625\n",
            "Iteration 576 \n",
            "X value is -4.999929317876933\n",
            "Iteration 577 \n",
            "X value is -4.999930731519394\n",
            "Iteration 578 \n",
            "X value is -4.999932116889006\n",
            "Iteration 579 \n",
            "X value is -4.999933474551226\n",
            "Iteration 580 \n",
            "X value is -4.999934805060202\n",
            "Iteration 581 \n",
            "X value is -4.999936108958998\n",
            "Iteration 582 \n",
            "X value is -4.999937386779818\n",
            "Iteration 583 \n",
            "X value is -4.999938639044221\n",
            "Iteration 584 \n",
            "X value is -4.999939866263337\n",
            "Iteration 585 \n",
            "X value is -4.99994106893807\n",
            "Iteration 586 \n",
            "X value is -4.999942247559309\n",
            "Iteration 587 \n",
            "X value is -4.999943402608123\n",
            "Iteration 588 \n",
            "X value is -4.9999445345559606\n",
            "Iteration 589 \n",
            "X value is -4.999945643864842\n",
            "Iteration 590 \n",
            "X value is -4.999946730987545\n",
            "Iteration 591 \n",
            "X value is -4.999947796367794\n",
            "Iteration 592 \n",
            "X value is -4.999948840440438\n",
            "Iteration 593 \n",
            "X value is -4.999949863631629\n",
            "Iteration 594 \n",
            "X value is -4.999950866358997\n",
            "Iteration 595 \n",
            "X value is -4.9999518490318176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p36Aatkq3Jf6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b20c4f81-8e59-40b2-8360-971e0e855668"
      },
      "source": [
        "print(f\"The local minimum occurs at {cur_x:0.4f}\")\n",
        "print(f\"The number of iterations to converge was {iters}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The local minimum occurs at -5.0000\n",
            "The number of iterations to converge was 595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN6Ju-g83Jf9",
        "colab_type": "text"
      },
      "source": [
        "### Output\n",
        "\n",
        "From the output above, we can observe the $x$ values for the first 10 iterations- which can be cross checked with our manual calculations. The algorithm runs for 595 iterations before it terminates. \n",
        "\n",
        "**Well that was fun.  Let's try another one.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBbnG0z33Jf9",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Descent in Python -- with Graphs!\n",
        "\n",
        "Here, we will implement a simple representation of gradient descent using python.\n",
        "We will create an arbitrary loss function and attempt to find a local minimum value for that function.\n",
        "\n",
        "Our function $f(x)$ will be :\n",
        "    \n",
        "$$f(x) = x^3 - 3x^2 + 7$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M88W-aNQ-s6s",
        "outputId": "37bb7ecd-3139-4867-e9fe-3d7e98de20e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# Create the function and plot it \n",
        "function = lambda x: (x ** 3)-(3 *(x ** 2))+7\n",
        "\n",
        "# Get 1000 evenly spaced numbers between -1 and 3 (arbitrarily chosen to ensure steep curve)\n",
        "x = np.linspace(-1,3,500)\n",
        "\n",
        "# Plot the curve\n",
        "plt.plot(x, function(x))\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSik/9P8BWZoT/tOdfdQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aqbsWYCjGfcv"
      },
      "source": [
        "Note that while this function is defined for $x$ in $[-\\infty, +\\infty]$ it only has a local minimum from $x$ in $[0.0, 2.0]$.\n",
        "\n",
        "We will then proceed to make two functions for the gradient descent implementation:\n",
        "\n",
        "- The first is a derivative function: This function takes in a value of x and returns its derivative based on the initial function we specified. It is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oqAKquEK-vis",
        "colab": {}
      },
      "source": [
        "def deriv(x):\n",
        "    \n",
        "    '''\n",
        "    Description: This function takes in a value of x and returns its derivative based on the \n",
        "    initial function we specified.\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    x - a numerical value of x \n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    x_deriv - a numerical value of the derivative of x\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    x_deriv = 3* (x**2) - (6 * (x))\n",
        "    return x_deriv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ltBAH7B8GmQH"
      },
      "source": [
        "- The second is a Step function: This is the function where the actual gradient descent takes place. This function takes in an initial or previous value for x, updates it based on steps taken via the learning rate and outputs the most minimum value of x that reaches the stop condition. For our stop condition, we are going to use a precision stop. This means that when the absolute difference between our old and updated x is greater than a value, the algorithm should stop. The function will also print out the minimum value of x as well as the number of steps or descents it took to reach that value.\n",
        "\n",
        "This function is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hNgbCCn8E6rM",
        "colab": {}
      },
      "source": [
        "def step(x_new, x_prev, precision, l_r):\n",
        "    \n",
        "    '''\n",
        "    Description: This function takes in an initial or previous value for x, updates it based on \n",
        "    steps taken via the learning rate and outputs the most minimum value of x that reaches the precision satisfaction.\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    x_new - a starting value of x that will get updated based on the learning rate\n",
        "    \n",
        "    x_prev - the previous value of x that is getting updated to the new one\n",
        "    \n",
        "    precision - a precision that determines the stop of the stepwise descent \n",
        "    \n",
        "    l_r - the learning rate (size of each descent step)\n",
        "    \n",
        "    Output:\n",
        "    \n",
        "    1. Prints out the latest new value of x which equates to the minimum we are looking for\n",
        "    2. Prints out the the number of x values which equates to the number of gradient descent steps\n",
        "    3. Plots a first graph of the function with the gradient descent path\n",
        "    4. Plots a second graph of the function with a zoomed in gradient descent path in the important area\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    # create empty lists where the updated values of x and y will be appended during each iteration\n",
        "    \n",
        "    x_list, y_list = [x_new], [function(x_new)]\n",
        "    # keep looping until your desired precision\n",
        "    while abs(x_new - x_prev) > precision:\n",
        "        \n",
        "        # change the value of x\n",
        "        x_prev = x_new\n",
        "        \n",
        "        # get the derivation of the old value of x\n",
        "        d_x = - deriv(x_prev)\n",
        "        \n",
        "        # get your new value of x by adding the previous, the multiplication of the derivative and the learning rate\n",
        "        x_new = x_prev + (l_r * d_x)\n",
        "        \n",
        "        # append the new value of x to a list of all x-s for later visualization of path\n",
        "        x_list.append(x_new)\n",
        "        \n",
        "        # append the new value of y to a list of all y-s for later visualization of path\n",
        "        y_list.append(function(x_new))\n",
        "\n",
        "    print (\"Local minimum occurs at: \"+ str(x_new))\n",
        "    print (\"Number of steps: \" + str(len(x_list)))\n",
        "    \n",
        "    \n",
        "    plt.subplot(1,2,2)\n",
        "    plt.scatter(x_list,y_list,c=\"g\")\n",
        "    plt.plot(x_list,y_list,c=\"g\")\n",
        "    plt.plot(x,function(x), c=\"r\")\n",
        "    plt.title(\"Gradient descent\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.scatter(x_list,y_list,c=\"g\")\n",
        "    plt.plot(x_list,y_list,c=\"g\")\n",
        "    plt.plot(x,function(x), c=\"r\")\n",
        "    plt.xlim([1.0,2.1])\n",
        "    plt.title(\"Zoomed in Gradient descent to Key Area\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XHYXHIHJGyYn"
      },
      "source": [
        "Next, we proceed to plot the gradient descent path as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qsKwp9w5FC5-",
        "outputId": "7978331d-b1cf-44a0-cfb0-10239ee7fbd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "#Implement gradient descent (all the arguments are arbitrarily chosen)\n",
        "\n",
        "step(2.99, 0, 0.001, 0.05)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local minimum occurs at: 2.001831460188019\n",
            "Number of steps: 17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMEAAAEICAYAAADm0pBUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAEICAYAAAC5yopxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v8wb26qFG4pQ"
      },
      "source": [
        "The importance of Gradient Descent in Machine Learning is one that will be encountered all through your machine learning journey. This is why it is imperative that you understand the inner workings of this algorithm. This tutorial has introduced you to the simplest form of the gradient descent algorithm as well as its implementation in python. Now, you have an intuitive understanding of this algorithm and you are ready to apply it to real world problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqkyK4Xc3JgL",
        "colab_type": "text"
      },
      "source": [
        "## A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size\n",
        "\n",
        "Stochastic gradient descent is the dominant method used to train deep learning models.\n",
        "\n",
        "There are three main variants of gradient descent and it can be confusing which one to use.\n",
        "\n",
        "In this post, you will discover the one type of gradient descent you should use in general and how to configure it.\n",
        "\n",
        "This is what we'll discuss here:\n",
        "\n",
        "- What gradient descent is and how it works from a high level.\n",
        "- What batch, stochastic, and mini-batch gradient descent are and the benefits and limitations of each method.\n",
        "- That mini-batch gradient descent is the go-to method and how to configure it on your applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN2J970W3JgL",
        "colab_type": "text"
      },
      "source": [
        "## Contrasting the 3 Types of Gradient Descent\n",
        "\n",
        "Gradient descent can vary in terms of the number of training patterns used to calculate error; that is in turn used to update the model.\n",
        "\n",
        "The number of patterns used to calculate the error includes how stable the gradient is that is used to update the model. We will see that there is a tension in gradient descent configurations of computational efficiency and the fidelity of the error gradient.\n",
        "\n",
        "The three main flavors of gradient descent are batch, stochastic, and mini-batch.\n",
        "\n",
        "Let’s take a closer look at each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR3AeUBa3JgM",
        "colab_type": "text"
      },
      "source": [
        "### What is Stochastic Gradient Descent?\n",
        "\n",
        "Stochastic gradient descent, often abbreviated **SGD**, is a variation of the gradient descent algorithm that calculates the error and updates the model for each example in the training dataset.\n",
        "\n",
        "The update of the model for each training example means that stochastic gradient descent is often called an online machine learning algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfrKv_R-3JgM",
        "colab_type": "text"
      },
      "source": [
        "#### Upsides\n",
        "- The frequent updates immediately give an insight into the performance of the model and the rate of improvement.\n",
        "- This variant of gradient descent may be the simplest to understand and implement, especially for beginners.\n",
        "- The increased model update frequency can result in faster learning on some problems.\n",
        "- The noisy update process can allow the model to avoid local minima (e.g. premature convergence).\n",
        "\n",
        "#### Downsides\n",
        "- Updating the model so frequently is more computationally expensive than other configurations of gradient descent, taking significantly longer to train models on large datasets.\n",
        "- The frequent updates can result in a noisy gradient signal, which may cause the model parameters and in turn the model error to jump around (have a higher variance over training epochs).\n",
        "- The noisy learning process down the error gradient can also make it hard for the algorithm to settle on an error minimum for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g325YRHa3JgN",
        "colab_type": "text"
      },
      "source": [
        "### What is Batch Gradient Descent?\n",
        "\n",
        "Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated.\n",
        "\n",
        "One cycle through the entire training dataset is called a training epoch. Therefore, it is often said that batch gradient descent performs model updates at the end of each training epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q-IV7HV3JgN",
        "colab_type": "text"
      },
      "source": [
        "#### Upsides\n",
        "- Fewer updates to the model means this variant of gradient descent is more computationally efficient than stochastic gradient descent.\n",
        "- The decreased update frequency results in a more stable error gradient and may result in a more stable convergence on some problems.\n",
        "- The separation of the calculation of prediction errors and the model update lends the algorithm to parallel processing based implementations.\n",
        "\n",
        "#### Downsides\n",
        "- The more stable error gradient may result in premature convergence of the model to a less optimal set of parameters.\n",
        "- The updates at the end of the training epoch require the additional complexity of accumulating prediction errors across all training examples.\n",
        "- Commonly, batch gradient descent is implemented in such a way that it requires the entire training dataset in memory and available to the algorithm.\n",
        "- Model updates, and in turn training speed, may become very slow for large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_loocaYb3JgO",
        "colab_type": "text"
      },
      "source": [
        "## What is Mini-Batch Gradient Descent?\n",
        "Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.\n",
        "\n",
        "Implementations may choose to sum the gradient over the mini-batch which further reduces the variance of the gradient.\n",
        "\n",
        "Mini-batch gradient descent seeks to find a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent. It is the most common implementation of gradient descent used in the field of deep learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1z_FYuL3JgP",
        "colab_type": "text"
      },
      "source": [
        "#### Upsides\n",
        "- The model update frequency is higher than batch gradient descent which allows for a more robust convergence, avoiding local minima.\n",
        "- The batched updates provide a computationally more efficient process than stochastic gradient descent.\n",
        "- The batching allows both the efficiency of not having all training data in memory and algorithm implementations.\n",
        "\n",
        "#### Downsides\n",
        "- Mini-batch requires the configuration of an additional “mini-batch size” hyperparameter for the learning algorithm.\n",
        "- Error information must be accumulated across mini-batches of training examples like batch gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "undgZ8Qs3JgP",
        "colab_type": "text"
      },
      "source": [
        "### How to Configure Mini-Batch Gradient Descent\n",
        "\n",
        "Mini-batch gradient descent is the recommended variant of gradient descent for most applications, especially in deep learning.\n",
        "\n",
        "Mini-batch sizes, commonly called “batch sizes” for brevity, are often tuned to an aspect of the computational architecture on which the implementation is being executed. Such as a power of two that fits the memory requirements of the GPU or CPU hardware like 32, 64, 128, 256, and so on.\n",
        "\n",
        "Batch size is a slider on the learning process.\n",
        "\n",
        "- Small values give a learning process that converges quickly at the cost of noise in the training process.\n",
        "- Large values give a learning process that converges slowly with accurate estimates of the error gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJpMYfIO3JgQ",
        "colab_type": "text"
      },
      "source": [
        "**Tip 1**: A good default for batch size might be 32.\n",
        "\n",
        "… [batch size] is typically chosen between 1 and a few hundreds, e.g. [batch size] = 32 is a good default value, with values above 10 taking advantage of the speedup of matrix-matrix products over matrix-vector products.\n",
        "\n",
        "— From [Practical recommendations for gradient-based training of deep architectures, 2012](https://arxiv.org/abs/1206.5533)\n",
        "\n",
        "\n",
        "**Tip 2**: It is a good idea to review learning curves of model validation error against training time with different batch sizes when tuning the batch size.\n",
        "\n",
        "… it can be optimized separately of the other hyperparameters, by comparing training curves (training and validation error vs amount of training time), after the other hyper-parameters (except learning rate) have been selected.\n",
        "\n",
        "**Tip 3**: Tune batch size and learning rate after tuning all other hyperparameters.\n",
        "\n",
        "… [batch size] and [learning rate] may slightly interact with other hyper-parameters so both should be re-optimized at the end. Once [batch size] is selected, it can generally be fixed while the other hyper-parameters can be further optimized (except for a momentum hyper-parameter, if one is used)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh4QpRrL3JgQ",
        "colab_type": "text"
      },
      "source": [
        "### End of note."
      ]
    }
  ]
}