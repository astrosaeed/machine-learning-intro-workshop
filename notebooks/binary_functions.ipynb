{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jfogarty/machine-learning-intro-workshop/blob/master/notebooks/binary_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnUCpC5PBGtJ"
   },
   "source": [
    "# Using Neural Networks for Fundamental Binary Functions\n",
    "\n",
    "This is nice illustration that a neural network can compute anything.\n",
    "\n",
    "\n",
    "The problem description below is from [The XOR Problem in Neural Networks](https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b) on medium.com by **Jayesh Bapu Ahire**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrJ9ZXN9aK9K"
   },
   "source": [
    "## The XOr Problem\n",
    "\n",
    "The XOr, or “exclusive or”, problem is a classic problem in ANN research. It is the problem of using a neural network to predict the outputs of XOr logic gates given two binary inputs. An XOr function should return a true value if the two inputs are not equal and a false value if they are equal. All possible inputs and predicted outputs are shown below:\n",
    "\n",
    "<center><img src=\"../images/xor-function-table.png\" />\n",
    "</center>\n",
    "\n",
    "XOr is a [classification problem](https://en.wikipedia.org/wiki/Statistical_classification) and one for which the expected outputs are known in advance. It is therefore appropriate to use a supervised learning approach.\n",
    "\n",
    "On the surface, XOr appears to be a very simple problem, however, Minksy and Papert (1969) showed that this was a big problem for neural network architectures of the 1960s, known as perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iIaGFxWkbC4N"
   },
   "source": [
    "## Perceptrons\n",
    "\n",
    "Like all ANNs, the perceptron is composed of a network of units, which are analagous to biological neurons. A unit can receive an input from other units. On doing so, it takes the sum of all values received and decides whether it is going to forward a signal on to other units to which it is connected. This is called activation. The [activation function](https://en.wikipedia.org/wiki/Activation_function) uses some means or other to reduce the sum of input values to a 1 or a 0 (or a value very close to a 1 or 0) in order to represent activation or lack thereof. Another form of unit, known as a bias unit, always activates, typically sending a hard coded 1 to all units to which it is connected.\n",
    "\n",
    "Perceptrons include a single layer of input units — including one bias unit — and a single output unit (see figure 2). Here a bias unit is depicted by a dashed circle, while other units are shown as blue circles. There are two non-bias input units representing the two binary input values for XOr. Any number of input units can be included.\n",
    "\n",
    "<figure>\n",
    "  <center><img src=\"../images/simple-perceptron.png\" />\n",
    "  <figcaption>Figure 2</fgcaption></center>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2A564BkCdlIc"
   },
   "source": [
    "The perceptron is a type of [feed-forward network](http://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Architecture/feedforward.html), which means the process of generating an output — known as forward propagation — flows in one direction from the input layer to the output layer. There are no connections between units in the input layer. Instead, all units in the input layer are connected directly to the output unit.\n",
    "\n",
    "A simplified explanation of the forward propagation process is that the input values X1 and X2, along with the bias value of 1, are multiplied by their respective weights W0..W2, and parsed to the output unit. The output unit takes the sum of those values and employs an activation function — typically the [Heavside step function](https://en.wikipedia.org/wiki/Heaviside_step_function) — to convert the resulting value to a 0 or 1, thus classifying the input values as 0 or 1.\n",
    "\n",
    "It is the setting of the weight variables that gives the network’s author control over the process of converting input values to an output value. It is the weights that determine where the classification line, the line that separates data points into classification groups, is drawn. If all data points on one side of a classification line are assigned the class of 0, all others are classified as 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFovI2K6c031"
   },
   "source": [
    "A limitation of this architecture is that it is only capable of separating data points with a single line. This is unfortunate because the XOr inputs are not [linearly separable](https://en.wikipedia.org/wiki/Linear_separability). This is particularly visible if you plot the XOr input values to a graph. As shown in figure 3, there is no way to separate the 1 and 0 predictions with a single classification line.\n",
    "\n",
    "<figure>\n",
    "  <center><img src=\"../images/xor_linearly_inseparable.gif\" />\n",
    "  <figcaption>Figure 3</fgcaption></center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQlJ-eQ0eKhT"
   },
   "source": [
    "## Multilayer Perceptrons\n",
    "\n",
    "The solution to this problem is to expand beyond the single-layer architecture by adding an additional layer of units without any direct access to the outside world, known as a hidden layer. This kind of architecture — shown in Figure 4 — is another feed-forward network known as a multilayer perceptron (MLP).\n",
    "\n",
    "<figure>\n",
    "  <center><img src=\"../images/xor_multilayer_perceptron.png\" />\n",
    "  <figcaption>Figure 4</fgcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZEEtn42eX0b"
   },
   "source": [
    "It is worth noting that an MLP can have any number of units in its input, hidden and output layers. There can also be any number of hidden layers. The architecture used here is designed specifically for the XOr problem.\n",
    "\n",
    "Similar to the classic perceptron, forward propagation begins with the input values and bias unit from the input layer being multiplied by their respective weights, however, in this case there is a weight for each combination of input (including the input layer’s bias unit) and hidden unit (excluding the hidden layer’s bias unit). The products of the input layer values and their respective weights are parsed as input to the non-bias units in the hidden layer. Each non-bias hidden unit invokes an activation function — usually the classic [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) in the case of the XOr problem — to squash the sum of their input values down to a value that falls between 0 and 1 (usually a value very close to either 0 or 1). The outputs of each hidden layer unit, including the bias unit, are then multiplied by another set of respective weights and parsed to an output unit. The output unit also parses the sum of its input values through an activation function — again, the sigmoid function is appropriate here — to return an output value falling between 0 and 1. This is the predicted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2Be1ACpeoKb"
   },
   "source": [
    "This architecture, while more complex than that of the classic perceptron network, is capable of achieving non-linear separation. Thus, with the right set of weight values, it can provide the necessary separation to accurately classify the XOr inputs.\n",
    "\n",
    "<figure>\n",
    "  <center><img src=\"../images/xor_linear_separable.png\" />\n",
    "  <figcaption>Figure 5</fgcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fs5cHOmRe3kT"
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "The elephant in the room, of course, is how one might come up with a set of weight values that ensure the network produces the expected output. In practice, trying to find an acceptable set of weights for an MLP network manually would be an incredibly laborious task. In fact, it is [NP-complete](https://en.wikipedia.org/wiki/NP-completeness) (Blum and Rivest, 1992). However, it is fortunately possible to learn a good set of weight values automatically through a process known as backpropagation. This was first demonstrated to work well for the XOr problem by Rumelhart et al. (1985).\n",
    "The backpropagation algorithm begins by comparing the actual value output by the forward propagation process to the expected value and then moves backward through the network, slightly adjusting each of the weights in a direction that reduces the size of the error by a small degree. Both forward and back propagation are re-run thousands of times on each input combination until the network can accurately predict the expected output of the possible inputs using forward propagation.\n",
    "\n",
    "For the xOr problem, 100% of possible data examples are available to use in the training process. We can therefore expect the trained network to be 100% accurate in its predictions and there is no need to be concerned with issues such as [bias and variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) in the resulting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YI5tmkdPfFnP"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This note explores the classic ANN XOr problem. The problem itself was described in detail, along with the fact that the inputs for XOr are not linearly separable into their correct classification categories. A non-linear solution — involving an MLP architecture — was explored at a high level, along with the forward propagation algorithm used to generate an output value from the network and the backpropagation algorithm, which is used to train the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oey6L9e0IShg"
   },
   "source": [
    "\n",
    "# A Keras Based Neural Network Solution\n",
    "\n",
    "**Usage NOTE!** Use `Shift+Enter` to step through this notebook, executing the code as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "NDzdSWEGIShh"
   },
   "outputs": [],
   "source": [
    "#@title Welcome\n",
    "import datetime\n",
    "print(f\"Welcome to exploring this notebook at {datetime.datetime.now()}! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "reyL8y-z8hsU"
   },
   "outputs": [],
   "source": [
    "class Context:\n",
    "    VERBOSE=False    # True for extensive logging during execution.\n",
    "    QUIET=False      # True for minimal logging during execution.\n",
    "    WARNINGS=False   # True to enable display of annoying but rarely useful messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "fpyQWK018c5t"
   },
   "outputs": [],
   "source": [
    "#@title Import Directives\n",
    "from contextlib import redirect_stderr\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Suppress Tensorflow log spew.\n",
    "if Context.WARNINGS:\n",
    "    import keras    \n",
    "else:\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "    # Suppress Keras log spew.\n",
    "    with redirect_stderr(open(os.devnull, \"w\")):\n",
    "        import keras\n",
    "    \n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "if tf.test.gpu_device_name().startswith('/device:GPU'): \n",
    "    print(\"Running with GPU acceleration.\")\n",
    "else:\n",
    "    print(\"Running on normal CPU without GPU acceleration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "IouBgIh48myZ"
   },
   "outputs": [],
   "source": [
    "#@title Text Formatting and Output Functions\n",
    "def fmt(f, *args):\n",
    "    if len(args) == 0:\n",
    "        return str(f)\n",
    "    else:\n",
    "        if type(f) is str:\n",
    "            return f.format(*args)\n",
    "        else:\n",
    "            return [str(f)] + [str(s) for s in args]\n",
    "\n",
    "def log(f, *args):\n",
    "    print(fmt(f, *args))\n",
    "\n",
    "def out(f, *args):\n",
    "    if not Context.QUIET:\n",
    "        log(fmt(f, *args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rweZwkz0ISiI"
   },
   "source": [
    "### inputs and training sets for the binary logic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lk3okGQY8-ch"
   },
   "outputs": [],
   "source": [
    "Binary_functions = {\n",
    "    'not' : { 'function' : 'Binary NOT', 'X' : np.array([[0],[1]]),                 \n",
    "                                         'Y' : np.array([[1],[0]]) },\n",
    "    \n",
    "    'xor' : { 'function' : 'Binary XOR', 'X' : np.array([[0,0],[0,1],[1,0],[1,1]]),              \n",
    "                                         'Y' : np.array([ [0],  [1],  [1],  [0]]) },\n",
    "    \n",
    "    'and' : { 'function' : 'Binary AND', 'X' : np.array([[0,0],[0,1],[1,0],[1,1]]),              \n",
    "                                         'Y' : np.array([ [0],  [0],  [0],  [1]]) },\n",
    "    \n",
    "    'or'  : { 'function' : 'Binary OR',  'X' : np.array([[0,0],[0,1],[1,0],[1,1]]), \n",
    "                                         'Y' : np.array([ [0],  [1],  [1],  [1]]) }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LnJKyozADhEu"
   },
   "outputs": [],
   "source": [
    "Binary_functions.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLsZpdWOISib"
   },
   "source": [
    "### The simple model and training function with no extra confusing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "daoV3kG28pmY"
   },
   "outputs": [],
   "source": [
    "def compute_nofrills(function, X, Y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=2, activation='tanh'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1))\n",
    "    \n",
    "    model.fit(X, Y, batch_size=1, epochs=1000, verbose=0)\n",
    "    model.summary()\n",
    "    \n",
    "    y = model.predict(X)\n",
    "    log(\"Predicted {} Truth Values from input table:\\n{}\", function, y)\n",
    "    log(\"Exact results:\\n{}\", Y)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Saps1ZpwISih"
   },
   "outputs": [],
   "source": [
    "compute_nofrills(**Binary_functions['xor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2JMT08lISin"
   },
   "source": [
    "### A more complete version with hyperparameters and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFfrflJUISip"
   },
   "outputs": [],
   "source": [
    "def compute(function, X, Y, epochs=1000, extralayers=0):\n",
    "    if Context.VERBOSE:\n",
    "        out(\"X.shape={}\", X.shape)\n",
    "        out(\"Input Array of {} Inputs:\\n{}\", function, X)\n",
    "        out(\"Ouput Array of Truth Results:\\n{}\".format(Y))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=X.shape[1], activation='tanh'))\n",
    "    for n in range(extralayers):\n",
    "      model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    log(f\"Training {function} model with {extralayers+2} layers in {epochs} iterations\")\n",
    "    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1))\n",
    "\n",
    "    out(\"--------------------------------------------------------------------------\")\n",
    "    out(\"----- Finding {} Mapping Function using a Brute Force Neural Network:\", function)\n",
    "\n",
    "    model.fit(X, Y, batch_size=1, epochs=epochs, verbose=1 if Context.VERBOSE else 0)\n",
    "\n",
    "    out(\"----- Keras information on the function:\")\n",
    "    if not Context.QUIET: model.summary()\n",
    "\n",
    "    log(\"--------------------------------------------------------------------------\")\n",
    "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.8f}\".format(x)})\n",
    "    y = model.predict(X)\n",
    "    log(\"Predicted {} Truth Values from input table:\\n{}\", function, y)\n",
    "    log(\"Exact results:\\n{}\", Y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xU1rGb1l82xI"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    for f in Binary_functions.values():\n",
    "        compute(**f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "IN9_NbTn9Zqw"
   },
   "outputs": [],
   "source": [
    "#@title Python Command Line Main Program \n",
    "try:\n",
    "    get_ipython\n",
    "except NameError:\n",
    "    if __name__=='__main__':\n",
    "        desc = \"This is a brute force method that demonstrates a concept rather than a practical example.\"\n",
    "        parser = argparse.ArgumentParser(\n",
    "            description = \"Keras based Neural Network to train for and evaluate Boolean Functions.\",\n",
    "            epilog = desc,\n",
    "        )    \n",
    "        parser.add_argument('-v', \"--verbose\",  help=\"increase output verbosity.\",       action='store_true')\n",
    "        parser.add_argument('-q', \"--quiet\",    help=\"decrease output verbosity.\",       action='store_true')\n",
    "        parser.add_argument('-w', \"--warnings\", help=\"don't suppress warning messages.\", action='store_true')        \n",
    "        parser.add_argument('-?',               help=argparse.SUPPRESS,                  action='store_true')\n",
    "\n",
    "        ap = parser.parse_args()\n",
    "        if ap.verbose:  Context.VERBOSE=True\n",
    "        if ap.quiet:    Context.QUIET=True\n",
    "        if ap.warnings: Context.WARNINGS=True\n",
    "        if getattr(ap, '?', False):\n",
    "            parser.print_help()\n",
    "            exit()\n",
    "        main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LneRAJ2OISjA"
   },
   "source": [
    "### Explore the simplest function of all - NOT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FiIchHMIiZvr"
   },
   "outputs": [],
   "source": [
    "Context.VERBOSE = False\n",
    "Context.QUIET = True\n",
    "m = compute('Binary NOT', \n",
    "        extralayers=0,\n",
    "        X=np.array([[0],[1]]),\n",
    "        Y=np.array([[1],[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hP8jpQhUoBmz"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "log(\"\\n----- 0 truthiness:\")\n",
    "for n in [random.randrange(0, 45)/100 for i in range(10)]:\n",
    "    v = m.predict([[n]])[0][0]\n",
    "    log(\"- Truthiness of {0:0.6f} is {1:0.6f} [{2}]\", n, v, v < 0.5)\n",
    "\n",
    "log(\"\\n----- 1 truthiness:\")\n",
    "for n in [random.randrange(55, 99)/100 for i in range(10)]:\n",
    "    v = m.predict([[n]])[0][0]\n",
    "    log(\"- Truthiness of {0:0.6f} is {1:0.6f} [{2}]\", n, v, v < 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51MThcpHISjQ"
   },
   "source": [
    "### Compute models with various hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "8KAeL_ns9pJI"
   },
   "outputs": [],
   "source": [
    "Context.VERBOSE = False\n",
    "Context.QUIET = False\n",
    "compute(**Binary_functions['xor'], extralayers=0, epochs=10)\n",
    "compute(**Binary_functions['xor'], extralayers=0, epochs=100)\n",
    "compute(**Binary_functions['xor'], extralayers=0, epochs=1000)\n",
    "compute(**Binary_functions['xor'], extralayers=1)\n",
    "compute(**Binary_functions['xor'], extralayers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apClhi66ISja"
   },
   "source": [
    "### End of notebook."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "binary_functions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
