{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "least_squares.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfogarty/machine-learning-intro-workshop/blob/master/notebooks/least_squares.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f4QiRDdVJhea"
      },
      "source": [
        "# Least Squares (Linear) Regression\n",
        "\n",
        "- From [this Least Squares example](https://github.com/SergioRAgostinho/bootstrap-ml/blob/master/least_squares.ipynb) Notebook.\n",
        "    - Author:  [Sérgio Agostinho](https://www.linkedin.com/in/sergioagostinho)\n",
        "    - [Sérgio's GitHub repository](https://github.com/SergioRAgostinho/bootstrap-ml)\n",
        "\n",
        "Updated by [John Fogarty](https://github.com/jfogarty) for Python 3.6 and [Base2 MLI](https://github.com/base2solutions/mli) and [colab](https://colab.research.google.com) standalone evaluation.\n",
        "\n",
        "This was made from the [Bootstrap-ML](http://sergioagostinho.com/bootstrap-ml) workshop which took place on the 22nd of March 2018 at the Mathematics Department of [Instituto Superior Técnico](https://tecnico.ulisboa.pt/en/).\n",
        "\n",
        "This workshop, given by Sérgio Agostinh, was a two-hour introductory session in Machine Learning (ML)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NLHofm9oJhed"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "We're going to tackle one of the most simple and yet powerful tools of Machine Learning, the Least Squares estimator. Given a function in the shape of \n",
        "\n",
        "$$\n",
        "\\hat y = w^T\\Phi\\left(x\\right) + b\n",
        "$$\n",
        "\n",
        "where $\\hat y, b \\in $ [$\\mathbb{R}$](https://en.wikipedia.org/wiki/Real_number) and [$\\Phi$](https://en.wikipedia.org/wiki/Phi) is a mapping from our input space to a given feature space such that $\\Phi: \\mathbb{R}^m \\to \\mathbb{R}^n$, we want to estimate the optimal $w$ which minimizes $e$.\n",
        "\n",
        "$$\n",
        "e = \\sum^{N}_{i = 1} \\left(y - \\hat y\\right)^2\n",
        "$$\n",
        "\n",
        "> $w$ is usually referred to as a [weight matrix](https://datascience.stackexchange.com/questions/23462/dimension-of-weight-matrix-in-neural-network), and you will encounter it in nearly every aspect of machine learning.\n",
        "\n",
        "> $e$ is the $error$ or $cost$ which results from the [loss function](https://en.wikipedia.org/wiki/Loss_function) (also referred to, almost interchangably, as an error function, cost function, etc.) All machine learning requires some such a function to evaluate the merits of a given model.\n",
        "\n",
        "## Problem: fit a straight line through a number of points\n",
        "\n",
        "This is the most basic and common use case, so we need to see it in action. Let's generate a number of random points between $\\left[0, 1\\right]$ according to the following model\n",
        "\n",
        "$$\n",
        "y = 2x + 1 + e\n",
        "$$\n",
        "\n",
        "where $e \\sim \\mathcal{N} \\left(0, 0.2^2\\right)$. In this case $w \\in \\mathbb{R}$ and is equal to $2$, $x \\in \\mathcal{U} \\left(0, 1\\right)$ and $b=1$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CgdssYaVJhee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "a7fff02f-07db-4384-d315-c78c108218b8"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "\n",
        "n = 100\n",
        "x = np.random.uniform(size=n)\n",
        "e = np.random.normal(scale=0.4, size=n)\n",
        "y = 2*x + 1 + e\n",
        "\n",
        "#\n",
        "plt.scatter(x=x, y=y)\n",
        "plt.show()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXUQkQQruIiIJUnAXEUmQgruISIL+H/aJEeuNF/vbAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OX9o97TnJheg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "5aa0ec62-faff-4c06-bce1-5dd18f756a19"
      },
      "source": [
        "x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.64932055, 0.02942217, 0.84918868, 0.65008906, 0.39065505,\n",
              "       0.83615415, 0.20053099, 0.55518311, 0.09563106, 0.38451668,\n",
              "       0.77981614, 0.17909889, 0.59909031, 0.26233891, 0.57812102,\n",
              "       0.4215293 , 0.14647903, 0.60627628, 0.86055432, 0.43874021,\n",
              "       0.46151048, 0.6363766 , 0.63155784, 0.14969594, 0.5730302 ,\n",
              "       0.18251966, 0.1162099 , 0.34266195, 0.56714942, 0.74719958,\n",
              "       0.40544424, 0.85662418, 0.91087273, 0.14596248, 0.05890494,\n",
              "       0.23534269, 0.3300849 , 0.53882453, 0.29138467, 0.80912183,\n",
              "       0.82978949, 0.68602399, 0.15155997, 0.895248  , 0.36717031,\n",
              "       0.39691851, 0.86419085, 0.30847143, 0.04871027, 0.81868712,\n",
              "       0.298392  , 0.41126286, 0.9984482 , 0.81407791, 0.89756117,\n",
              "       0.51560119, 0.37166732, 0.24147044, 0.90217288, 0.48056301,\n",
              "       0.41900459, 0.50644043, 0.66947829, 0.97521157, 0.01709265,\n",
              "       0.65805518, 0.00437657, 0.64797626, 0.08940724, 0.70660516,\n",
              "       0.05739903, 0.42371059, 0.79406221, 0.26753589, 0.85499901,\n",
              "       0.14851178, 0.99883309, 0.40554294, 0.68524663, 0.12560948,\n",
              "       0.26352438, 0.60234921, 0.89944053, 0.91863208, 0.40937071,\n",
              "       0.92760261, 0.66232543, 0.22453777, 0.59167555, 0.58321362,\n",
              "       0.80688461, 0.77966086, 0.05270274, 0.34980899, 0.60615439,\n",
              "       0.82320736, 0.32852652, 0.53327435, 0.66545717, 0.69083795])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mC3MndDEJhei",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "86d0c01f-cec5-421e-f75e-0830a6a85a05"
      },
      "source": [
        "y"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.71848562, 1.15799099, 2.54530042, 1.78181105, 1.41188779,\n",
              "       2.34312941, 1.75128753, 2.72995677, 0.86312082, 1.67114798,\n",
              "       2.51834739, 1.32837117, 2.99242552, 1.76204304, 2.86395483,\n",
              "       1.74283917, 1.34591838, 2.09641479, 3.04237965, 2.52413672,\n",
              "       1.75765429, 1.74472498, 2.26210723, 1.11888326, 2.39193581,\n",
              "       1.66253576, 0.6770261 , 2.41651189, 2.77556974, 2.2660998 ,\n",
              "       2.36070569, 2.19873125, 2.75271777, 0.8053869 , 1.37441006,\n",
              "       1.13727045, 1.72306177, 2.50390673, 1.18795825, 1.75028208,\n",
              "       2.14025392, 2.35364134, 1.29703637, 3.00444097, 1.94468173,\n",
              "       1.22960147, 2.84415506, 1.73339551, 0.8947892 , 2.26078161,\n",
              "       1.49078043, 2.11437894, 2.90681766, 2.14380786, 3.10926051,\n",
              "       2.11053847, 1.48871587, 1.07401183, 2.82645436, 1.58792813,\n",
              "       2.19391311, 2.17939581, 1.89238815, 3.1980936 , 0.79503009,\n",
              "       2.36852619, 0.92099279, 1.67431693, 0.85711212, 2.2700353 ,\n",
              "       0.99306436, 2.26133728, 2.07419286, 0.81931207, 1.78460202,\n",
              "       0.62859853, 2.44230382, 1.41616771, 2.96049716, 1.12771435,\n",
              "       0.70059438, 1.97511641, 2.41199431, 2.67648033, 2.07489741,\n",
              "       3.11145657, 2.81775561, 1.52761456, 2.51918775, 2.9519635 ,\n",
              "       3.00183313, 3.0720613 , 1.58627875, 1.74970647, 2.81793923,\n",
              "       3.09545522, 1.96809204, 1.82200046, 2.76096614, 2.5580743 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KCRfGs9AJhel"
      },
      "source": [
        "### How to find the best $w$ and $b$?\n",
        "\n",
        "Let's derive this one time so that you understand the most basic concepts of all these algorithms. We are interested in finding the weight vector $w$ which minimizes the sum of the square error\n",
        "\n",
        "\\begin{equation}\n",
        "\\underset{w}{\\operatorname{argmin}} e = \\sum^{N}_{i = 1} \\left(y_i - w^T \\Phi(x_i) - b \\right)^2 \n",
        "\\end{equation}\n",
        "\n",
        "We can augment $w$ such that $w_0 = b$ and $w \\in \\mathbb{R}^{n+1}$ and rewrite the expression above as\n",
        "\n",
        "\\begin{equation}\n",
        "\\underset{w}{\\operatorname{argmin}} e = \\sum^{N}_{i = 1} \\left(y_i - w^T \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\right)^2 \n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Since the equation is linear, it has a closed-form solution. Let's start by finding the $w$ which gives us a null gradient. We're also going to make use of an important property of matrix derivatives, in this case applied to vectors\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{d x^Ta}{dx} =  a\n",
        "\\end{equation}\n",
        "\n",
        "for $a, x \\in \\mathbb{R}^n$.\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\frac{d}{dw} \\sum^{N}_{i = 1} \\left(y_i - w^T \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\right)^2  & = & 0 \\\\\n",
        "\\sum^{N}_{i = 1} \\frac{d}{dw} \\left(y_i - w^T \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\right)^2 & = & 0 \\\\\n",
        "\\sum^{N}_{i = 1} -2 \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\left(y_i - w^T \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\right) & = & 0 \\\\\n",
        "\\sum^{N}_{i = 1} \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\left(\\left[\\begin{matrix}\\Phi(x_i)^T & 1 \\end{matrix} \\right] w - y_i\\right) & = & 0 \\\\\n",
        "\\sum^{N}_{i = 1} \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\left[\\begin{matrix}\\Phi(x_i)^T & 1 \\end{matrix} \\right] w & = & \\sum^{N}_{i = 1} \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] y_i \\\\\n",
        "\\sum^{N}_{i = 1} \\left[\\begin{matrix}\\Phi(x_i)\\Phi(x_i)^T & \\Phi(x_i) \\\\ \\Phi(x_i)^T & 1 \\end{matrix} \\right] w & = & \\sum^{N}_{i = 1} \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] y_i \\\\\n",
        " \\left[\\begin{matrix}\\sum^{N}_{i = 1} \\Phi(x_i)\\Phi(x_i)^T & \\sum^{N}_{i = 1} \\Phi(x_i) \\\\ \\sum^{N}_{i = 1} \\Phi(x_i)^T & N \\end{matrix} \\right] w & = & \\left[\\begin{matrix} \\sum^{N}_{i = 1}\\Phi(x_i) y_i \\\\ \\sum^{N}_{i = 1}y_i \\end{matrix} \\right]\n",
        "\\end{eqnarray}\n",
        "\n",
        "resulting of course in\n",
        "\n",
        "\\begin{equation}\n",
        " w = \\left[\\begin{matrix}\\sum^{N}_{i = 1} \\Phi(x_i)\\Phi(x_i)^T & \\sum^{N}_{i = 1} \\Phi(x_i) \\\\ \\sum^{N}_{i = 1} \\Phi(x_i)^T & N \\end{matrix} \\right]^{-1}\\left[\\begin{matrix} \\sum^{N}_{i = 1}\\Phi(x_i) y_i \\\\ \\sum^{N}_{i = 1}y_i \\end{matrix} \\right]\\\\\n",
        "\\end{equation}\n",
        "\n",
        "Let's apply this proof to our case. First we need to figure what is what. We know the data was generated under the following model\n",
        "\n",
        "\\begin{equation}\n",
        "y = 2x + 1\n",
        "\\end{equation}\n",
        "\n",
        "plus some added noise. In this case the $\\Phi(x) = x$, $w = 2$ and $b = 1$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jeIcXlC-Jhel"
      },
      "source": [
        "## Numpy [linalg.solve](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html)\n",
        "\n",
        "We have a handy function available that solves exactly this kind of system, but we had to do a lot of math first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jn-1MzjXJhem",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5cec9bfa-ad5a-416f-d10a-8df136f2f9c0"
      },
      "source": [
        "X = np.array([[np.dot(x, x), np.sum(x)],[np.sum(x), n]])\n",
        "Y = np.array([np.dot(x,y), np.sum(y)])\n",
        "theta = np.linalg.solve(X,Y)\n",
        "print(theta)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.01895708 0.96753642]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0kwABdoXJheo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "cd6e2031-d4a4-492a-b2b9-49fbc28067e0"
      },
      "source": [
        "plt.plot(x, y, 'o', label='original data')\n",
        "plt.plot(x, theta[0]*x + theta[1], 'r', label='fitted line')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npWJIg7tSSsXQfwFp8zH6OJo5iAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W4vu4gRhJher"
      },
      "source": [
        "## Scikit Learn [linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) \n",
        "\n",
        "Scikit already provides linear regression capabilities among others. So everything we went through could have been replaced by:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fMcy_aetJher",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b949a856-74ab-4c46-fb75-68fc7b433494"
      },
      "source": [
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "\n",
        "regr = linear_model.LinearRegression()\n",
        "regr.fit(x.reshape(-1,1), y)\n",
        "\n",
        "# The coefficients\n",
        "print('Coefficients: \\n', regr.coef_)\n",
        "print('Intercept: \\n', regr.intercept_)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coefficients: \n",
            " [2.01895708]\n",
            "Intercept: \n",
            " 0.9675364156965405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yMhnXfaxJhex"
      },
      "source": [
        "Note that we needed to use [numpy reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) here so that our inputs to the fit function are 2D matrices\n",
        "\n",
        "See [this example for more on using sklearn linear models](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "91D4uKQmJhey",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4ef1b5bb-4f39-4ce4-dbaf-2f67e9f156b4"
      },
      "source": [
        "print(f\"x's shape was {y.shape}\")\n",
        "print(f\"x is reshaped to {x.reshape(-1,1).shape}\")\n",
        "print(f\"y's shape is {y.shape}\")\n",
        "print(\"--So for each entry in Y we have an input vector in X containing exactly one item.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x's shape was (100,)\n",
            "x is reshaped to (100, 1)\n",
            "y's shape is (100,)\n",
            "--So for each entry in Y we have an input vector in X containing exactly one item.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NJ457ZAEJhe1"
      },
      "source": [
        "## Bonus math\n",
        "\n",
        "If you haven't had enough, feel free to note this nice equivalence in linear algebra using normal equations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sNoyDZL5Jhe2"
      },
      "source": [
        "### Normal Equations\n",
        "\n",
        "\\begin{equation}\n",
        " w = \\left[\\begin{matrix}\\sum^{N}_{i = 1} \\Phi(x_i)\\Phi(x_i)^T & \\sum^{N}_{i = 1} \\Phi(x_i)^T \\\\ \\sum^{N}_{i = 1} \\Phi(x_i) & N \\end{matrix} \\right]^{-1}\\left[\\begin{matrix} \\sum^{N}_{i = 1}\\Phi(x_i) y_i \\\\ \\sum^{N}_{i = 1}y_i \\end{matrix} \\right]\\\\\n",
        "\\end{equation}\n",
        "\n",
        "Can be written in another way commonly known as the **normal equations**. Just like before, consider you have $N$ samples and you stack your features and your target like this\n",
        "\n",
        "\\begin{equation}\n",
        "X =  \\left[\\begin{matrix}\\Phi(x_1)^T & \\dots  & 1 \\\\\n",
        "                         \\vdots    & \\ddots & \\vdots \\\\\n",
        "                         \\Phi(x_N)^T & \\dots  & 1 \\end{matrix}\\right]\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\textbf{y} =  \\left[\\begin{matrix} y_1 \\\\ \\vdots \\\\ y_N \\end{matrix}\\right]\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The best $w$ is given by\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{w} = (X^TX)^{-1} X^T  \\textbf{y}\n",
        "\\end{equation}\n",
        "\n",
        "Let's test it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7H58VjzhJhe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "625e63c3-c861-453b-d4d4-736df2e9152a"
      },
      "source": [
        "X = np.matrix([x, np.ones(len(x))]).T\n",
        "Y = np.matrix(y).T\n",
        "print(\"X shape:\\n\", X.shape)\n",
        "print(\"Y shape:\\n\", Y.shape)\n",
        "\n",
        "w = np.linalg.solve(X.T * X, X.T * Y)\n",
        "print(\"w:\\n\", w)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape:\n",
            " (100, 2)\n",
            "Y shape:\n",
            " (100, 1)\n",
            "w:\n",
            " [[2.01895708]\n",
            " [0.96753642]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3RNKiq0cJhe6"
      },
      "source": [
        "## Bonus! LinearRegression Example\n",
        "\n",
        "- [This example](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py) is pulled directly from the scikit-learn examples, and lightly edited. This works with only a single feature, so it's pretty useless."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b0bAGhhXJhe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "867b0537-6aba-4f67-fa14-4c0c7a2f1669"
      },
      "source": [
        "# Get the dataset and segment it into training and test sets.\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the diabetes dataset\n",
        "diabetes = datasets.load_diabetes()\n",
        "\n",
        "print(f\"- Available features are : {diabetes.feature_names}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Available features are : ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3aabZPJbJhe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63a0dd4f-0919-4f5d-9339-da303eb0b933"
      },
      "source": [
        "# Use only one feature,\n",
        "feature = 2\n",
        "print(f'- Using only feature #{feature} which is \"{diabetes.feature_names[feature]}\"')\n",
        "diabetes_X = diabetes.data[:, np.newaxis, feature]\n",
        "\n",
        "# Split the data into training/testing sets\n",
        "diabetes_X_train = diabetes_X[:-20]\n",
        "diabetes_X_test  = diabetes_X[-20:]\n",
        "\n",
        "# Split the targets into training/testing sets\n",
        "diabetes_y_train = diabetes.target[:-20]\n",
        "diabetes_y_test  = diabetes.target[-20:]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Using only feature #2 which is \"bmi\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9FIRvYQb6Vv",
        "colab_type": "text"
      },
      "source": [
        "After you've tried this, let's see if some other features would be helpful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qkoZvRZDJhfB",
        "colab": {}
      },
      "source": [
        "# Build and train the model.\n",
        "\n",
        "# Create linear regression object\n",
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "# Train the model using the training sets\n",
        "regr.fit(diabetes_X_train, diabetes_y_train)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "diabetes_y_pred = regr.predict(diabetes_X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rzG0Ddl4JhfD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "7e889e41-bea9-4f46-a66a-5daa99b75e39"
      },
      "source": [
        "# Display the results.\n",
        "\n",
        "# The coefficients\n",
        "print('Coefficients: \\n', regr.coef_)\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\"\n",
        "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
        "# Explained variance score: 1 is perfect prediction\n",
        "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
        "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
        "\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coefficients: \n",
            " [938.23786125]\n",
            "Mean squared error: 2548.07\n",
            "Variance score: 0.47\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBIEMAJb4L/4/ciktfwZ6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YQSUosnrJhfF"
      },
      "source": [
        "### Explore the Diabetes data set\n",
        "\n",
        "Take a closer look at the content of the dataset using Pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4YWfsENMJhfG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "3e3e7d60-3089-41d3-c02c-2c76b41eecfa"
      },
      "source": [
        "ds = diabetes\n",
        "print(ds.DESCR)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _diabetes_dataset:\n",
            "\n",
            "Diabetes dataset\n",
            "----------------\n",
            "\n",
            "Ten baseline variables, age, sex, body mass index, average blood\n",
            "pressure, and six blood serum measurements were obtained for each of n =\n",
            "442 diabetes patients, as well as the response of interest, a\n",
            "quantitative measure of disease progression one year after baseline.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "  :Number of Instances: 442\n",
            "\n",
            "  :Number of Attributes: First 10 columns are numeric predictive values\n",
            "\n",
            "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
            "\n",
            "  :Attribute Information:\n",
            "      - Age\n",
            "      - Sex\n",
            "      - Body mass index\n",
            "      - Average blood pressure\n",
            "      - S1\n",
            "      - S2\n",
            "      - S3\n",
            "      - S4\n",
            "      - S5\n",
            "      - S6\n",
            "\n",
            "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
            "\n",
            "Source URL:\n",
            "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
            "\n",
            "For more information see:\n",
            "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
            "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DNHhQMoeJhfI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7154b8a4-d6d5-47e9-bbd6-5b925129e9d2"
      },
      "source": [
        "ds"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n  :Number of Instances: 442\\n\\n  :Number of Attributes: First 10 columns are numeric predictive values\\n\\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n  :Attribute Information:\\n      - Age\\n      - Sex\\n      - Body mass index\\n      - Average blood pressure\\n      - S1\\n      - S2\\n      - S3\\n      - S4\\n      - S5\\n      - S6\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)',\n",
              " 'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
              "          0.01990842, -0.01764613],\n",
              "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
              "         -0.06832974, -0.09220405],\n",
              "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
              "          0.00286377, -0.02593034],\n",
              "        ...,\n",
              "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
              "         -0.04687948,  0.01549073],\n",
              "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
              "          0.04452837, -0.02593034],\n",
              "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
              "         -0.00421986,  0.00306441]]),\n",
              " 'data_filename': '/usr/local/lib/python3.6/dist-packages/sklearn/datasets/data/diabetes_data.csv.gz',\n",
              " 'feature_names': ['age',\n",
              "  'sex',\n",
              "  'bmi',\n",
              "  'bp',\n",
              "  's1',\n",
              "  's2',\n",
              "  's3',\n",
              "  's4',\n",
              "  's5',\n",
              "  's6'],\n",
              " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
              "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
              "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
              "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
              "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
              "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
              "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
              "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
              "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
              "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
              "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
              "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
              "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
              "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
              "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
              "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
              "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
              "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
              "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
              "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
              "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
              "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
              "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
              "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
              "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
              "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
              "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
              "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
              "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
              "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
              "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
              "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
              "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
              "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
              "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
              "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
              "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
              "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
              "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
              "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
              "        220.,  57.]),\n",
              " 'target_filename': '/usr/local/lib/python3.6/dist-packages/sklearn/datasets/data/diabetes_target.csv.gz'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KRI89X4vJhfK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a0371105-1a02-432d-fe51-2d3c80974b3b"
      },
      "source": [
        "import pandas as pd\n",
        "X = pd.DataFrame(ds['data'], columns=ds['feature_names'])\n",
        "Y = pd.DataFrame(ds['target'], columns=['target'])\n",
        "\n",
        "df = pd.DataFrame(np.c_[ds['data'], ds['target']],\n",
        "                  columns= np.append(ds['feature_names'], ['target']))\n",
        "df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>bp</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>s6</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.038076</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.061696</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>-0.044223</td>\n",
              "      <td>-0.034821</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.019908</td>\n",
              "      <td>-0.017646</td>\n",
              "      <td>151.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001882</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.051474</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>-0.008449</td>\n",
              "      <td>-0.019163</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.068330</td>\n",
              "      <td>-0.092204</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.085299</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>-0.005671</td>\n",
              "      <td>-0.045599</td>\n",
              "      <td>-0.034194</td>\n",
              "      <td>-0.032356</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>-0.025930</td>\n",
              "      <td>141.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.089063</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.011595</td>\n",
              "      <td>-0.036656</td>\n",
              "      <td>0.012191</td>\n",
              "      <td>0.024991</td>\n",
              "      <td>-0.036038</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.022692</td>\n",
              "      <td>-0.009362</td>\n",
              "      <td>206.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005383</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.036385</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>0.003935</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>0.008142</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.031991</td>\n",
              "      <td>-0.046641</td>\n",
              "      <td>135.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.092695</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.040696</td>\n",
              "      <td>-0.019442</td>\n",
              "      <td>-0.068991</td>\n",
              "      <td>-0.079288</td>\n",
              "      <td>0.041277</td>\n",
              "      <td>-0.076395</td>\n",
              "      <td>-0.041180</td>\n",
              "      <td>-0.096346</td>\n",
              "      <td>97.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.045472</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.047163</td>\n",
              "      <td>-0.015999</td>\n",
              "      <td>-0.040096</td>\n",
              "      <td>-0.024800</td>\n",
              "      <td>0.000779</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.062913</td>\n",
              "      <td>-0.038357</td>\n",
              "      <td>138.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.063504</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.001895</td>\n",
              "      <td>0.066630</td>\n",
              "      <td>0.090620</td>\n",
              "      <td>0.108914</td>\n",
              "      <td>0.022869</td>\n",
              "      <td>0.017703</td>\n",
              "      <td>-0.035817</td>\n",
              "      <td>0.003064</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.061696</td>\n",
              "      <td>-0.040099</td>\n",
              "      <td>-0.013953</td>\n",
              "      <td>0.006202</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.014956</td>\n",
              "      <td>0.011349</td>\n",
              "      <td>110.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.070900</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>-0.033214</td>\n",
              "      <td>-0.012577</td>\n",
              "      <td>-0.034508</td>\n",
              "      <td>-0.024993</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.067736</td>\n",
              "      <td>-0.013504</td>\n",
              "      <td>310.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-0.096328</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.083808</td>\n",
              "      <td>0.008101</td>\n",
              "      <td>-0.103389</td>\n",
              "      <td>-0.090561</td>\n",
              "      <td>-0.013948</td>\n",
              "      <td>-0.076395</td>\n",
              "      <td>-0.062913</td>\n",
              "      <td>-0.034215</td>\n",
              "      <td>101.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.027178</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.017506</td>\n",
              "      <td>-0.033214</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>0.045972</td>\n",
              "      <td>-0.065491</td>\n",
              "      <td>0.071210</td>\n",
              "      <td>-0.096433</td>\n",
              "      <td>-0.059067</td>\n",
              "      <td>69.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.016281</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.028840</td>\n",
              "      <td>-0.009113</td>\n",
              "      <td>-0.004321</td>\n",
              "      <td>-0.009769</td>\n",
              "      <td>0.044958</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.030751</td>\n",
              "      <td>-0.042499</td>\n",
              "      <td>179.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.005383</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.001895</td>\n",
              "      <td>0.008101</td>\n",
              "      <td>-0.004321</td>\n",
              "      <td>-0.015719</td>\n",
              "      <td>-0.002903</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.038393</td>\n",
              "      <td>-0.013504</td>\n",
              "      <td>185.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.045341</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.025607</td>\n",
              "      <td>-0.012556</td>\n",
              "      <td>0.017694</td>\n",
              "      <td>-0.000061</td>\n",
              "      <td>0.081775</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.031991</td>\n",
              "      <td>-0.075636</td>\n",
              "      <td>118.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-0.052738</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.018062</td>\n",
              "      <td>0.080401</td>\n",
              "      <td>0.089244</td>\n",
              "      <td>0.107662</td>\n",
              "      <td>-0.039719</td>\n",
              "      <td>0.108111</td>\n",
              "      <td>0.036056</td>\n",
              "      <td>-0.042499</td>\n",
              "      <td>171.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>-0.005515</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.042296</td>\n",
              "      <td>0.049415</td>\n",
              "      <td>0.024574</td>\n",
              "      <td>-0.023861</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>0.052280</td>\n",
              "      <td>0.027917</td>\n",
              "      <td>166.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.070769</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.012117</td>\n",
              "      <td>0.056301</td>\n",
              "      <td>0.034206</td>\n",
              "      <td>0.049416</td>\n",
              "      <td>-0.039719</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.027368</td>\n",
              "      <td>-0.001078</td>\n",
              "      <td>144.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.038207</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.010517</td>\n",
              "      <td>-0.036656</td>\n",
              "      <td>-0.037344</td>\n",
              "      <td>-0.019476</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.018118</td>\n",
              "      <td>-0.017646</td>\n",
              "      <td>97.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-0.027310</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.018062</td>\n",
              "      <td>-0.040099</td>\n",
              "      <td>-0.002945</td>\n",
              "      <td>-0.011335</td>\n",
              "      <td>0.037595</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.008944</td>\n",
              "      <td>-0.054925</td>\n",
              "      <td>168.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>-0.049105</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.056863</td>\n",
              "      <td>-0.043542</td>\n",
              "      <td>-0.045599</td>\n",
              "      <td>-0.043276</td>\n",
              "      <td>0.000779</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.011901</td>\n",
              "      <td>0.015491</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.085430</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.022373</td>\n",
              "      <td>0.001215</td>\n",
              "      <td>-0.037344</td>\n",
              "      <td>-0.026366</td>\n",
              "      <td>0.015505</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.072128</td>\n",
              "      <td>-0.017646</td>\n",
              "      <td>49.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>-0.085430</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.004050</td>\n",
              "      <td>-0.009113</td>\n",
              "      <td>-0.002945</td>\n",
              "      <td>0.007767</td>\n",
              "      <td>0.022869</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.061177</td>\n",
              "      <td>-0.013504</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.045341</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.060618</td>\n",
              "      <td>0.031053</td>\n",
              "      <td>0.028702</td>\n",
              "      <td>-0.047347</td>\n",
              "      <td>-0.054446</td>\n",
              "      <td>0.071210</td>\n",
              "      <td>0.133599</td>\n",
              "      <td>0.135612</td>\n",
              "      <td>245.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>-0.063635</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.035829</td>\n",
              "      <td>-0.022885</td>\n",
              "      <td>-0.030464</td>\n",
              "      <td>-0.018850</td>\n",
              "      <td>-0.006584</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.025952</td>\n",
              "      <td>-0.054925</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>-0.067268</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.012673</td>\n",
              "      <td>-0.040099</td>\n",
              "      <td>-0.015328</td>\n",
              "      <td>0.004636</td>\n",
              "      <td>-0.058127</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.019199</td>\n",
              "      <td>-0.034215</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>-0.107226</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.077342</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>-0.089630</td>\n",
              "      <td>-0.096198</td>\n",
              "      <td>0.026550</td>\n",
              "      <td>-0.076395</td>\n",
              "      <td>-0.042572</td>\n",
              "      <td>-0.005220</td>\n",
              "      <td>137.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.023677</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.059541</td>\n",
              "      <td>-0.040099</td>\n",
              "      <td>-0.042848</td>\n",
              "      <td>-0.043589</td>\n",
              "      <td>0.011824</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.015998</td>\n",
              "      <td>0.040343</td>\n",
              "      <td>85.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.052606</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.021295</td>\n",
              "      <td>-0.074528</td>\n",
              "      <td>-0.040096</td>\n",
              "      <td>-0.037639</td>\n",
              "      <td>-0.006584</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.000609</td>\n",
              "      <td>-0.054925</td>\n",
              "      <td>131.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.067136</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.006206</td>\n",
              "      <td>0.063187</td>\n",
              "      <td>-0.042848</td>\n",
              "      <td>-0.095885</td>\n",
              "      <td>0.052322</td>\n",
              "      <td>-0.076395</td>\n",
              "      <td>0.059424</td>\n",
              "      <td>0.052770</td>\n",
              "      <td>283.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>0.074401</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.085408</td>\n",
              "      <td>0.063187</td>\n",
              "      <td>0.014942</td>\n",
              "      <td>0.013091</td>\n",
              "      <td>0.015505</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.006209</td>\n",
              "      <td>0.085907</td>\n",
              "      <td>261.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>-0.052738</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.000817</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>0.010815</td>\n",
              "      <td>0.007141</td>\n",
              "      <td>0.048640</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.035817</td>\n",
              "      <td>0.019633</td>\n",
              "      <td>113.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>0.081666</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.006728</td>\n",
              "      <td>-0.004523</td>\n",
              "      <td>0.109883</td>\n",
              "      <td>0.117056</td>\n",
              "      <td>-0.032356</td>\n",
              "      <td>0.091875</td>\n",
              "      <td>0.054724</td>\n",
              "      <td>0.007207</td>\n",
              "      <td>131.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>-0.005515</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.008883</td>\n",
              "      <td>-0.050428</td>\n",
              "      <td>0.025950</td>\n",
              "      <td>0.047224</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>0.071210</td>\n",
              "      <td>0.014823</td>\n",
              "      <td>0.003064</td>\n",
              "      <td>174.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>-0.027310</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.080019</td>\n",
              "      <td>0.098763</td>\n",
              "      <td>-0.002945</td>\n",
              "      <td>0.018101</td>\n",
              "      <td>-0.017629</td>\n",
              "      <td>0.003312</td>\n",
              "      <td>-0.029528</td>\n",
              "      <td>0.036201</td>\n",
              "      <td>257.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417</th>\n",
              "      <td>-0.052738</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.071397</td>\n",
              "      <td>-0.074528</td>\n",
              "      <td>-0.015328</td>\n",
              "      <td>-0.001314</td>\n",
              "      <td>0.004460</td>\n",
              "      <td>-0.021412</td>\n",
              "      <td>-0.046879</td>\n",
              "      <td>0.003064</td>\n",
              "      <td>55.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>0.009016</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.024529</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>0.098876</td>\n",
              "      <td>0.094196</td>\n",
              "      <td>0.070730</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.021394</td>\n",
              "      <td>0.007207</td>\n",
              "      <td>84.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419</th>\n",
              "      <td>-0.020045</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.054707</td>\n",
              "      <td>-0.053871</td>\n",
              "      <td>-0.066239</td>\n",
              "      <td>-0.057367</td>\n",
              "      <td>0.011824</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.074089</td>\n",
              "      <td>-0.005220</td>\n",
              "      <td>42.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420</th>\n",
              "      <td>0.023546</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.036385</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.001183</td>\n",
              "      <td>0.034698</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>-0.033249</td>\n",
              "      <td>0.061054</td>\n",
              "      <td>146.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>0.038076</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.016428</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>0.039710</td>\n",
              "      <td>0.045032</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>0.071210</td>\n",
              "      <td>0.049769</td>\n",
              "      <td>0.015491</td>\n",
              "      <td>212.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>-0.078165</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.077863</td>\n",
              "      <td>0.052858</td>\n",
              "      <td>0.078236</td>\n",
              "      <td>0.064447</td>\n",
              "      <td>0.026550</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.040672</td>\n",
              "      <td>-0.009362</td>\n",
              "      <td>233.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>0.009016</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.039618</td>\n",
              "      <td>0.028758</td>\n",
              "      <td>0.038334</td>\n",
              "      <td>0.073529</td>\n",
              "      <td>-0.072854</td>\n",
              "      <td>0.108111</td>\n",
              "      <td>0.015567</td>\n",
              "      <td>-0.046641</td>\n",
              "      <td>91.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>0.001751</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.011039</td>\n",
              "      <td>-0.019442</td>\n",
              "      <td>-0.016704</td>\n",
              "      <td>-0.003819</td>\n",
              "      <td>-0.047082</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.024053</td>\n",
              "      <td>0.023775</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>-0.078165</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.040696</td>\n",
              "      <td>-0.081414</td>\n",
              "      <td>-0.100638</td>\n",
              "      <td>-0.112795</td>\n",
              "      <td>0.022869</td>\n",
              "      <td>-0.076395</td>\n",
              "      <td>-0.020289</td>\n",
              "      <td>-0.050783</td>\n",
              "      <td>152.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>0.030811</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.034229</td>\n",
              "      <td>0.043677</td>\n",
              "      <td>0.057597</td>\n",
              "      <td>0.068831</td>\n",
              "      <td>-0.032356</td>\n",
              "      <td>0.057557</td>\n",
              "      <td>0.035462</td>\n",
              "      <td>0.085907</td>\n",
              "      <td>120.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>-0.034575</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.005650</td>\n",
              "      <td>-0.005671</td>\n",
              "      <td>-0.073119</td>\n",
              "      <td>-0.062691</td>\n",
              "      <td>-0.006584</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.045421</td>\n",
              "      <td>0.032059</td>\n",
              "      <td>67.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>0.048974</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.088642</td>\n",
              "      <td>0.087287</td>\n",
              "      <td>0.035582</td>\n",
              "      <td>0.021546</td>\n",
              "      <td>-0.024993</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.066048</td>\n",
              "      <td>0.131470</td>\n",
              "      <td>310.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>-0.041840</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.033151</td>\n",
              "      <td>-0.022885</td>\n",
              "      <td>0.046589</td>\n",
              "      <td>0.041587</td>\n",
              "      <td>0.056003</td>\n",
              "      <td>-0.024733</td>\n",
              "      <td>-0.025952</td>\n",
              "      <td>-0.038357</td>\n",
              "      <td>94.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>-0.009147</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.056863</td>\n",
              "      <td>-0.050428</td>\n",
              "      <td>0.021822</td>\n",
              "      <td>0.045345</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>-0.009919</td>\n",
              "      <td>-0.017646</td>\n",
              "      <td>183.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>0.070769</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.030996</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>-0.037344</td>\n",
              "      <td>-0.047034</td>\n",
              "      <td>0.033914</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.014956</td>\n",
              "      <td>-0.001078</td>\n",
              "      <td>66.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>432</th>\n",
              "      <td>0.009016</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.055229</td>\n",
              "      <td>-0.005671</td>\n",
              "      <td>0.057597</td>\n",
              "      <td>0.044719</td>\n",
              "      <td>-0.002903</td>\n",
              "      <td>0.023239</td>\n",
              "      <td>0.055684</td>\n",
              "      <td>0.106617</td>\n",
              "      <td>173.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>-0.027310</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.060097</td>\n",
              "      <td>-0.029771</td>\n",
              "      <td>0.046589</td>\n",
              "      <td>0.019980</td>\n",
              "      <td>0.122273</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.051401</td>\n",
              "      <td>-0.009362</td>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>0.016281</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.001339</td>\n",
              "      <td>0.008101</td>\n",
              "      <td>0.005311</td>\n",
              "      <td>0.010899</td>\n",
              "      <td>0.030232</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.045421</td>\n",
              "      <td>0.032059</td>\n",
              "      <td>49.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>-0.012780</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.023451</td>\n",
              "      <td>-0.040099</td>\n",
              "      <td>-0.016704</td>\n",
              "      <td>0.004636</td>\n",
              "      <td>-0.017629</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.038459</td>\n",
              "      <td>-0.038357</td>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>-0.056370</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.074108</td>\n",
              "      <td>-0.050428</td>\n",
              "      <td>-0.024960</td>\n",
              "      <td>-0.047034</td>\n",
              "      <td>0.092820</td>\n",
              "      <td>-0.076395</td>\n",
              "      <td>-0.061177</td>\n",
              "      <td>-0.046641</td>\n",
              "      <td>48.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.019662</td>\n",
              "      <td>0.059744</td>\n",
              "      <td>-0.005697</td>\n",
              "      <td>-0.002566</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.031193</td>\n",
              "      <td>0.007207</td>\n",
              "      <td>178.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>-0.005515</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.015906</td>\n",
              "      <td>-0.067642</td>\n",
              "      <td>0.049341</td>\n",
              "      <td>0.079165</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>-0.018118</td>\n",
              "      <td>0.044485</td>\n",
              "      <td>104.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>-0.015906</td>\n",
              "      <td>0.017282</td>\n",
              "      <td>-0.037344</td>\n",
              "      <td>-0.013840</td>\n",
              "      <td>-0.024993</td>\n",
              "      <td>-0.011080</td>\n",
              "      <td>-0.046879</td>\n",
              "      <td>0.015491</td>\n",
              "      <td>132.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>-0.045472</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.001215</td>\n",
              "      <td>0.016318</td>\n",
              "      <td>0.015283</td>\n",
              "      <td>-0.028674</td>\n",
              "      <td>0.026560</td>\n",
              "      <td>0.044528</td>\n",
              "      <td>-0.025930</td>\n",
              "      <td>220.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>-0.045472</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.073030</td>\n",
              "      <td>-0.081414</td>\n",
              "      <td>0.083740</td>\n",
              "      <td>0.027809</td>\n",
              "      <td>0.173816</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.004220</td>\n",
              "      <td>0.003064</td>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>442 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          age       sex       bmi  ...        s5        s6  target\n",
              "0    0.038076  0.050680  0.061696  ...  0.019908 -0.017646   151.0\n",
              "1   -0.001882 -0.044642 -0.051474  ... -0.068330 -0.092204    75.0\n",
              "2    0.085299  0.050680  0.044451  ...  0.002864 -0.025930   141.0\n",
              "3   -0.089063 -0.044642 -0.011595  ...  0.022692 -0.009362   206.0\n",
              "4    0.005383 -0.044642 -0.036385  ... -0.031991 -0.046641   135.0\n",
              "5   -0.092695 -0.044642 -0.040696  ... -0.041180 -0.096346    97.0\n",
              "6   -0.045472  0.050680 -0.047163  ... -0.062913 -0.038357   138.0\n",
              "7    0.063504  0.050680 -0.001895  ... -0.035817  0.003064    63.0\n",
              "8    0.041708  0.050680  0.061696  ... -0.014956  0.011349   110.0\n",
              "9   -0.070900 -0.044642  0.039062  ...  0.067736 -0.013504   310.0\n",
              "10  -0.096328 -0.044642 -0.083808  ... -0.062913 -0.034215   101.0\n",
              "11   0.027178  0.050680  0.017506  ... -0.096433 -0.059067    69.0\n",
              "12   0.016281 -0.044642 -0.028840  ... -0.030751 -0.042499   179.0\n",
              "13   0.005383  0.050680 -0.001895  ...  0.038393 -0.013504   185.0\n",
              "14   0.045341 -0.044642 -0.025607  ... -0.031991 -0.075636   118.0\n",
              "15  -0.052738  0.050680 -0.018062  ...  0.036056 -0.042499   171.0\n",
              "16  -0.005515 -0.044642  0.042296  ...  0.052280  0.027917   166.0\n",
              "17   0.070769  0.050680  0.012117  ...  0.027368 -0.001078   144.0\n",
              "18  -0.038207 -0.044642 -0.010517  ... -0.018118 -0.017646    97.0\n",
              "19  -0.027310 -0.044642 -0.018062  ... -0.008944 -0.054925   168.0\n",
              "20  -0.049105 -0.044642 -0.056863  ... -0.011901  0.015491    68.0\n",
              "21  -0.085430  0.050680 -0.022373  ... -0.072128 -0.017646    49.0\n",
              "22  -0.085430 -0.044642 -0.004050  ... -0.061177 -0.013504    68.0\n",
              "23   0.045341  0.050680  0.060618  ...  0.133599  0.135612   245.0\n",
              "24  -0.063635 -0.044642  0.035829  ... -0.025952 -0.054925   184.0\n",
              "25  -0.067268  0.050680 -0.012673  ...  0.019199 -0.034215   202.0\n",
              "26  -0.107226 -0.044642 -0.077342  ... -0.042572 -0.005220   137.0\n",
              "27  -0.023677 -0.044642  0.059541  ... -0.015998  0.040343    85.0\n",
              "28   0.052606 -0.044642 -0.021295  ... -0.000609 -0.054925   131.0\n",
              "29   0.067136  0.050680 -0.006206  ...  0.059424  0.052770   283.0\n",
              "..        ...       ...       ...  ...       ...       ...     ...\n",
              "412  0.074401 -0.044642  0.085408  ...  0.006209  0.085907   261.0\n",
              "413 -0.052738 -0.044642 -0.000817  ... -0.035817  0.019633   113.0\n",
              "414  0.081666  0.050680  0.006728  ...  0.054724  0.007207   131.0\n",
              "415 -0.005515 -0.044642  0.008883  ...  0.014823  0.003064   174.0\n",
              "416 -0.027310 -0.044642  0.080019  ... -0.029528  0.036201   257.0\n",
              "417 -0.052738 -0.044642  0.071397  ... -0.046879  0.003064    55.0\n",
              "418  0.009016 -0.044642 -0.024529  ... -0.021394  0.007207    84.0\n",
              "419 -0.020045 -0.044642 -0.054707  ... -0.074089 -0.005220    42.0\n",
              "420  0.023546 -0.044642 -0.036385  ... -0.033249  0.061054   146.0\n",
              "421  0.038076  0.050680  0.016428  ...  0.049769  0.015491   212.0\n",
              "422 -0.078165  0.050680  0.077863  ...  0.040672 -0.009362   233.0\n",
              "423  0.009016  0.050680 -0.039618  ...  0.015567 -0.046641    91.0\n",
              "424  0.001751  0.050680  0.011039  ...  0.024053  0.023775   111.0\n",
              "425 -0.078165 -0.044642 -0.040696  ... -0.020289 -0.050783   152.0\n",
              "426  0.030811  0.050680 -0.034229  ...  0.035462  0.085907   120.0\n",
              "427 -0.034575  0.050680  0.005650  ... -0.045421  0.032059    67.0\n",
              "428  0.048974  0.050680  0.088642  ...  0.066048  0.131470   310.0\n",
              "429 -0.041840 -0.044642 -0.033151  ... -0.025952 -0.038357    94.0\n",
              "430 -0.009147 -0.044642 -0.056863  ... -0.009919 -0.017646   183.0\n",
              "431  0.070769  0.050680 -0.030996  ... -0.014956 -0.001078    66.0\n",
              "432  0.009016 -0.044642  0.055229  ...  0.055684  0.106617   173.0\n",
              "433 -0.027310 -0.044642 -0.060097  ... -0.051401 -0.009362    72.0\n",
              "434  0.016281 -0.044642  0.001339  ... -0.045421  0.032059    49.0\n",
              "435 -0.012780 -0.044642 -0.023451  ... -0.038459 -0.038357    64.0\n",
              "436 -0.056370 -0.044642 -0.074108  ... -0.061177 -0.046641    48.0\n",
              "437  0.041708  0.050680  0.019662  ...  0.031193  0.007207   178.0\n",
              "438 -0.005515  0.050680 -0.015906  ... -0.018118  0.044485   104.0\n",
              "439  0.041708  0.050680 -0.015906  ... -0.046879  0.015491   132.0\n",
              "440 -0.045472 -0.044642  0.039062  ...  0.044528 -0.025930   220.0\n",
              "441 -0.045472 -0.044642 -0.073030  ... -0.004220  0.003064    57.0\n",
              "\n",
              "[442 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5C6wsebNJhfM"
      },
      "source": [
        "### End of notebook."
      ]
    }
  ]
}