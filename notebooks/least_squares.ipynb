{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares (Linear) Regression\n",
    "\n",
    "- From [this Least Squares example](https://github.com/SergioRAgostinho/bootstrap-ml/blob/master/least_squares.ipynb) Notebook.\n",
    "    - Author:  [Sérgio Agostinho](https://www.linkedin.com/in/sergioagostinho)\n",
    "    - [Sérgio's GitHub repository](https://github.com/SergioRAgostinho/bootstrap-ml)\n",
    "\n",
    "Updated by [John Fogarty](https://github.com/jfogarty) for Python 3.6 and [Base2 MLI](https://github.com/base2solutions/mli) and [colab](https://colab.research.google.com) standalone evaluation.\n",
    "\n",
    "This was made from the [Bootstrap-ML](http://sergioagostinho.com/bootstrap-ml) workshop which took place on the 22nd of March 2018 at the Mathematics Department of [Instituto Superior Técnico](https://tecnico.ulisboa.pt/en/).\n",
    "\n",
    "This workshop, given by Sérgio Agostinh, was a two-hour introductory session in Machine Learning (ML)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We're going to tackle one of the most simple and yet powerful tool of Machine Learning, the Least Squares estimator. Given a function in the shape of \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat y = w^T\\Phi\\left(x\\right) + b \\label{linear}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat y, b \\in \\mathbb{R}$ and $\\Phi$ is a mapping from our input space to a given feature space such that $\\Phi: \\mathbb{R}^m \\to \\mathbb{R}^n$, we want to estimate the optimal $w$ which minimizes\n",
    "\n",
    "\\begin{equation}\n",
    "e = \\sum^{N}_{i = 1} \\left(y - \\hat y\\right)^2 \\label{sse}\n",
    "\\end{equation}\n",
    "\n",
    "## Problem: fit the a straight line through a number of points\n",
    "\n",
    "This is the most basic and common use case, so we need to see it in action. Let's generate a number of random points between $\\left[0, 1\\right]$ according to the following model\n",
    "\n",
    "\\begin{equation}\n",
    "y = 2x + 1 + e\n",
    "\\end{equation}\n",
    "\n",
    "where $e \\sim \\mathcal{N} \\left(0, 0.2^2\\right)$. In this case $w \\in \\mathbb{R}$ and is equal to $2$, $x \\in \\mathcal{U} \\left(0, 1\\right)$ and $b=1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "n = 100\n",
    "x = np.random.uniform(size=n)\n",
    "e = np.random.normal(scale=0.4, size=n)\n",
    "y = 2*x + 1 + e\n",
    "\n",
    "#\n",
    "plt.scatter(x=x, y=y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find the best $w$ and $b$?\n",
    "\n",
    "Let's derive this one time so that you understand the most basic concepts of all these algorithms. We are interested in finding the weight vector $w$ which minimizes the sum of the square error\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{w}{\\operatorname{argmin}} e = \\sum^{N}_{i = 1} \\left(y_i - w^T \\Phi(x_i) - b \\right)^2 \n",
    "\\end{equation}\n",
    "\n",
    "We can augment $w$ such that $w_0 = b$ and $w \\in \\mathbb{R}^{n+1}$ and rerite the expression above as\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{w}{\\operatorname{argmin}} e = \\sum^{N}_{i = 1} \\left(y_i - w^T \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\right)^2 \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Since the equation is linear, it has a closed-form solution. Let's start by finding the $w$ which gives as a null gradient. We're also going to make use of an important property of matrix derivatives, in this case applied to vectors\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d x^Ta}{dx} =  a\n",
    "\\end{equation}\n",
    "\n",
    "for $a, x \\in \\mathbb{R}^n$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{d}{dw} \\sum^{N}_{i = 1} \\left(y_i - w^T \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\right)^2  & = & 0 \\\\\n",
    "\\sum^{N}_{i = 1} \\frac{d}{dw} \\left(y_i - w^T \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\right)^2 & = & 0 \\\\\n",
    "\\sum^{N}_{i = 1} -2 \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\left(y_i - w^T \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\right) & = & 0 \\\\\n",
    "\\sum^{N}_{i = 1} \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\left(\\left[\\begin{matrix}\\Phi(x_i)^T & 1 \\end{matrix} \\right] w - y_i\\right) & = & 0 \\\\\n",
    "\\sum^{N}_{i = 1} \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] \\left[\\begin{matrix}\\Phi(x_i)^T & 1 \\end{matrix} \\right] w & = & \\sum^{N}_{i = 1} \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] y_i \\\\\n",
    "\\sum^{N}_{i = 1} \\left[\\begin{matrix}\\Phi(x_i)\\Phi(x_i)^T & \\Phi(x_i) \\\\ \\Phi(x_i)^T & 1 \\end{matrix} \\right] w & = & \\sum^{N}_{i = 1} \\left[\\begin{matrix}\\Phi(x_i) \\\\ 1 \\end{matrix} \\right] y_i \\\\\n",
    " \\left[\\begin{matrix}\\sum^{N}_{i = 1} \\Phi(x_i)\\Phi(x_i)^T & \\sum^{N}_{i = 1} \\Phi(x_i) \\\\ \\sum^{N}_{i = 1} \\Phi(x_i)^T & N \\end{matrix} \\right] w & = & \\left[\\begin{matrix} \\sum^{N}_{i = 1}\\Phi(x_i) y_i \\\\ \\sum^{N}_{i = 1}y_i \\end{matrix} \\right]\n",
    "\\end{eqnarray}\n",
    "\n",
    "resulting of course in\n",
    "\n",
    "\\begin{equation}\n",
    " w = \\left[\\begin{matrix}\\sum^{N}_{i = 1} \\Phi(x_i)\\Phi(x_i)^T & \\sum^{N}_{i = 1} \\Phi(x_i) \\\\ \\sum^{N}_{i = 1} \\Phi(x_i)^T & N \\end{matrix} \\right]^{-1}\\left[\\begin{matrix} \\sum^{N}_{i = 1}\\Phi(x_i) y_i \\\\ \\sum^{N}_{i = 1}y_i \\end{matrix} \\right]\\\\\n",
    "\\end{equation}\n",
    "\n",
    "Let's apply this proof to our case. First we need to figure what is what. We know the data was generated under the following model\n",
    "\n",
    "\\begin{equation}\n",
    "y = 2x + 1\n",
    "\\end{equation}\n",
    "\n",
    "plus some added noise. In this case the $\\Phi(x) = x$, $w = 2$ and $b = 1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy [linalg.solve](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html)\n",
    "\n",
    "We have a handy function available that solves exactly this kind of system, but we had to do a lot of math first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[np.dot(x, x), np.sum(x)],[np.sum(x), n]])\n",
    "Y = np.array([np.dot(x,y), np.sum(y)])\n",
    "theta = np.linalg.solve(X,Y)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o', label='original data')\n",
    "plt.plot(x, theta[0]*x + theta[1], 'r', label='fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn [linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) \n",
    "\n",
    "Scikit already provides linear regression capabilities among others. So everything we went through could have been replaced by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x.reshape(-1,1), y)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "print('Intercept: \\n', regr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we needed to use [numpy reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) here so that our inputs to the fit function are a 2D matix \n",
    "\n",
    "See [this example for more on using sklearn linear models](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"x's shape was {y.shape}\")\n",
    "print(f\"x is reshaped to {x.reshape(-1,1).shape}\")\n",
    "print(f\"y's shape is {y.shape}\")\n",
    "print(\"--So for each entry in Y we have an input vector in X containing exactly one item.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus math\n",
    "\n",
    "If you haven't had enough, feel free to note this nice equivalence in linear algebra using normal equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations\n",
    "\n",
    "\\begin{equation}\n",
    " w = \\left[\\begin{matrix}\\sum^{N}_{i = 1} \\Phi(x_i)\\Phi(x_i)^T & \\sum^{N}_{i = 1} \\Phi(x_i)^T \\\\ \\sum^{N}_{i = 1} \\Phi(x_i) & N \\end{matrix} \\right]^{-1}\\left[\\begin{matrix} \\sum^{N}_{i = 1}\\Phi(x_i) y_i \\\\ \\sum^{N}_{i = 1}y_i \\end{matrix} \\right]\\\\\n",
    "\\end{equation}\n",
    "\n",
    "Can be written in another way commonly know as the **normal equations**. Just like before, consider you have $N$ samples and you stack your features and your target like this\n",
    "\n",
    "\\begin{equation}\n",
    "X =  \\left[\\begin{matrix}\\Phi(x_1)^T & \\dots  & 1 \\\\\n",
    "                         \\vdots    & \\ddots & \\vdots \\\\\n",
    "                         \\Phi(x_N)^T & \\dots  & 1 \\end{matrix}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{y} =  \\left[\\begin{matrix} y_1 \\\\ \\vdots \\\\ y_N \\end{matrix}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The best $w$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{w} = (X^TX)^{-1} X^T  \\textbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "Let's test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.matrix([x, np.ones(len(x))]).T\n",
    "Y = np.matrix(y).T\n",
    "print(\"X shape:\\n\", X.shape)\n",
    "print(\"Y shape:\\n\", Y.shape)\n",
    "\n",
    "w = np.linalg.solve(X.T * X, X.T * Y)\n",
    "print(\"w:\\n\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
