{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decision_trees_intro.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (py36)",
      "language": "python",
      "name": "py36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfogarty/machine-learning-intro-workshop/blob/master/notebooks/decision_trees_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ctLiSDR4RHFN"
      },
      "source": [
        "# An Introduction to Decision Trees\n",
        "\n",
        "- First section [A Guide to Decision Trees for Machine Learning and Data Science](https://towardsdatascience.com/a-guide-to-decision-trees-for-machine-learning-and-data-science-fe2607241956)) by George Seif in [towardsdatascience.com](https://towardsdatascience.com)\n",
        "- Additional notes incorporated from [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/tree.html) by Christoph Molnar.\n",
        "- Last section [Scikit-Learn Decision Trees Explained](https://towardsdatascience.com/scikit-learn-decision-trees-explained-803f3812290d) by Frank Ceballos in [towardsdatascience.com](https://towardsdatascience.com)\n",
        "- Additional resource [Understanding Decision Trees for Classification (Python)](https://towardsdatascience.com/understanding-decision-trees-for-classification-python-9663d683c952) by Michael Galarnyk.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sCTBAVXHXpY_"
      },
      "source": [
        "Updated by [John Fogarty](https://github.com/jfogarty) for Python 3.6 and [Base2 MLI](https://github.com/base2solutions/mli)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "im5EhMsKRaQc"
      },
      "source": [
        "Decision Trees are a class of very powerful Machine Learning model capable of achieving high accuracy in many tasks while being highly interpretable. What makes decision trees special in the realm of ML models is really their clarity of information representation. The “knowledge” learned by a decision tree through training is directly formulated into a hierarchical structure. This structure holds and displays the knowledge in such a way that it can easily be understood, even by non-experts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAum6aCKRvH8"
      },
      "source": [
        "## Decision Trees in Real-Life\n",
        "\n",
        "You’ve probably used a decision tree before to make a decision in your own life. Take for example the decision about what activity you should do this weekend. It might depend on whether or not you feel like going out with your friends or spending the weekend alone; in both cases, your decision also depends on the weather. If it’s sunny and your friends are available, you may want to play soccer. If it ends up raining you’ll go to a movie. And if your friends don’t show up at all, well then you like playing video games no matter what the weather is like!\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/dt_1.png?raw=1\" /></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R7Qpq3iaSB3b"
      },
      "source": [
        "This is a clear example of a real-life decision tree. We’ve built a tree to model a set of **sequential, hierarchical decisions** that ultimately lead to some final result. Notice that we’ve also chosen our decisions to be quite “high-level” in order to keep the tree small. For example, what if we set up many possible options for the weather such as 25 degrees sunny, 25 degrees raining, 26 degrees sunny, 26 degrees raining, 27 degrees sunny…. etc, our tree would be huge! The **exact** temperature really isn’t too relevant, we just want to know whether it’s OK to be outside or not.\n",
        "\n",
        "The concept is the same for decision trees in Machine Learning. We want to build a tree with a set of hierarchical decisions which eventually give us a final result, i.e our classification or regression prediction. The decisions will be selected such that the tree is as small as possible while aiming for high classification / regression accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tu1Ka7SDSN6H"
      },
      "source": [
        "### Induction\n",
        "\n",
        "From a high level, decision tree induction goes through 4 main steps to build the tree:\n",
        "\n",
        "1. Begin with your training dataset, which should have some feature variables and classification or regression output.\n",
        "\n",
        "2. Determine the “best feature” in the dataset to split the data on; more on how we define “best feature” later\n",
        "\n",
        "3. Split the data into subsets that contain the possible values for this best feature. This splitting basically defines a node on the tree i.e each node is a splitting point based on a certain feature from our data.\n",
        "\n",
        "4. Recursively generate new tree nodes by using the subset of data created from step 3. We keep splitting until we reach a point where we have optimised, by some measure, maximum accuracy while minimising the number of splits / nodes.\n",
        "\n",
        "Step 1 is easy, just grab your dataset!\n",
        "\n",
        "For step 2, the selection of which feature to use and the specific split is commonly chosen using a greedy algorithm to minimise a cost function. If we think about it for a second, performing a split when building a decision tree is equivalent to dividing up the feature space. We will iteratively try out different split points and then at the end select the one that has the lowest cost. Of course we can do a couple of smart things like only splitting within the range of values in our dataset. This will keep us from wasting computations on testing out split points that are trivially poor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dHHTqG1lSgNx"
      },
      "source": [
        "For a [**regression tree**](http://saedsayad.com/decision_tree_reg.htm), we want to build a decision tree that reports an analog response - prices, temperature, etc.  We can use a simple squared error as our cost function:\n",
        "\n",
        "$$\n",
        "E = \\sum (Y - \\hat{Y})^2\n",
        "$$\n",
        "\n",
        "Where $Y$ is our ground truth and $\\hat{Y}$ is our predicted value; we sum over all the samples in our dataset to get the total error. \n",
        "\n",
        "A [**classification tree**](https://www.solver.com/classification-tree) assigns nodes to discrete classes; for this we use the [Gini Index Function](https://en.wikipedia.org/wiki/Gini_coefficient):\n",
        "\n",
        "$$\n",
        "E = \\sum (p_k * (1 - p_k))\n",
        "$$\n",
        "\n",
        "Where $p_k$ are the proportion of training instances of class $k$ in a particular prediction node. A node should *ideally* have an error value of zero, which means that each split outputs a single class 100% of the time. This is exactly what we want because then we know, once we get to that particular decision node, what exactly our output will be whether we are on one side of the decision boundary or the other.\n",
        "\n",
        "Sometimes instead of the Gini coefficient, a very similar measurement called entropy is used:\n",
        "\n",
        "$$\n",
        "E = - \\sum  p_k * log_2(p_k)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fwC4B1DfUNXL"
      },
      "source": [
        "This concept of having a single class per-split across our dataset is known as *information gain*. Check out the example below.\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/dt_2.png?raw=1\" /></center>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M-LyIzrfUZjf"
      },
      "source": [
        "If we were to choose a split where each output has a mix of classes depending on the input data, then we really haven’t *gained* any information at all; we don’t know any better whether or not a particular node i.e feature has any influence in classifying our data! On the other hand, if our split has a high percentage of each class for each output, then we have *gained* the information that splitting in that particular way on that particular feature variable gives us a particular output!\n",
        "\n",
        "Now we could of course keep splitting and splitting and splitting until our tree has thousands of branches….. but that’s not really such a good idea! Our decision tree would be huge, slow, and overfitted to our training dataset. Thus, we will set some predefined stopping criterion to halt the construction of the tree.\n",
        "\n",
        "The most common stopping method is to use a minimum count on the number of training examples assigned to each leaf node. If the count is less than some minimum value then the split is not accepted and the node is taken as a final leaf node. If all of our leaf nodes become final, the training stops. A smaller minimum count will give you finer splits and potentially more information, but is also prone to overfitting on your training data. Too large of a min count and you might stop too early. As such, the min value is usually set based on the dataset, depending on how many examples are expected to be in each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9QSjNGjcUcmn"
      },
      "source": [
        "### Pruning\n",
        "\n",
        "Because of the nature of training decision trees they can be prone to major overfitting. Setting the correct value for minimum number of instances per node can be challenging. Most of the time, we might just go with a safe bet and make that minimum quite small, resulting in there being many splits and a very large, complex tree. The key is that many of these splits will end up being redundant and unnecessary to increasing the accuracy of our model.\n",
        "\n",
        "Tree pruning is a technique that leverages this splitting redundancy to remove i.e *prune* the unnecessary splits in our tree. From a high-level, pruning compresses part of the tree from strict and rigid decision boundaries into ones that are more smooth and generalise better, effectively reducing the tree complexity. The complexity of a decision tree is defined as the number of splits in the tree.\n",
        "\n",
        "A simple yet highly effective pruning method is to go through each node in the tree and evaluate the effect of removing it on the cost function. If it doesn’t change much, then prune away!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UUYN-xPhJW6y"
      },
      "source": [
        "## Advantages\n",
        "The tree structure is ideal for **capturing interactions** between features in the data.\n",
        "\n",
        "The data ends up in **distinct groups** that are often easier to understand than points on a multi-dimensional hyperplane as in linear regression. The interpretation is arguably pretty simple.\n",
        "\n",
        "The tree structure also has a **natural visualization**, with its nodes and edges.\n",
        "\n",
        "Trees create good explanations. The tree structure automatically invites you to think about predicted values for individual instances as counterfactuals: “If a feature had been greater / smaller than the split point, the prediction would have been $y1$ instead of $y2$. The tree explanations are contrastive, since you can always compare the prediction of an instance with relevant ”what if“-scenarios (as defined by the tree) that are simply the other leaf nodes of the tree. If the tree is short, like one to three splits deep, the resulting explanations are selective. A tree with a depth of three requires a maximum of three features and split points to create the explanation for the prediction of an individual instance. The truthfulness of the prediction depends on the predictive performance of the tree. The explanations for short trees are very simple and general, because for each split the instance falls into either one or the other leaf, and binary decisions are easy to understand.\n",
        "\n",
        "There is no need to transform features. In linear models, it is sometimes necessary to take the logarithm of a feature. A decision tree works equally well with any monotonic transformation of a feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y-zq3kr3JW60"
      },
      "source": [
        "## Disadvantages\n",
        "\n",
        "**Trees fail to deal with linear relationships**. Any linear relationship between an input feature and the outcome has to be approximated by splits, creating a step function. This is not efficient.\n",
        "\n",
        "This goes hand in hand with **lack of smoothness**. Slight changes in the input feature can have a big impact on the predicted outcome, which is usually not desirable. Imagine a tree that predicts the value of a house and the tree uses the size of the house as one of the split feature. The split occurs at 100.5 square meters. Imagine user of a house price estimator using your decision tree model: They measure their house, come to the conclusion that the house has 99 square meters, enter it into the price calculator and get a prediction of 200 000 Euro. The users notice that they have forgotten to measure a small storage room with 2 square meters. The storage room has a sloping wall, so they are not sure whether they can count all of the area or only half of it. So they decide to try both 100.0 and 101.0 square meters. The results: The price calculator outputs 200 000 Euro and 205 000 Euro, which is rather unintuitive, because there has been no change from 99 square meters to 100.\n",
        "\n",
        "Trees are also quite **unstable**. A few changes in the training dataset can create a completely different tree. This is because each split depends on the parent split. And if a different feature is selected as the first split feature, the entire tree structure changes. It does not create confidence in the model if the structure changes so easily.\n",
        "\n",
        "Decision trees are very interpretable – as long as they are short. **The number of terminal nodes increases quickly with depth**. The more terminal nodes and the deeper the tree, the more difficult it becomes to understand the decision rules of a tree. A depth of 1 means 2 terminal nodes. Depth of 2 means max. 4 nodes. Depth of 3 means max. 8 nodes. The maximum number of terminal nodes in a tree is 2 to the power of the depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fDV7Pu9GVLJC"
      },
      "source": [
        "# Python code\n",
        "\n",
        "**Usage NOTE!** Use `Shift+Enter` to step through this notebook, executing the code as you go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "seaocorXUc8G"
      },
      "source": [
        "## An Example in Scikit Learn\n",
        "\n",
        "Decision trees for both classification and regression are super easy to use in Scikit Learn with a built in class! We’ll first load in our dataset and initialise our decision tree for classification. Running training is then a simple one-liner!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a4QNXg0dOzVM",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "\n",
        "# Load in our dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Initialize our decision tree object\n",
        "classification_tree = tree.DecisionTreeClassifier()\n",
        "\n",
        "# Train our decision tree (tree induction + pruning)\n",
        "classification_tree = classification_tree.fit(iris.data, iris.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-PhWAKScSM8t"
      },
      "source": [
        "Scikit Learn also allows us to visualise our tree using the graphviz library. It comes with a few options that will help in visualising the decision nodes and splits that the model learned which is super useful for understanding how it all works! Below we will colour the nodes based on the feature names and display the class and feature information of each node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aQVj4_MwJW7G",
        "colab": {}
      },
      "source": [
        "tree.plot_tree(classification_tree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2NXNkafuJW7O"
      },
      "source": [
        "### Graphviz display of the tree\n",
        "\n",
        "**[python-graphviz](https://anaconda.org/conda-forge/python-graphviz)** is another great visualization tool based on the [graphviz](https://www.graphviz.org/) tool suite. Sadly, it can be a bit of a pain to install in some environments [read [Windows](https://github.com/ContinuumIO/anaconda-issues/issues/1666)] so expect issues such as:\n",
        "\n",
        "```\n",
        "    ExecutableNotFound: failed to execute ['dot', '-Tpdf', '-O', 'iris'], make sure the Graphviz executables are on your systems' PATH\n",
        "```\n",
        "\n",
        "These can be solved on a system by system basis, but there is no universal solution for all platforms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dd9M_3mGPAlk",
        "colab": {}
      },
      "source": [
        "import graphviz \n",
        "from IPython.display import Image\n",
        "\n",
        "dot_data = tree.export_graphviz(\n",
        "    classification_tree, out_file=None, \n",
        "    feature_names=iris.feature_names,  \n",
        "    class_names=iris.target_names,  \n",
        "    filled=True,\n",
        "    rounded=True,  \n",
        "    special_characters=True\n",
        ")  \n",
        "graph = graphviz.Source(dot_data)  \n",
        "thefile = graph.render(\"iris\", format='png') \n",
        "Image(filename=thefile) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eqCf5J4eO2PC"
      },
      "source": [
        "## Adjusting decision tree parameters\n",
        "\n",
        "There are several parameters that you can set for your decision tree model in Scikit Learn too. Here are a few of the more interesting ones to play around with to try and get some better results:\n",
        "\n",
        "- **max_depth**: The max depth of the tree where we will stop splitting the nodes. This is similar to controlling the maximum number of layers in a deep neural network. Lower will make your model faster but not as accurate; higher can give you accuracy but risks overfitting and may be slow.\n",
        "\n",
        "- **min_samples_split**: The minimum number of samples required to split a node. We discussed this aspect of decision trees above and how setting it to a higher value would help mitigate overfitting.\n",
        "\n",
        "- **max_features**: The number of features to consider when looking for the best split. Higher means potentially better results with the tradeoff of training taking longer.\n",
        "\n",
        "- **min_impurity_split**: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold. This can be used to tradeoff combating overfitting (high value, small tree) vs high accuracy (low value, big tree).\n",
        "\n",
        "- **presort**: Whether to presort the data to speed up the finding of best splits in fitting. If we sort our data on each feature beforehand, our training algorithm will have a much easier time finding good values to split on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EM1GE0bjWOjS"
      },
      "source": [
        "## Tips for Practically Applying Decision Trees\n",
        "\n",
        "Here are a few of the pro and cons of decision trees that can help you decide on whether or not it’s the right model for your problem, as well as some tips as to how you can effectively apply them:\n",
        "\n",
        "### Pros\n",
        "\n",
        "- **Easy to understand and interpret**. At each node, we are able to see exactly what decision our model is making. In practice we’ll be able to fully understand where our accuracies and errors are coming from, what type of data the model would do well with, and how the output is influenced by the values of the features. Scikit learn’s visualisation tool is a fantastic option for visualising and understanding decision trees.\n",
        "\n",
        "- **Require very little data preparation**. Many ML models may require heavy data pre-processing such as normalization and may require complex regularisation schemes. Decision trees on the other hand work quite well out of the box after tweaking a few of the parameters.\n",
        "\n",
        "- **The cost of using the tree for inference is logarithmic in the number of data points used to train the tree**. That’s a huge plus since it means that having more data won’t necessarily make a huge dent in our inference speed.\n",
        "\n",
        "### Cons\n",
        "\n",
        "- **Overfitting is quite common** with decision trees simply due to the nature of their training. It’s often recommended to perform some type of dimensionality reduction such as PCA so that the tree doesn’t have to learn splits on so many features.\n",
        "\n",
        "- **Vulnerable to bias**. For similar reasons as the case of overfitting, decision trees are also vulnerable to becoming biased to the classes that have a majority in the dataset. It’s always a good idea to do some kind of class balancing such as class weights, sampling, or a specialised loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IC-Y-OUFJW7f"
      },
      "source": [
        "# Scikit-Learn Decision Trees Explained\n",
        "\n",
        "Decision trees are the most important elements of a **[Random Forest](https://en.wikipedia.org/wiki/Random_forest)**. They are capable of fitting complex data sets while allowing the user to see how a decision was taken. While searching the web I was unable to find one clear article that could easily describe them, so here I am writing about what I have learned so far. It’s important to note, a single decision tree is not a very good predictor; however, one can create an ensemble of them (a forest) and collect their predictions.  Then, by allowing each tree vote for its prediction and selecting the winner, one of the most powerful machine learning tools can be obtained — the so called Random Forest.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5UTVKZXOJW7g",
        "colab": {}
      },
      "source": [
        "# Importing Libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load data and store it into pandas DataFrame objects\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data[:, :], columns = iris.feature_names[:])\n",
        "y = pd.DataFrame(iris.target, columns =[\"Species\"])\n",
        "\n",
        "# Defining and fitting a DecisionTreeClassifier instance\n",
        "tree = DecisionTreeClassifier(max_depth = 2)\n",
        "tree.fit(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nvpgwhdSJW7l"
      },
      "source": [
        "## Visualizing The Decision Tree\n",
        "Of course we still do not know how this tree classifies samples, so let’s visualize this tree by first creating a dot file using Scikit-Learn export_graphviz module and then processing it with graphviz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "toEftHIpJW7n",
        "colab": {}
      },
      "source": [
        "# Visualize Decision Tree\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "# Creates dot file named tree.dot\n",
        "export_graphviz(\n",
        "            tree,\n",
        "            out_file =  \"myTreeName.dot\",\n",
        "            feature_names = list(X.columns),\n",
        "            class_names = iris.target_names,\n",
        "            filled = True,\n",
        "            rounded = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Av1vDvw2JW7x"
      },
      "source": [
        "This will create a file named tree.dot that needs to be processed on graphviz. The end result should be similar to the one shown in Figure-1; however, a different tree might sprout even if the training data is the same!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gTM0rD1WJW7y",
        "colab": {}
      },
      "source": [
        "thefile = graph.render(\"myTreeName.dot\", format='png') \n",
        "Image(filename=thefile) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b5e4nttzJW75"
      },
      "source": [
        "<figure><br>\n",
        "  <center><img src=\"https://miro.medium.com/max/484/1*gql3YU01S-BUiCt8mgE66w.png\" />\n",
        "  <figcaption>Our decision tree: In this case, nodes are colored in white, while leaves are colored in orange,\n",
        "      green, and purple. More about leaves and nodes later.\n",
        "   </figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2E2UvLiAJW7-"
      },
      "source": [
        "### Understanding the Contents of a Node\n",
        "\n",
        "In the figure above, you can see that each box contains several characteristics. Let’s start by describing the content of the top most node, most commonly referred to as the root node. The root node is at a depth of zero, see below. A node is a point along the decision tree graph, where a question is asked. This action divides the data into smaller subsets.\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://miro.medium.com/max/700/1*tMU0XhEbj5aKgGt9RX-UQQ.png\" /></center>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2eUCsaPGJW8A"
      },
      "source": [
        "- **petal length (cm) <=2.45**: The first question the decision tree asks is if the petal length is less than 2.45. Based on the result, it either follows the true or the false path.\n",
        "\n",
        "- **gini = 0.667**: The gini score is a metric that quantifies the purity of the node/leaf (more about leaves in a bit). A gini score greater than zero implies that samples contained within that node belong to different classes. A gini score of zero means that the node is pure, that within that node only a single class of samples exist. You can find out more about impurity measures here. Notice that we have a gini score greater than zero; therefore, we know that the samples contained within the root node belong to different classes.\n",
        "\n",
        "- **samples = 150**: Since the iris flower data set contains 150 samples, this value is set to 150.\n",
        "\n",
        "- **value = [50, 50, 50]**: The value list tells you how many samples at the given node fall into each category. The first element of the list shows the number of samples that belong to the setosa class, the second element of the list shows the number of samples that belong to the versicolor class, and the third element in the list shows the number of samples that belong to the virginica class. Notice how this node is not a pure one since different types of classes are contained within the same node. We knew this already from the gini score, but it’s nice to actually see it.\n",
        "\n",
        "- **class = setosa**: The class value shows the prediction a given node will make and it can be determined from the value list. Whichever class occurs the most within the node will be selected as the class value. If the decision tree were to end at the root node, it would predict that all 150 samples belonged to the setosa class. Of course this makes no sense, since there is an equal number of samples for each class. It seems to me that the decision tree is programmed to choose the first class on the list if there is an equal number of samples for each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U6Fej-wsJW8B"
      },
      "source": [
        "### Understanding How a Tree Makes a Split\n",
        "\n",
        "To determine which feature to use to make the first split — that is, to make the root node — the algorithm chooses a feature and makes a split. It then looks at the subsets and measures their impurity using the gini score. It does this for multiple thresholds and determines that the best split for the given feature is the one that produces the purest subsets. This is repeated for all the features in the training set. Ultimately, the root node is determined by the feature that produces a split with purest subsets. Once the root node is decided, the tree is grown to a depth of one. The same process is repeated for the other nodes in the tree.\n",
        "\n",
        "### Understanding How a Tree Will Make a Prediction\n",
        "\n",
        "Suppose we have a flower with `petal_length = 1` and `petal_width = 3`. If we follow the logic of the decision tree shown on Figure-1, we will see that we will end up in the orange box. In the tree graph, if the question a node asks turns out to be true (false), we will move to the left (right). The orange box is at a depth of one, see above. Since there is nothing growing out of this box, we will refer to it as a leaf node. Notice the resemblance this has to an actual tree, see below. Moreover, note that the gini score is zero — which makes it a pure leaf. The total number of samples is 50. Out of the 50 samples that end up on the orange leaf node, we can see that all of them belong to the setosa class, see the value list for this leaf. Therefore, the tree will predict that the sample is a setosa flower.\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/dt_3.png?raw=1\" />\n",
        "      <figcaption>Real tree vs Decision Tree Similarity: The tree on the left is inverted to illustrate how a tree grows from its root and ends at its leaves. Seeing the decision tree on the right should make this analogy more clear.\n",
        "   </figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F46KfJyRJW8E"
      },
      "source": [
        "### Making a Prediction On New Samples Using a Trained Tree\n",
        "\n",
        "Now that we know how our decision tree works, let us make predictions. The input should be in a list and ordered as [sepal length, sepal width, petal length, petal width] where the sepal length and sepal width won't affect the predictions made by the decision tree shown in the graph; therefore, we can assign them an arbitrary value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xWlqkJL-JW8G",
        "colab": {}
      },
      "source": [
        "# Making a Prediction On a New Sample\n",
        "sample_one_pred = int(tree.predict([[5, 5, 1, 3]]))\n",
        "sample_two_pred = int(tree.predict([[5, 5, 2.6, 1.5]]))\n",
        "print(f\"The first sample most likely belongs to the {iris.target_names[sample_one_pred]} class.\")\n",
        "print(f\"The second sample most likely belongs to the {iris.target_names[sample_two_pred]} class.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ocQVSvqJW8Q"
      },
      "source": [
        "## Scikit-Learn Decision Tree Parameters\n",
        "\n",
        "If you take a look at the parameters the DecisionTreeClassifier can take, you might be surprised so, let’s look at some of them.\n",
        "\n",
        "- **criterion**: This parameter determines how the impurity of a split will be measured. The default value is “gini” but you can also use “entropy” as a metric for impurity.\n",
        "\n",
        "- **splitter**: This is how the decision tree searches the features for a split. The default value is set to “best”. That is, for each node, the algorithm considers all the features and chooses the best split. If you decide to set the splitter parameter to “random,” then a random subset of features will be considered. The split will then be made by the best feature within the random subset. The size of the random subset is determined by the max_features parameter. This is partly where a Random Forest gets its name.\n",
        "\n",
        "- **max_depth**: This determines the maximum depth of the tree. In our case, we use a depth of two to make our decision tree. The default value is set to none. This will often result in over-fitted decision trees. The depth parameter is one of the ways in which we can regularize the tree, or limit the way it grows to prevent over-fitting. Below, you can see what happens if you don’t set the depth of the tree — pure madness!\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/dt_4.png?raw=1\" />\n",
        "      <figcaption>A fully grown Decision Tree: In the tree shown above, none of the parameters were set. The tree grows fully to a depth of five. There are eight nodes and nine leaves. Not limiting the growth of a decision tree may lead to over-fitting.\n",
        "   </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "- **min_samples_split**: The minimum number of samples a node must contain in order to consider splitting. The default value is two. You can use this parameter to regularize your tree.\n",
        "\n",
        "- **min_samples_leaf**: The minimum number of samples needed to be considered a leaf node. The default value is set to one. Use this parameter to limit the growth of the tree.\n",
        "\n",
        "- **max_features**: The number of features to consider when looking for the best split. If this value is not set, the decision tree will consider all features available to make the best split. Depending on your application, it’s often a good idea to tune this parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bl2opXn9JW8S",
        "colab": {}
      },
      "source": [
        "# Setting parameters\n",
        "tree = DecisionTreeClassifier(criterion = \"entropy\", splitter = \"random\", max_depth = 2,  min_samples_split = 5,\n",
        "                              min_samples_leaf = 2, max_features = 2)\n",
        "tree.fit(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Op7eyhs1KIzk",
        "colab": {}
      },
      "source": [
        "# Visualize Decision Tree\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "# Creates dot file named tree.dot\n",
        "dotfile = \"anotherTreeName.dot\"\n",
        "export_graphviz(\n",
        "            tree,\n",
        "            out_file = dotfile,\n",
        "            feature_names = list(X.columns),\n",
        "            class_names = iris.target_names,\n",
        "            filled = True,\n",
        "            rounded = True)\n",
        "thefile = graph.render(dotfile, format='png') \n",
        "Image(filename=thefile) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vxPsnILdJW8Y"
      },
      "source": [
        "## Back to the Forest...\n",
        "\n",
        "Now you know how to create a decision tree using Scikit-learn. More importantly, you should be able to visualize it and understand how it classifies samples. It’s important to note that one needs to limit the liberty of a decision tree. There are several parameters that can regularize a tree. By default, the max_depth is set to none. Therefore, a tree will grow fully, which often results in over-fitting. Moreover, a single decision tree is not a very powerful predictor.\n",
        "\n",
        "The real power of decision trees unfolds more so when cultivating many of them — while limiting the way they grow — and collecting their individual predictions to form a final conclusion. In other words, you grow a forest, and if your forest is random in nature, using the concept of bagging and with splitter = \"random\", we call this a Random Forest. Many of the parameters used in Scikit-Learn Random Forest are the same ones explained in this article. So it’s a good idea to understand what a single decision tree is and how it works, before moving on to using the big guns.\n",
        "\n",
        "Let's build a forest.  In this case, we use 20 decision trees, each with a maximum depth of 2 - much shorter than the trees we produced above, so they won't all be correct.  We also set a random number seed, and ignore warnings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gSKOoQ3OIZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "sherwood = RandomForestClassifier(n_estimators=20, max_depth=2, random_state=5)\n",
        "sherwood.fit(X, y)\n",
        "print(f\"Relative feature importances: {sherwood.feature_importances_}\")\n",
        "sample_pred = sherwood.predict([[5,5,1,3]])\n",
        "print(f\"This sample most likely belongs to the {iris.target_names[sample_pred]} class.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9MLKWLRQvNT",
        "colab_type": "text"
      },
      "source": [
        "One important note: you still need to tune your hyperparameters a bit even when working with a mighty forest.  For example, if you don't have a large enough max_depth, then your forest might still get its answer wrong despite lots of trees voting.  For example, if you set a random number seed of 0 and max_depth of 2, this example comes up with the wrong answer, even though most other seeds get the right answer.  If you increase the max_depth, then it will eventually find the correct class even with the unlucky seed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-mf7bZjS1eG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sherwood = RandomForestClassifier(n_estimators=20, max_depth=2, random_state=0)\n",
        "sherwood.fit(X, y)\n",
        "sample_pred = sherwood.predict([[5,5,1,3]])\n",
        "print(f\"Our forest says this most likely belongs to the {iris.target_names[sample_pred]} class, but we know better.\")\n",
        "\n",
        "sherwood = RandomForestClassifier(n_estimators=20, max_depth=3, random_state=0)\n",
        "sherwood.fit(X, y)\n",
        "sample_pred = sherwood.predict([[5,5,1,3]])\n",
        "print(f\"Our DEEPER forest says this most likely belongs to the {iris.target_names[sample_pred]}.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1bZowfnaBTv",
        "colab_type": "text"
      },
      "source": [
        "<figure><br>\n",
        "  <center><img src=\"https://raw.githubusercontent.com/KevinSikorskiBase2S/machine-learning-intro-workshop/master/images/entMoot.gif\" />\n",
        "      <figcaption>Rare footage of an entmoot, where trees gather to vote on what to do about deforestation, jewelry, and short people.\n",
        "   </figcaption></center>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SsCQej6ZVuan"
      },
      "source": [
        "### End of note."
      ]
    }
  ]
}