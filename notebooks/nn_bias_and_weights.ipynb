{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jfogarty/machine-learning-intro-workshop/blob/master/notebooks/nn_bias_and_weights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOAKu9LrExqR"
   },
   "source": [
    "# Neural Networks Bias And Weights\n",
    "\n",
    "## Understanding The Two Most Important Components\n",
    "\n",
    "- From [Neural Networks Bias And Weights](https://medium.com/fintechexplained/neural-networks-bias-and-weights-10b53e6285da) by Farhad Malik in [medium.com](https://medium.com/)\n",
    "\n",
    "This article aims to provide an overview of what bias and weights are. The weights and bias are possibly the most important concept of a neural network. When the inputs are transmitted between neurons, the weights are applied to the inputs and passed into an activation function along with the bias.\n",
    "\n",
    "Updated by [John Fogarty](https://github.com/jfogarty) for Python 3.6 and [Base2 MLI](https://github.com/base2solutions/mli) and [colab](https://colab.research.google.com) standalone evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCGMsFoiExxh"
   },
   "source": [
    "## What Are Weights?\n",
    "\n",
    "This is an example neural network with 2 hidden layers and an input and output layer. Each synapse has a weight associated with it.\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <br><center>\n",
    "    <img src=\"../images/nn_weights.png\" />\n",
    "    <figcaption></figcaption>     \n",
    "  </center>\n",
    "</figure>\n",
    "\n",
    "**Note!** in the above example, there are only two weights from each of the input neurons, as well as only one from each node in the last hidden layer to the output layer.  This is a bit misleading since most simple neural networks are **fully connected**, which is to say that each node in prior later has weights which link to every node in the next layer.\n",
    "\n",
    "<figure>\n",
    "  <br><center>\n",
    "    <img src=\"../images/mlp.jpg\" />\n",
    "    <figcaption>This is a more typical fully connected simple MLP</figcaption>     \n",
    "  </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TDm4NO2BExup"
   },
   "source": [
    "> ***Weights are the co-efficients of the equation which you are trying to resolve. Negative weights reduce the value of an output.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EWbRrps_FT04"
   },
   "source": [
    "When a neural network is trained on the training set, it is initialised with a set of weights. These weights are then optimised during the training period and the optimum weights are produced.\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <br><center>\n",
    "    <img src=\"../images/nn_neuron.png\" />\n",
    "    <figcaption></figcaption>     \n",
    "  </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rNzBs6VFV0I"
   },
   "source": [
    "A neuron first computes the weighted sum of the inputs.\n",
    "\n",
    "$$\n",
    "    Y = \\sum (weight * input) + bias\n",
    "$$\n",
    "\n",
    "As an instance, if the inputs are:\n",
    "\n",
    "$$\n",
    "    x_1,x_2,...,x_n\n",
    "$$\n",
    "\n",
    "And the weights are:\n",
    "\n",
    "$$\n",
    "    w_1,w_2,...,w_n\n",
    "$$\n",
    "\n",
    "Then a weighted sum is computed as:\n",
    "\n",
    "$$\n",
    "    x_1 w_1 + x_2 w_2 + ... + x_n w_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_NuWgsvHGB1B"
   },
   "source": [
    "Subsequently, a bias (constant) is added to the weighted sum\n",
    "\n",
    "$$\n",
    "    x_1 w_1 + x_2 w_2 + ... + x_n w_n + bias\n",
    "$$\n",
    "\n",
    "> ***Think of the activation function as a mathematical function that can normalise the inputs.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pb7BG6ErFVwu"
   },
   "source": [
    "## Let’s understand with a scenario:\n",
    "\n",
    "Assume you are predicting the price of a car in dollars. Your understanding is that the price of the car is dependent on the year it was made and the number of miles it has driven.\n",
    "\n",
    "Let’s assume that your hypothesis is that the higher the year of the car, the pricey the car. And subsequently, the more the car is driven, the cheaper the car.\n",
    "\n",
    "> ***The weights are essentially reflecting how important an input is.***\n",
    "\n",
    "This example should help you see that there is a positive relationship between the price of the car and the year it was made and a negative relationship between the price of the car and the miles it has been driven. As a result, we expect to see positive weight for the feature that represents year and negative weight for the feature that represents miles.\n",
    "<br><br>\n",
    "$$\n",
    "    price\\ of\\ car = (w_1 * Year + w_2 * Miles)\n",
    "$$\n",
    "<br>\n",
    "*w1 is going to be positive and w2 is expected to be negative*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REp6OPn-H3oH"
   },
   "source": [
    "## Imagine this scenario:\n",
    "\n",
    "Let’s assume you want your neural network to return 2 when the input is 0. As the sum of product of weight and input is going to be 0, how will you ensure the neuron of the network returns 2?\n",
    "\n",
    "*You can add a bias of 2.*\n",
    "\n",
    "If we do not include the bias then the neural network is simply performing a matrix multiplication on the inputs and weights. This can easily end up over-fitting the data set.\n",
    "\n",
    "> ***The addition of bias reduces the variance and hence introduces flexibility and better generlisation to the neural network.***\n",
    "\n",
    "Bias is essentially the negative of the threshold, therefore the value of bias controls when to activate the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "182v53G3FVtk"
   },
   "source": [
    "### End of notebook."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "nn_bias_and_weights.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
