{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "colab": {
      "name": "binary_functions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfogarty/machine-learning-intro-workshop/blob/master/notebooks/math-of-neural-networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DnUCpC5PBGtJ"
      },
      "source": [
        "# Math of Neural Networks - from scratch in Python\n",
        "\n",
        "\n",
        "This is Colab implementation from [Math of Neural Networks — from scratch in Python](https://medium.com/datadriveninvestor/math-neural-network-from-scratch-in-python-d6da9f29ce65) on medium.com by **Omar Aflak**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iIaGFxWkbC4N"
      },
      "source": [
        "In this notebook we will go through the mathematics of machine learning and code from scratch, in Python, a small library to build neural networks with a variety of layers (**Fully Connected**, **Convolutional**, etc.). \n",
        "\n",
        "Eventually, we will be able to create networks in a modular fashion (very similar to the [Keras Sequential](https://keras.io/models/sequential/) framework used) :\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-3-layer.png?raw=1\" />\n",
        "  <figcaption>3-layer neural network</fgcaption></center>\n",
        "</figure>\n",
        "\n",
        "I’m assuming you already have some knowledge about neural networks. The purpose here is not to explain why we make these models, but to show **how to make a proper implementation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2A564BkCdlIc"
      },
      "source": [
        "## Layer by Layer\n",
        "\n",
        "We need to keep in mind the big picture here :\n",
        "\n",
        "1. We feed input data into the neural network.\n",
        "1. The data flows from layer to layer until we have the output.\n",
        "1. Once we have the output, we can calculate the error which is a scalar.\n",
        "1. Finally we can adjust a given parameter (weight or bias) by subtracting the derivative of the error with respect to the parameter itself.\n",
        "1. We iterate through that process.\n",
        "\n",
        "The most important step is the **4th**. We want to be able to have as many layers as we want, and of any type. But if we modify/add/remove one layer from the network, the output of the network is going to change, which is going to change the error, which is going to change the derivative of the error with respect to the parameters. We need to be able to compute the derivatives regardless of the network architecture, regardless of the activation functions, regardless of the loss we use.\n",
        "\n",
        "In order to achieve that, we must implement **each layer separately**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YFovI2K6c031"
      },
      "source": [
        "## What every layer should implement\n",
        "\n",
        "Every layer that we might create (fully connected, convolutional, maxpooling, dropout, etc.) have at least 2 things in common : **input** and **output** data.\n",
        "\n",
        "<figure>&nbsp;&nbsp;&nbsp;\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-xy-layer.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WQlJ-eQ0eKhT"
      },
      "source": [
        "### Forward propagation\n",
        "\n",
        "We can already emphasize one important point which is : **the output of one layer is the input of the next one**.\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-forward-propagation.png?raw=1\" />\n",
        "  <figcaption>forward propagation</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "This is called **forward propagation**. Essentially, we give the input data to the first layer, then the output of every layer becomes the input of the next layer until we reach the end of the network. By comparing the result of the network ($Y$) with the desired output (let’s say $Y^*$), we can calculate en error $E$. \n",
        "\n",
        "The goal is to minimize that error by changing the parameters in the network. That is backward propagation (backpropagation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cZEEtn42eX0b"
      },
      "source": [
        "### Gradient Descent\n",
        "\n",
        "This is a quick **reminder**, if you need to learn more about [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) there are tons of resources on the [internet](https://www.washingtonpost.com/blogs/wonkblog/post/the-internet-is-in-fact-a-series-of-tubes/2011/09/20/gIQALZwfiK_blog.html).\n",
        "\n",
        "Basically, we want to change some parameter in the network (call it $w$, usually referred to as a [**weight**](https://hackernoon.com/everything-you-need-to-know-about-neural-networks-8988c3ee4491)) so that the total error **$E$ decreases**. There is a clever way to do it (not randomly) which is the following :\n",
        "\n",
        "<figure>&nbsp;&nbsp;&nbsp;\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-weight-adjust.png?raw=1\" />\n",
        "  <figcaption><br>systematic adjustment of weights</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "Where $α$ is a parameter in the range $[0,1]$ that we set and that is called the [**learning rate**](https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10). \n",
        "\n",
        "Anyway, the important thing here is $\\frac{\\partial E}{\\partial w}$ (the derivative of $E$ with respect to $w$). We need to be able to find the value of that expression for any parameter of the network regardless of its architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9PAQhqBg4o1W"
      },
      "source": [
        "### Backward propagation\n",
        "\n",
        "Suppose that we give a layer the **derivative of the error with respect to its output** ($\\frac{\\partial E}{\\partial Y}$), then it must be able to provide the **derivative of the error with respect to its input** ($\\frac{\\partial E}{\\partial X}$).\n",
        "\n",
        "<figure>&nbsp;&nbsp;&nbsp;\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-back-propagation.png?raw=1\" />\n",
        "  <figcaption><br>back propagation</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "Remember that $E$ is a scalar (a number) and $X$ and $Y$ are matrices.\n",
        "\n",
        "<figure>&nbsp;&nbsp;&nbsp;\n",
        "    <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-back-propagation-matrix.png?raw=1\" /></center>\n",
        "</figure>\n",
        "\n",
        "Let’s forget about $\\frac{\\partial E}{\\partial X}$ for now. The trick here, is that if we have access to $\\frac{\\partial E}{\\partial Y}$ we can very easily calculate $\\frac{\\partial E}{\\partial W}$ (if the layer has any trainable parameters) **without knowing anything about the network architecture!** We simply use the [chain rule](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review) :\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-chain-rule.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "The unknown is $\\frac{\\partial y_j}{\\partial w}$ which totally depends on how the layer is computing its output. So if every layer has access to $\\frac{\\partial E}{\\partial Y}$, where $Y$ is its own output, then we can update our parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z2Be1ACpeoKb"
      },
      "source": [
        "## But why ∂E/∂X ?\n",
        "\n",
        "Don’t forget, the output of one layer is the input of the next layer. Which means $\\frac{\\partial E}{\\partial X}$ for one layer is $\\frac{\\partial E}{\\partial Y}$ for the previous layer.\n",
        "\n",
        "That’s it; tt’s just a clever way to propagate the error!\n",
        "\n",
        "Again, we can use the chain rule :\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-chain-rule2.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "This is very important, it’s the *key* to understand backpropagation!\n",
        "\n",
        "After that, we’ll be able to code a Deep Convolutional Neural Network from scratch in no time!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fs5cHOmRe3kT"
      },
      "source": [
        "### Diagram to understand backpropagation\n",
        "\n",
        "This is what I described earlier. Layer 3 is going to update its parameters using $∂E/∂Y$, and is then going to pass $∂E/∂H_2$ to the previous layer, which is its own “$∂E/∂Y$”. Layer 2 is then going to do the same, and so on and so forth.\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-chain-rule3.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "This may seem abstract here, but it will get very clear when we will apply this to a specific type of layer. \n",
        "\n",
        "Speaking of abstract, now is a good time to write our first python class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oey6L9e0IShg"
      },
      "source": [
        "\n",
        "# And finally some Python code\n",
        "\n",
        "**Usage NOTE!** Use `Shift+Enter` to step through this notebook, executing the code as you go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "NDzdSWEGIShh",
        "colab": {}
      },
      "source": [
        "#@title Welcome\n",
        "import datetime\n",
        "print(f\"Welcome to exploring this notebook at {datetime.datetime.now()}! \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "reyL8y-z8hsU",
        "colab": {}
      },
      "source": [
        "class Context:\n",
        "    VERBOSE=False    # True for extensive logging during execution.\n",
        "    QUIET=False      # True for minimal logging during execution.\n",
        "    WARNINGS=False   # True to enable display of annoying but rarely useful messages."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YI5tmkdPfFnP"
      },
      "source": [
        "## Abstract Base Class : Layer\n",
        "\n",
        "The abstract class Layer, which all other layers will inherit from, handles simple properties which are an input, an output, and both a forward and backward methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MwSocVz4o1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Base class\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    # computes the output Y of a layer for a given input X\n",
        "    def forward_propagation(self, input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSQd6ul74o19",
        "colab_type": "text"
      },
      "source": [
        "As you can see there is an extra parameter in `backward_propagation` that I didn’t mention, it is the `learning_rate`. This parameter should be something like an update policy, or an optimizer as they call it in Keras, but for the sake of simplicity we’re simply going to pass a learning rate and update our parameters using gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROXAoSGU4o2A",
        "colab_type": "text"
      },
      "source": [
        "## Fully Connected Layer\n",
        "\n",
        "Now lets define and implement the first type of layer : fully connected layer or FC layer. FC layers are the most basic layers as every input neuron is connected to every output neurons. These connections are the weights ($W$), which is a matrix of $w$ parameters.\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-nn-layer.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "This may seem abstract here, but it will get very clear when we will apply this to a specific type of layer. \n",
        "\n",
        "Before we an implement this, we need to know how we will compute the `forward_propagation` and `backward_propagation` functions for the class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac6pbrsr4o2F",
        "colab_type": "text"
      },
      "source": [
        "### Forward Propagation\n",
        "\n",
        "The value of each output neuron can be calculated as the following :\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-fwd-output.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "With matrices, we can compute this formula for every output neuron in one shot using a [dot product](https://en.wikipedia.org/wiki/Dot_product) :\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-dot-product.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-dot-product2.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "We’re done with the forward pass. Now let’s do the backward pass of the FC layer.\n",
        "\n",
        "The $B$ vector ($b$ values) contains [bias values](https://www.geeksforgeeks.org/effect-of-bias-in-neural-network/). This is typically a non-zero constant (usually **1**) which like the intercept added in a linear equation. It is an additional parameter used to adjust the output along with the weighted sum of the inputs to the neuron. \n",
        "\n",
        "*Note that I’m not using any [activation function]() yet, that’s because we'will implement it in a separate layer!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYrFM0hH4o2H",
        "colab_type": "text"
      },
      "source": [
        "### Backward Propagation\n",
        "\n",
        "As we said, suppose we have a matrix containing the derivative of the error with respect to that layer’s output ($∂E/∂Y$). We need :\n",
        "\n",
        "1. The derivative of the error with respect to the parameters ($∂E/∂W$, $∂E/∂B$)\n",
        "2. The derivative of the error with respect to the input ($∂E/∂X$)\n",
        "\n",
        "Lets calculate $∂E/∂W$.\n",
        "\n",
        "This matrix should be the same size as $W$ itself : $i$ x $j$ where $i$ is the number of input neurons and $j$ the number of output neurons. \n",
        "\n",
        "We need **one gradient for every weight** :\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-weight-derivatives.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "Using the chain rule stated earlier, we can write :\n",
        "\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-weight-derivatives2.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "Therefore,\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-weight-derivatives3.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeKkCgTx4o2J",
        "colab_type": "text"
      },
      "source": [
        "That’s it we have the first formula to update the weights!\n",
        "\n",
        "Now lets calculate $∂E/∂B$.\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-update-weights.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "Again $∂E/∂B$ needs to be of the same size as $B$ itself, one gradient per bias. We can use the chain rule again :\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-update-weights2.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "And conclude that,\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-update-weights3.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-ZU7VNT4o2M",
        "colab_type": "text"
      },
      "source": [
        "Now that we have $∂E/∂W$ and $∂E/∂B$, we are left with $∂E/∂X$ which is **very important** as it will “act” as $∂E/∂Y$ for the layer before that one.\n",
        "\n",
        "Again, using the chain rule,\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-update-weights4.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "Finally, we can write the whole matrix :\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-update-weights5.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "That’s it ! We have the three formulas we needed for the **fully connected** (FC) layer !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9Quy0st4o2O",
        "colab_type": "text"
      },
      "source": [
        "## Coding the Fully Connected Layer\n",
        "\n",
        "We can now write some python code to bring this math to life !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYBih-Qp4o2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inherit from base class Layer\n",
        "class FCLayer(Layer):\n",
        "    # input_size = number of input neurons\n",
        "    # output_size = number of output neurons\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
        "        self.bias = np.random.rand(1, output_size) - 0.5\n",
        "\n",
        "    # returns output for a given input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = np.dot(self.input, self.weights) + self.bias\n",
        "        return self.output\n",
        "\n",
        "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        input_error = np.dot(output_error, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, output_error)\n",
        "        # dBias = output_error\n",
        "\n",
        "        # update parameters\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * output_error\n",
        "        return input_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEjwqMbE4o2W",
        "colab_type": "text"
      },
      "source": [
        "## Activation Layer\n",
        "\n",
        "All the calculation we did until now were completely linear. It's hopeless to learn anything with that kind of model. We need to add **non-linearity** to the model by applying non linear functions to the output of some layers. \n",
        "\n",
        "Now we need to redo the whole process for this new type of layer!\n",
        "\n",
        "No worries, it’s going to be way faster as there are no *learnable* parameters. We just need to calculate $∂E/∂X$.\n",
        "\n",
        "We will call $f$ and $f'$ the [**activation function**](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6) and its derivative respectively.\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-nonlinear.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "### Forward Propagation\n",
        "\n",
        "As you will see, it is quite straightforward. For a given input $X$ , the output is simply the activation function applied to every element of $X$. Which means **input** and **output** have the same **dimensions**.\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-nonlinear2.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "### Backward Propagation\n",
        "\n",
        "Given $∂E/∂Y$, we want to calculate $∂E/∂X$.\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-nonlinear3.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "Be careful, here we are using an **element-wise** multiplication between the two matrices (whereas in the formulas above, it was a dot product).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90QebACm4o2X",
        "colab_type": "text"
      },
      "source": [
        "## Coding the Activation Layer\n",
        "\n",
        "The code for the activation layer is as straightforward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDByw4ti4o2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inherit from base class Layer\n",
        "class ActivationLayer(Layer):\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    # returns the activated input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = self.activation(self.input)\n",
        "        return self.output\n",
        "\n",
        "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
        "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        return self.activation_prime(self.input) * output_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfeezaI84o2e",
        "colab_type": "text"
      },
      "source": [
        "Now lets write some activation functions and their derivatives. These will be used later to create an `ActivationLayer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fkyeYTN4o2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# activation functions and their derivatives\n",
        "def tanh(x):\n",
        "    return np.tanh(x);\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1-tanh(x)**2;\n",
        "\n",
        "def sigmoid(x):\n",
        "    s=1/(1+np.exp(-x))\n",
        "    return s\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    s=sigmoid(x)\n",
        "    ds=s*(1-s)\n",
        "    return ds\n",
        "\n",
        "def relu(x):\n",
        "    return np.where(x > 0, x, 0)\n",
        "\n",
        "def relu_prime(x):\n",
        "    v = relu(x)\n",
        "    return np.where(v > 0, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zWEH3tL4o2p",
        "colab_type": "text"
      },
      "source": [
        "### Activation Functions\n",
        "\n",
        "We've chosen to implement only one functions **tanh**, but quite a few others are available. The selection of functions is a key part of the network graph design, but outside the scope of this note.\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/activation-function-cheat-sheet.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/activation-function-derivatives.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_j0whS_4o2s",
        "colab_type": "text"
      },
      "source": [
        "## Loss Function\n",
        "\n",
        "Until now, for a given layer, we supposed that $∂E/∂Y$ was given (by the next layer). But what happens to the last layer?\n",
        "How does it get $∂E/∂Y$?\n",
        "\n",
        "We provide it manually, and it depends on how we define the error.\n",
        "\n",
        "The error of the network, which measures how good or bad the network did for a given input data, is defined by **you**. There are many ways to define the error, and one of the most known is called [**MSE — Mean Squared Error**](https://en.wikipedia.org/wiki/Mean_squared_error).\n",
        "\n",
        "Where $y^*$ and $y$ denotes desired output and actual output respectively. You can think of the loss as a last layer which takes all the output neurons and squashes them into one single neuron. What we need now, as for every other layer, is to define $∂E/∂Y$. Except now, we finally reached $E$ !\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/nn-scratch-mse.png?raw=1\" />\n",
        "  <figcaption></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "These are simply two python functions that you can put in a separate file. They will be used when creating the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S48MZrw54o2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# loss function and its derivative\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true-y_pred, 2));\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2*(y_pred-y_true)/y_true.size;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071Wcq9W4o20",
        "colab_type": "text"
      },
      "source": [
        "## Network Class\n",
        "\n",
        "We're almost done!\n",
        "\n",
        "We are going to make a Network class to create neural networks very easily akin to the first picture.\n",
        "\n",
        "I commented almost every part of the code, it shouldn’t be too complicated to understand if you grasped the previous steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htA9Lgcj4o22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.loss_prime = None\n",
        "\n",
        "    # add layer to network\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # set loss to use\n",
        "    def use(self, loss, loss_prime):\n",
        "        self.loss = loss\n",
        "        self.loss_prime = loss_prime\n",
        "\n",
        "    # predict output for given input\n",
        "    def predict(self, input_data):\n",
        "        # sample dimension first\n",
        "        samples = len(input_data)\n",
        "        result = []\n",
        "\n",
        "        # run network over all samples\n",
        "        for i in range(samples):\n",
        "            # forward propagation\n",
        "            output = input_data[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward_propagation(output)\n",
        "            result.append(output)\n",
        "\n",
        "        return result\n",
        "\n",
        "    # train the network\n",
        "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "        # sample dimension first\n",
        "        samples = len(x_train)\n",
        "\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "                # forward propagation\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward_propagation(output)\n",
        "\n",
        "                # compute loss (for display purpose only)\n",
        "                err += self.loss(y_train[j], output)\n",
        "\n",
        "                # backward propagation\n",
        "                error = self.loss_prime(y_train[j], output)\n",
        "                for layer in reversed(self.layers):\n",
        "                    error = layer.backward_propagation(error, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            err /= samples\n",
        "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LX9Gj3G4o28",
        "colab_type": "text"
      },
      "source": [
        "## Building Neural Networks\n",
        "\n",
        "Finally! \n",
        "\n",
        "We can use our class to create a neural network with as many layers as we want ! We are going to build two neural networks : a simple **XOR** and a **MNIST** solver.\n",
        "\n",
        "### Solve XOR\n",
        "\n",
        "Starting with XOR is always important as it’s a simple way to tell if the network is learning anything at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrAC6VIC4o3A",
        "colab_type": "text"
      },
      "source": [
        "I don’t think I need to emphasize many things. Just be careful with the training data, you should always have the number of elements in each **input sample first**. For example here, the input shape is **(4,1,2)**; there are 4 training cases, each training sample has one vector; the vector has two elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Z9lwJY4o3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# training data\n",
        "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
        "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
        "\n",
        "# network\n",
        "net = Network()\n",
        "net.add(FCLayer(2, 8))\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "net.add(FCLayer(8, 1))\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "\n",
        "print(f\"- The training set inputs shape: {x_train.shape}\")\n",
        "print(f\"- The training set output shape: {y_train.shape}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pFyShoH4o3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the xor function.\n",
        "net.use(mse, mse_prime)\n",
        "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8v7jZeu4o3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test the Xor function.\n",
        "out = net.predict(x_train)\n",
        "print(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pblwz23u4o3Y",
        "colab_type": "text"
      },
      "source": [
        "This should produce a result close to the training set output such as:\n",
        "\n",
        "```\n",
        "  [array([[0.00069917]]), array([[0.97479752]]), array([[0.97443034]]), array([[-0.0002348]])]\n",
        "```\n",
        "\n",
        "If this works (which it really, really, should), great!\n",
        "We can now solve something more interesting, let’s solve [MNIST](https://en.wikipedia.org/wiki/MNIST_database) [LeCunn](http://yann.lecun.com/exdb/mnist/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2SK2lMr4o3a",
        "colab_type": "text"
      },
      "source": [
        "## Solve MNIST\n",
        "\n",
        "We didn’t implemented the Convolutional Layer but this is not a problem. All we need to do is to reshape our data so that it \n",
        "can fit into a Fully Connected Layer.\n",
        "\n",
        "*The MNIST Dataset consists of images of digits from 0 to 9, of shape 28 x 28 x 1.*\n",
        "\n",
        "\n",
        "<figure><br>\n",
        "  <center><img src=\"https://github.com/jfogarty/machine-learning-intro-workshop/blob/master/images/MNIST.png?raw=1\" />\n",
        "  <figcaption>A sample of NMIST digits</figcaption></center>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-1VSFLA4o3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# load MNIST from server\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# training data : 60000 samples\n",
        "# reshape and normalize input data\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "# encode output which is a number in range [0,9] into a vector of size 10\n",
        "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "\n",
        "# same for test data : 10000 samples\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "print(f\"- The training set inputs shape: {x_train.shape}\")\n",
        "print(f\"- The training set output shape: {y_train.shape}\")\n",
        "print(\"\")\n",
        "print(f\"- The test set inputs shape: {x_test.shape}\")\n",
        "print(f\"- The test set output shape: {y_test.shape}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PUHJ2qb4o3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Network\n",
        "net = Network()\n",
        "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do4h6G_V4o31",
        "colab_type": "text"
      },
      "source": [
        "This implements simple full batch gradient descent which is pretty slow. Read [this](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) for info on [implementing mini batch gradient descent](https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/) which would be considerably faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5yKu7hx4o32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAINING_SAMPLES=1000\n",
        "TRAINING_EPOCHS=35\n",
        "LEARNING_RATE=0.1\n",
        "# train on 1000 samples\n",
        "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
        "\n",
        "# Define the error function to use, along with its first derivative.\n",
        "net.use(mse, mse_prime)\n",
        "\n",
        "# Do the actual training.\n",
        "net.fit(x_train[0:TRAINING_SAMPLES],\n",
        "        y_train[0:TRAINING_SAMPLES],\n",
        "        epochs=TRAINING_EPOCHS, \n",
        "        learning_rate=LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9pwzVUr4o4B",
        "colab_type": "text"
      },
      "source": [
        "Note that in epoch 1, the `error=0.241182` was an error rate of 25%.  After 35 epochs this shoud be close to 1% (0.01).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHQlwlio4o4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N=4\n",
        "# test on 4 samples\n",
        "out = net.predict(x_test[0:N])\n",
        "\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:6.3f}\".format(x)})\n",
        "\n",
        "print(\"--- predicted values : \")\n",
        "for v in out: print(v)\n",
        "\n",
        "print(\"--- true values : \")\n",
        "for v in y_test[0:N]: print(f\"[{v}]\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrDS9lIt4o4L",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This is working perfectly! \n",
        "\n",
        "**Amazing `:)`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "apClhi66ISja"
      },
      "source": [
        "### End of notebook."
      ]
    }
  ]
}