{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jfogarty/machine-learning-intro-workshop/blob/master/notebooks/binary_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnUCpC5PBGtJ"
   },
   "source": [
    "# Math of Neural Networks - from scratch in Python\n",
    "\n",
    "\n",
    "This is Colab implementation from [Math of Neural Networks — from scratch in Python](https://medium.com/datadriveninvestor/math-neural-network-from-scratch-in-python-d6da9f29ce65) on medium.com by **Omar Aflak**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iIaGFxWkbC4N"
   },
   "source": [
    "In this notebook we will go through the mathematics of machine learning and code from scratch, in Python, a small library to build neural networks with a variety of layers (**Fully Connected**, **Convolutional**, etc.). Eventually, we will be able to create networks in a modular fashion :\n",
    "\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-3-layer.png\" />\n",
    "  <figcaption>3-layer neural network</fgcaption></center>\n",
    "</figure>\n",
    "\n",
    "I’m assuming you already have some knowledge about neural networks. The purpose here is not to explain why we make these models, but to show **how to make a proper implementation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2A564BkCdlIc"
   },
   "source": [
    "## Layer by Layer\n",
    "\n",
    "We need to keep in mind the big picture here :\n",
    "\n",
    "1. We feed input data into the neural network.\n",
    "1. The data flows from layer to layer until we have the output.\n",
    "1. Once we have the output, we can calculate the error which is a scalar.\n",
    "1. Finally we can adjust a given parameter (weight or bias) by subtracting the derivative of the error with respect to the parameter itself.\n",
    "1. We iterate through that process.\n",
    "\n",
    "The most important step is the **4th**. We want to be able to have as many layers as we want, and of any type. But if we modify/add/remove one layer from the network, the output of the network is going to change, which is going to change the error, which is going to change the derivative of the error with respect to the parameters. We need to be able to compute the derivatives regardless of the network architecture, regardless of the activation functions, regardless of the loss we use.\n",
    "\n",
    "In order to achieve that, we must implement **each layer separately**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFovI2K6c031"
   },
   "source": [
    "## What every layer should implement\n",
    "\n",
    "Every layer that we might create (fully connected, convolutional, maxpooling, dropout, etc.) have at least 2 things in common : **input** and **output** data.\n",
    "\n",
    "<figure>&nbsp;&nbsp;&nbsp;\n",
    "  <center><img src=\"../images/nn-scratch-xy-layer.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQlJ-eQ0eKhT"
   },
   "source": [
    "### Forward propagation\n",
    "\n",
    "We can already emphasize one important point which is : **the output of one layer is the input of the next one**.\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-forward-propagation.png\" />\n",
    "  <figcaption>forward propagation</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "This is called **forward propagation**. Essentially, we give the input data to the first layer, then the output of every layer becomes the input of the next layer until we reach the end of the network. By comparing the result of the network ($Y$) with the desired output (let’s say $Y^*$), we can calculate en error $E$. The goal is to minimize that error by changing the parameters in the network. That is backward propagation (backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZEEtn42eX0b"
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "This is a quick **reminder**, if you need to learn more about [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) there are tons of resources on the [internet](https://www.washingtonpost.com/blogs/wonkblog/post/the-internet-is-in-fact-a-series-of-tubes/2011/09/20/gIQALZwfiK_blog.html).\n",
    "\n",
    "Basically, we want to change some parameter in the network (call it $w$, usually referred to as a [**weight**](https://hackernoon.com/everything-you-need-to-know-about-neural-networks-8988c3ee4491)) so that the total error **$E$ decreases**. There is a clever way to do it (not randomly) which is the following :\n",
    "\n",
    "<figure>&nbsp;&nbsp;&nbsp;\n",
    "  <center><img src=\"../images/nn-scratch-weight-adjust.png\" />\n",
    "  <figcaption><br>systematic adjustment of weights</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "Where $α$ is a parameter in the range $[0,1]$ that we set and that is called the [**learning rate**](https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10). \n",
    "\n",
    "Anyway, the important thing here is $\\frac{\\partial E}{\\partial w}$ (the derivative of $E$ with respect to $w$). We need to be able to find the value of that expression for any parameter of the network regardless of its architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZEEtn42eX0b"
   },
   "source": [
    "### Backward propagation\n",
    "\n",
    "Suppose that we give a layer the **derivative of the error with respect to its output** ($\\frac{\\partial E}{\\partial Y}$), then it must be able to provide the **derivative of the error with respect to its input** ($\\frac{\\partial E}{\\partial X}$).\n",
    "\n",
    "<figure>&nbsp;&nbsp;&nbsp;\n",
    "  <center><img src=\"../images/nn-scratch-back-propagation.png\" />\n",
    "  <figcaption><br>back propagation</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "Remember that $E$ is a scalar (a number) and $X$ and $Y$ are matrices.\n",
    "\n",
    "<figure>&nbsp;&nbsp;&nbsp;\n",
    "    <center><img src=\"../images/nn-scratch-back-propagation-matrix.png\" /></center>\n",
    "</figure>\n",
    "\n",
    "Let’s forget about $\\frac{\\partial E}{\\partial X}$ for now. The trick here, is that if we have access to $\\frac{\\partial E}{\\partial Y}$ we can very easily calculate $\\frac{\\partial E}{\\partial W}$ (if the layer has any trainable parameters) **without knowing anything about the network architecture!** We simply use the [chain rule](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review) :\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-chain-rule.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "The unknown is $\\frac{\\partial y_j}{\\partial w}$ which totally depends on how the layer is computing its output. So if every layer has access to $\\frac{\\partial E}{\\partial Y}$, where $Y$ is its own output, then we can update our parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2Be1ACpeoKb"
   },
   "source": [
    "## But why ∂E/∂X ?\n",
    "\n",
    "Don’t forget, the output of one layer is the input of the next layer. Which means $\\frac{\\partial E}{\\partial X}$ for one layer is $\\frac{\\partial E}{\\partial Y}$ for the previous layer.\n",
    "\n",
    "That’s it; tt’s just a clever way to propagate the error!\n",
    "\n",
    "Again, we can use the chain rule :\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-chain-rule2.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "This is very important, it’s the *key* to understand backpropagation!\n",
    "\n",
    "After that, we’ll be able to code a Deep Convolutional Neural Network from scratch in no time!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fs5cHOmRe3kT"
   },
   "source": [
    "### Diagram to understand backpropagation\n",
    "\n",
    "This is what I described earlier. Layer 3 is going to update its parameters using $∂E/∂Y$, and is then going to pass $∂E/∂H_2$ to the previous layer, which is its own “$∂E/∂Y$”. Layer 2 is then going to do the same, and so on and so forth.\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-chain-rule3.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "This may seem abstract here, but it will get very clear when we will apply this to a specific type of layer. \n",
    "\n",
    "Speaking of abstract, now is a good time to write our first python class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oey6L9e0IShg"
   },
   "source": [
    "\n",
    "# And finally some Python code\n",
    "\n",
    "**Usage NOTE!** Use `Shift+Enter` to step through this notebook, executing the code as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "NDzdSWEGIShh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to exploring this notebook at 2019-08-08 15:06:57.372928! \n"
     ]
    }
   ],
   "source": [
    "#@title Welcome\n",
    "import datetime\n",
    "print(f\"Welcome to exploring this notebook at {datetime.datetime.now()}! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "reyL8y-z8hsU"
   },
   "outputs": [],
   "source": [
    "class Context:\n",
    "    VERBOSE=False    # True for extensive logging during execution.\n",
    "    QUIET=False      # True for minimal logging during execution.\n",
    "    WARNINGS=False   # True to enable display of annoying but rarely useful messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YI5tmkdPfFnP"
   },
   "source": [
    "## Abstract Base Class : Layer\n",
    "\n",
    "The abstract class Layer, which all other layers will inherit from, handles simple properties which are an input, an output, and both a forward and backward methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is an extra parameter in `backward_propagation` that I didn’t mention, it is the `learning_rate`. This parameter should be something like an update policy, or an optimizer as they call it in Keras, but for the sake of simplicity we’re simply going to pass a learning rate and update our parameters using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer\n",
    "\n",
    "Now lets define and implement the first type of layer : fully connected layer or FC layer. FC layers are the most basic layers as every input neurons are connected to every output neurons.\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-nn-layer.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "This may seem abstract here, but it will get very clear when we will apply this to a specific type of layer. Speaking of abstract, now is a good time to write our first python class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "The value of each output neuron can be calculated as the following :\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-fwd-output.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "With matrices, we can compute this formula for every output neuron in one shot using a [dot product](https://en.wikipedia.org/wiki/Dot_product) :\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-dot-product.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "$$\n",
    "Y = XW + B\n",
    "$$\n",
    "\n",
    "We’re done with the forward pass. Now let’s do the backward pass of the FC layer.\n",
    "\n",
    "*Note that I’m not using any activation function yet, that’s because we will implement it in a separate layer !*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "\n",
    "As we said, suppose we have a matrix containing the derivative of the error with respect to that layer’s output ($∂E/∂Y$). We need :\n",
    "\n",
    "1. The derivative of the error with respect to the parameters ($∂E/∂W$, $∂E/∂B$)\n",
    "2. The derivative of the error with respect to the input ($∂E/∂X$)\n",
    "\n",
    "Lets calculate $∂E/∂W$. This matrix should be the same size as $W$ itself : $i$ x $j$ where $i$ is the number of input neurons and $j$ the number of output neurons. We need **one gradient for every weight** :\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-weight-derivatives.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "Using the chain rule stated earlier, we can write :\n",
    "\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-weight-derivatives2.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "Therefore,\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-weight-derivatives3.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s it we have the first formula to update the weights ! Now lets calculate $∂E/∂B$.\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-update-weights.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "Again $∂E/∂B$ needs to be of the same size as $B$ itself, one gradient per bias. We can use the chain rule again :\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-update-weights2.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "And conclude that,\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-update-weights3.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have $∂E/∂W$ and $∂E/∂B$, we are left with $∂E/∂X$ which is very important as it will “act” as $∂E/∂Y$ for the layer before that one.\n",
    "\n",
    "Again, using the chain rule,\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-update-weights4.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "Finally, we can write the whole matrix :\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-update-weights5.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "That’s it ! We have the three formulas we needed for the **fully connected** (FC) layer !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the Fully Connected Layer\n",
    "\n",
    "We can now write some python code to bring this math to life !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Layer\n",
    "\n",
    "All the calculation we did until now were completely linear. Its hopeless to learn anything with that kind of model. We need to add **non-linearity** to the model by applying non linear functions to the output of some layers.\n",
    "\n",
    "Now we need to redo the whole process for this new type of layer !\n",
    "\n",
    "No worries, it’s going to be way faster as there are no *learnable* parameters. We just need to calculate $∂E/∂X$.\n",
    "\n",
    "We will call $f$ and $f'$ the activation function and its derivative respectively.\n",
    "\n",
    "Finally, we can write the whole matrix :\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-nonlinear.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "As you will see, it is quite straightforward. For a given input $X$ , the output is simply the activation function applied to every element of $X$. Which means **input** and **output** have the same **dimensions**.\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-nonlinear2.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "### Backward Propagation\n",
    "\n",
    "Given $∂E/∂Y$, we want to calculate $∂E/∂X$.\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-nonlinear3.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "Be careful, here we are using an **element-wise** multiplication between the two matrices (whereas in the formulas above, it was a dot product).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the Activation Layer\n",
    "\n",
    "The code for the activation layer is as straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will write some activation functions and their derivatives. These will be used later to create an `ActivationLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "Until now, for a given layer, we supposed that $∂E/∂Y$ was given (by the next layer). But what happens to the last layer ? How does it get $∂E/∂Y$ ? We simply give it manually, and it depends on how we define the error.\n",
    "\n",
    "The error of the network, which measures how good or bad the network did for a given input data, is defined by **you**. There are many ways to define the error, and one of the most known is called [**MSE — Mean Squared Error**](https://en.wikipedia.org/wiki/Mean_squared_error).\n",
    "\n",
    "Where $y^*$ and $y$ denotes desired output and actual output respectively. You can think of the loss as a last layer which takes all the output neurons and squashes them into one single neuron. What we need now, as for every other layer, is to define $∂E/∂Y$. Except now, we finally reached $E$ !\n",
    "\n",
    "<figure><br>\n",
    "  <center><img src=\"../images/nn-scratch-mse.png\" />\n",
    "  <figcaption></figcaption></center>\n",
    "</figure>\n",
    "\n",
    "These are simply two python functions that you can put in a separate file. They will be used when creating the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class\n",
    "\n",
    "Almost done!\n",
    "\n",
    "We are going to make a Network class to create neural networks very easily akin the first picture !\n",
    "I commented almost every part of the code, it shouldn’t be too complicated to understand if you grasped the previous steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Networks\n",
    "\n",
    "Finally! \n",
    "\n",
    "We can use our class to create a neural network with as many layers as we want ! We are going to build two neural networks : a simple **XOR** and a **MNIST** solver.\n",
    "\n",
    "### Solve XOR\n",
    "\n",
    "Starting with XOR is always important as it’s a simple way to tell if the network is learning anything at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don’t think I need to emphasize many things. Just be careful with the training data, you should always have the **sample** **dimension** first. For example here, the input shape is **(4,1,2)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.443703\n",
      "epoch 2/1000   error=0.345896\n",
      "epoch 3/1000   error=0.333514\n",
      "epoch 4/1000   error=0.328750\n",
      "epoch 5/1000   error=0.325853\n",
      "epoch 6/1000   error=0.323809\n",
      "epoch 7/1000   error=0.322256\n",
      "epoch 8/1000   error=0.321005\n",
      "epoch 9/1000   error=0.319941\n",
      "epoch 10/1000   error=0.318988\n",
      "epoch 11/1000   error=0.318098\n",
      "epoch 12/1000   error=0.317237\n",
      "epoch 13/1000   error=0.316387\n",
      "epoch 14/1000   error=0.315535\n",
      "epoch 15/1000   error=0.314673\n",
      "epoch 16/1000   error=0.313796\n",
      "epoch 17/1000   error=0.312901\n",
      "epoch 18/1000   error=0.311988\n",
      "epoch 19/1000   error=0.311055\n",
      "epoch 20/1000   error=0.310102\n",
      "epoch 21/1000   error=0.309128\n",
      "epoch 22/1000   error=0.308132\n",
      "epoch 23/1000   error=0.307115\n",
      "epoch 24/1000   error=0.306075\n",
      "epoch 25/1000   error=0.305012\n",
      "epoch 26/1000   error=0.303924\n",
      "epoch 27/1000   error=0.302811\n",
      "epoch 28/1000   error=0.301672\n",
      "epoch 29/1000   error=0.300506\n",
      "epoch 30/1000   error=0.299311\n",
      "epoch 31/1000   error=0.298087\n",
      "epoch 32/1000   error=0.296832\n",
      "epoch 33/1000   error=0.295545\n",
      "epoch 34/1000   error=0.294226\n",
      "epoch 35/1000   error=0.292872\n",
      "epoch 36/1000   error=0.291484\n",
      "epoch 37/1000   error=0.290061\n",
      "epoch 38/1000   error=0.288602\n",
      "epoch 39/1000   error=0.287107\n",
      "epoch 40/1000   error=0.285575\n",
      "epoch 41/1000   error=0.284008\n",
      "epoch 42/1000   error=0.282404\n",
      "epoch 43/1000   error=0.280765\n",
      "epoch 44/1000   error=0.279093\n",
      "epoch 45/1000   error=0.277387\n",
      "epoch 46/1000   error=0.275651\n",
      "epoch 47/1000   error=0.273886\n",
      "epoch 48/1000   error=0.272094\n",
      "epoch 49/1000   error=0.270278\n",
      "epoch 50/1000   error=0.268442\n",
      "epoch 51/1000   error=0.266589\n",
      "epoch 52/1000   error=0.264722\n",
      "epoch 53/1000   error=0.262845\n",
      "epoch 54/1000   error=0.260963\n",
      "epoch 55/1000   error=0.259081\n",
      "epoch 56/1000   error=0.257201\n",
      "epoch 57/1000   error=0.255329\n",
      "epoch 58/1000   error=0.253468\n",
      "epoch 59/1000   error=0.251623\n",
      "epoch 60/1000   error=0.249797\n",
      "epoch 61/1000   error=0.247994\n",
      "epoch 62/1000   error=0.246217\n",
      "epoch 63/1000   error=0.244469\n",
      "epoch 64/1000   error=0.242751\n",
      "epoch 65/1000   error=0.241066\n",
      "epoch 66/1000   error=0.239415\n",
      "epoch 67/1000   error=0.237799\n",
      "epoch 68/1000   error=0.236218\n",
      "epoch 69/1000   error=0.234672\n",
      "epoch 70/1000   error=0.233161\n",
      "epoch 71/1000   error=0.231685\n",
      "epoch 72/1000   error=0.230241\n",
      "epoch 73/1000   error=0.228830\n",
      "epoch 74/1000   error=0.227449\n",
      "epoch 75/1000   error=0.226097\n",
      "epoch 76/1000   error=0.224771\n",
      "epoch 77/1000   error=0.223471\n",
      "epoch 78/1000   error=0.222192\n",
      "epoch 79/1000   error=0.220935\n",
      "epoch 80/1000   error=0.219695\n",
      "epoch 81/1000   error=0.218471\n",
      "epoch 82/1000   error=0.217260\n",
      "epoch 83/1000   error=0.216060\n",
      "epoch 84/1000   error=0.214869\n",
      "epoch 85/1000   error=0.213683\n",
      "epoch 86/1000   error=0.212500\n",
      "epoch 87/1000   error=0.211317\n",
      "epoch 88/1000   error=0.210132\n",
      "epoch 89/1000   error=0.208942\n",
      "epoch 90/1000   error=0.207745\n",
      "epoch 91/1000   error=0.206538\n",
      "epoch 92/1000   error=0.205317\n",
      "epoch 93/1000   error=0.204080\n",
      "epoch 94/1000   error=0.202825\n",
      "epoch 95/1000   error=0.201548\n",
      "epoch 96/1000   error=0.200246\n",
      "epoch 97/1000   error=0.198917\n",
      "epoch 98/1000   error=0.197557\n",
      "epoch 99/1000   error=0.196164\n",
      "epoch 100/1000   error=0.194733\n",
      "epoch 101/1000   error=0.193262\n",
      "epoch 102/1000   error=0.191746\n",
      "epoch 103/1000   error=0.190183\n",
      "epoch 104/1000   error=0.188569\n",
      "epoch 105/1000   error=0.186898\n",
      "epoch 106/1000   error=0.185167\n",
      "epoch 107/1000   error=0.183372\n",
      "epoch 108/1000   error=0.181507\n",
      "epoch 109/1000   error=0.179567\n",
      "epoch 110/1000   error=0.177546\n",
      "epoch 111/1000   error=0.175440\n",
      "epoch 112/1000   error=0.173242\n",
      "epoch 113/1000   error=0.170947\n",
      "epoch 114/1000   error=0.168549\n",
      "epoch 115/1000   error=0.166042\n",
      "epoch 116/1000   error=0.163422\n",
      "epoch 117/1000   error=0.160683\n",
      "epoch 118/1000   error=0.157822\n",
      "epoch 119/1000   error=0.154834\n",
      "epoch 120/1000   error=0.151717\n",
      "epoch 121/1000   error=0.148469\n",
      "epoch 122/1000   error=0.145088\n",
      "epoch 123/1000   error=0.141576\n",
      "epoch 124/1000   error=0.137934\n",
      "epoch 125/1000   error=0.134165\n",
      "epoch 126/1000   error=0.130274\n",
      "epoch 127/1000   error=0.126268\n",
      "epoch 128/1000   error=0.122156\n",
      "epoch 129/1000   error=0.117949\n",
      "epoch 130/1000   error=0.113660\n",
      "epoch 131/1000   error=0.109305\n",
      "epoch 132/1000   error=0.104901\n",
      "epoch 133/1000   error=0.100468\n",
      "epoch 134/1000   error=0.096028\n",
      "epoch 135/1000   error=0.091603\n",
      "epoch 136/1000   error=0.087216\n",
      "epoch 137/1000   error=0.082893\n",
      "epoch 138/1000   error=0.078655\n",
      "epoch 139/1000   error=0.074525\n",
      "epoch 140/1000   error=0.070523\n",
      "epoch 141/1000   error=0.066666\n",
      "epoch 142/1000   error=0.062968\n",
      "epoch 143/1000   error=0.059442\n",
      "epoch 144/1000   error=0.056093\n",
      "epoch 145/1000   error=0.052927\n",
      "epoch 146/1000   error=0.049945\n",
      "epoch 147/1000   error=0.047145\n",
      "epoch 148/1000   error=0.044523\n",
      "epoch 149/1000   error=0.042073\n",
      "epoch 150/1000   error=0.039789\n",
      "epoch 151/1000   error=0.037661\n",
      "epoch 152/1000   error=0.035681\n",
      "epoch 153/1000   error=0.033840\n",
      "epoch 154/1000   error=0.032128\n",
      "epoch 155/1000   error=0.030536\n",
      "epoch 156/1000   error=0.029056\n",
      "epoch 157/1000   error=0.027679\n",
      "epoch 158/1000   error=0.026398\n",
      "epoch 159/1000   error=0.025204\n",
      "epoch 160/1000   error=0.024090\n",
      "epoch 161/1000   error=0.023051\n",
      "epoch 162/1000   error=0.022081\n",
      "epoch 163/1000   error=0.021173\n",
      "epoch 164/1000   error=0.020324\n",
      "epoch 165/1000   error=0.019528\n",
      "epoch 166/1000   error=0.018781\n",
      "epoch 167/1000   error=0.018079\n",
      "epoch 168/1000   error=0.017419\n",
      "epoch 169/1000   error=0.016798\n",
      "epoch 170/1000   error=0.016213\n",
      "epoch 171/1000   error=0.015661\n",
      "epoch 172/1000   error=0.015140\n",
      "epoch 173/1000   error=0.014647\n",
      "epoch 174/1000   error=0.014181\n",
      "epoch 175/1000   error=0.013739\n",
      "epoch 176/1000   error=0.013320\n",
      "epoch 177/1000   error=0.012922\n",
      "epoch 178/1000   error=0.012544\n",
      "epoch 179/1000   error=0.012185\n",
      "epoch 180/1000   error=0.011843\n",
      "epoch 181/1000   error=0.011517\n",
      "epoch 182/1000   error=0.011206\n",
      "epoch 183/1000   error=0.010910\n",
      "epoch 184/1000   error=0.010626\n",
      "epoch 185/1000   error=0.010356\n",
      "epoch 186/1000   error=0.010097\n",
      "epoch 187/1000   error=0.009849\n",
      "epoch 188/1000   error=0.009612\n",
      "epoch 189/1000   error=0.009384\n",
      "epoch 190/1000   error=0.009166\n",
      "epoch 191/1000   error=0.008956\n",
      "epoch 192/1000   error=0.008755\n",
      "epoch 193/1000   error=0.008562\n",
      "epoch 194/1000   error=0.008376\n",
      "epoch 195/1000   error=0.008197\n",
      "epoch 196/1000   error=0.008024\n",
      "epoch 197/1000   error=0.007858\n",
      "epoch 198/1000   error=0.007698\n",
      "epoch 199/1000   error=0.007544\n",
      "epoch 200/1000   error=0.007395\n",
      "epoch 201/1000   error=0.007251\n",
      "epoch 202/1000   error=0.007112\n",
      "epoch 203/1000   error=0.006978\n",
      "epoch 204/1000   error=0.006848\n",
      "epoch 205/1000   error=0.006723\n",
      "epoch 206/1000   error=0.006601\n",
      "epoch 207/1000   error=0.006484\n",
      "epoch 208/1000   error=0.006370\n",
      "epoch 209/1000   error=0.006259\n",
      "epoch 210/1000   error=0.006152\n",
      "epoch 211/1000   error=0.006048\n",
      "epoch 212/1000   error=0.005948\n",
      "epoch 213/1000   error=0.005850\n",
      "epoch 214/1000   error=0.005755\n",
      "epoch 215/1000   error=0.005663\n",
      "epoch 216/1000   error=0.005574\n",
      "epoch 217/1000   error=0.005487\n",
      "epoch 218/1000   error=0.005402\n",
      "epoch 219/1000   error=0.005320\n",
      "epoch 220/1000   error=0.005240\n",
      "epoch 221/1000   error=0.005162\n",
      "epoch 222/1000   error=0.005086\n",
      "epoch 223/1000   error=0.005012\n",
      "epoch 224/1000   error=0.004940\n",
      "epoch 225/1000   error=0.004870\n",
      "epoch 226/1000   error=0.004802\n",
      "epoch 227/1000   error=0.004735\n",
      "epoch 228/1000   error=0.004670\n",
      "epoch 229/1000   error=0.004607\n",
      "epoch 230/1000   error=0.004545\n",
      "epoch 231/1000   error=0.004485\n",
      "epoch 232/1000   error=0.004426\n",
      "epoch 233/1000   error=0.004369\n",
      "epoch 234/1000   error=0.004312\n",
      "epoch 235/1000   error=0.004258\n",
      "epoch 236/1000   error=0.004204\n",
      "epoch 237/1000   error=0.004152\n",
      "epoch 238/1000   error=0.004100\n",
      "epoch 239/1000   error=0.004050\n",
      "epoch 240/1000   error=0.004001\n",
      "epoch 241/1000   error=0.003954\n",
      "epoch 242/1000   error=0.003907\n",
      "epoch 243/1000   error=0.003861\n",
      "epoch 244/1000   error=0.003816\n",
      "epoch 245/1000   error=0.003772\n",
      "epoch 246/1000   error=0.003729\n",
      "epoch 247/1000   error=0.003687\n",
      "epoch 248/1000   error=0.003645\n",
      "epoch 249/1000   error=0.003605\n",
      "epoch 250/1000   error=0.003565\n",
      "epoch 251/1000   error=0.003527\n",
      "epoch 252/1000   error=0.003488\n",
      "epoch 253/1000   error=0.003451\n",
      "epoch 254/1000   error=0.003415\n",
      "epoch 255/1000   error=0.003379\n",
      "epoch 256/1000   error=0.003343\n",
      "epoch 257/1000   error=0.003309\n",
      "epoch 258/1000   error=0.003275\n",
      "epoch 259/1000   error=0.003242\n",
      "epoch 260/1000   error=0.003209\n",
      "epoch 261/1000   error=0.003177\n",
      "epoch 262/1000   error=0.003145\n",
      "epoch 263/1000   error=0.003114\n",
      "epoch 264/1000   error=0.003084\n",
      "epoch 265/1000   error=0.003054\n",
      "epoch 266/1000   error=0.003025\n",
      "epoch 267/1000   error=0.002996\n",
      "epoch 268/1000   error=0.002968\n",
      "epoch 269/1000   error=0.002940\n",
      "epoch 270/1000   error=0.002913\n",
      "epoch 271/1000   error=0.002886\n",
      "epoch 272/1000   error=0.002859\n",
      "epoch 273/1000   error=0.002833\n",
      "epoch 274/1000   error=0.002808\n",
      "epoch 275/1000   error=0.002783\n",
      "epoch 276/1000   error=0.002758\n",
      "epoch 277/1000   error=0.002734\n",
      "epoch 278/1000   error=0.002710\n",
      "epoch 279/1000   error=0.002686\n",
      "epoch 280/1000   error=0.002663\n",
      "epoch 281/1000   error=0.002640\n",
      "epoch 282/1000   error=0.002618\n",
      "epoch 283/1000   error=0.002596\n",
      "epoch 284/1000   error=0.002574\n",
      "epoch 285/1000   error=0.002553\n",
      "epoch 286/1000   error=0.002531\n",
      "epoch 287/1000   error=0.002511\n",
      "epoch 288/1000   error=0.002490\n",
      "epoch 289/1000   error=0.002470\n",
      "epoch 290/1000   error=0.002450\n",
      "epoch 291/1000   error=0.002431\n",
      "epoch 292/1000   error=0.002411\n",
      "epoch 293/1000   error=0.002392\n",
      "epoch 294/1000   error=0.002374\n",
      "epoch 295/1000   error=0.002355\n",
      "epoch 296/1000   error=0.002337\n",
      "epoch 297/1000   error=0.002319\n",
      "epoch 298/1000   error=0.002301\n",
      "epoch 299/1000   error=0.002284\n",
      "epoch 300/1000   error=0.002267\n",
      "epoch 301/1000   error=0.002250\n",
      "epoch 302/1000   error=0.002233\n",
      "epoch 303/1000   error=0.002216\n",
      "epoch 304/1000   error=0.002200\n",
      "epoch 305/1000   error=0.002184\n",
      "epoch 306/1000   error=0.002168\n",
      "epoch 307/1000   error=0.002153\n",
      "epoch 308/1000   error=0.002137\n",
      "epoch 309/1000   error=0.002122\n",
      "epoch 310/1000   error=0.002107\n",
      "epoch 311/1000   error=0.002092\n",
      "epoch 312/1000   error=0.002077\n",
      "epoch 313/1000   error=0.002063\n",
      "epoch 314/1000   error=0.002049\n",
      "epoch 315/1000   error=0.002035\n",
      "epoch 316/1000   error=0.002021\n",
      "epoch 317/1000   error=0.002007\n",
      "epoch 318/1000   error=0.001993\n",
      "epoch 319/1000   error=0.001980\n",
      "epoch 320/1000   error=0.001967\n",
      "epoch 321/1000   error=0.001954\n",
      "epoch 322/1000   error=0.001941\n",
      "epoch 323/1000   error=0.001928\n",
      "epoch 324/1000   error=0.001916\n",
      "epoch 325/1000   error=0.001903\n",
      "epoch 326/1000   error=0.001891\n",
      "epoch 327/1000   error=0.001879\n",
      "epoch 328/1000   error=0.001867\n",
      "epoch 329/1000   error=0.001855\n",
      "epoch 330/1000   error=0.001843\n",
      "epoch 331/1000   error=0.001831\n",
      "epoch 332/1000   error=0.001820\n",
      "epoch 333/1000   error=0.001809\n",
      "epoch 334/1000   error=0.001798\n",
      "epoch 335/1000   error=0.001786\n",
      "epoch 336/1000   error=0.001776\n",
      "epoch 337/1000   error=0.001765\n",
      "epoch 338/1000   error=0.001754\n",
      "epoch 339/1000   error=0.001743\n",
      "epoch 340/1000   error=0.001733\n",
      "epoch 341/1000   error=0.001723\n",
      "epoch 342/1000   error=0.001712\n",
      "epoch 343/1000   error=0.001702\n",
      "epoch 344/1000   error=0.001692\n",
      "epoch 345/1000   error=0.001682\n",
      "epoch 346/1000   error=0.001673\n",
      "epoch 347/1000   error=0.001663\n",
      "epoch 348/1000   error=0.001653\n",
      "epoch 349/1000   error=0.001644\n",
      "epoch 350/1000   error=0.001634\n",
      "epoch 351/1000   error=0.001625\n",
      "epoch 352/1000   error=0.001616\n",
      "epoch 353/1000   error=0.001607\n",
      "epoch 354/1000   error=0.001598\n",
      "epoch 355/1000   error=0.001589\n",
      "epoch 356/1000   error=0.001580\n",
      "epoch 357/1000   error=0.001572\n",
      "epoch 358/1000   error=0.001563\n",
      "epoch 359/1000   error=0.001554\n",
      "epoch 360/1000   error=0.001546\n",
      "epoch 361/1000   error=0.001538\n",
      "epoch 362/1000   error=0.001529\n",
      "epoch 363/1000   error=0.001521\n",
      "epoch 364/1000   error=0.001513\n",
      "epoch 365/1000   error=0.001505\n",
      "epoch 366/1000   error=0.001497\n",
      "epoch 367/1000   error=0.001489\n",
      "epoch 368/1000   error=0.001481\n",
      "epoch 369/1000   error=0.001473\n",
      "epoch 370/1000   error=0.001466\n",
      "epoch 371/1000   error=0.001458\n",
      "epoch 372/1000   error=0.001451\n",
      "epoch 373/1000   error=0.001443\n",
      "epoch 374/1000   error=0.001436\n",
      "epoch 375/1000   error=0.001429\n",
      "epoch 376/1000   error=0.001421\n",
      "epoch 377/1000   error=0.001414\n",
      "epoch 378/1000   error=0.001407\n",
      "epoch 379/1000   error=0.001400\n",
      "epoch 380/1000   error=0.001393\n",
      "epoch 381/1000   error=0.001386\n",
      "epoch 382/1000   error=0.001379\n",
      "epoch 383/1000   error=0.001373\n",
      "epoch 384/1000   error=0.001366\n",
      "epoch 385/1000   error=0.001359\n",
      "epoch 386/1000   error=0.001353\n",
      "epoch 387/1000   error=0.001346\n",
      "epoch 388/1000   error=0.001340\n",
      "epoch 389/1000   error=0.001333\n",
      "epoch 390/1000   error=0.001327\n",
      "epoch 391/1000   error=0.001321\n",
      "epoch 392/1000   error=0.001314\n",
      "epoch 393/1000   error=0.001308\n",
      "epoch 394/1000   error=0.001302\n",
      "epoch 395/1000   error=0.001296\n",
      "epoch 396/1000   error=0.001290\n",
      "epoch 397/1000   error=0.001284\n",
      "epoch 398/1000   error=0.001278\n",
      "epoch 399/1000   error=0.001272\n",
      "epoch 400/1000   error=0.001266\n",
      "epoch 401/1000   error=0.001260\n",
      "epoch 402/1000   error=0.001255\n",
      "epoch 403/1000   error=0.001249\n",
      "epoch 404/1000   error=0.001243\n",
      "epoch 405/1000   error=0.001238\n",
      "epoch 406/1000   error=0.001232\n",
      "epoch 407/1000   error=0.001227\n",
      "epoch 408/1000   error=0.001221\n",
      "epoch 409/1000   error=0.001216\n",
      "epoch 410/1000   error=0.001211\n",
      "epoch 411/1000   error=0.001205\n",
      "epoch 412/1000   error=0.001200\n",
      "epoch 413/1000   error=0.001195\n",
      "epoch 414/1000   error=0.001189\n",
      "epoch 415/1000   error=0.001184\n",
      "epoch 416/1000   error=0.001179\n",
      "epoch 417/1000   error=0.001174\n",
      "epoch 418/1000   error=0.001169\n",
      "epoch 419/1000   error=0.001164\n",
      "epoch 420/1000   error=0.001159\n",
      "epoch 421/1000   error=0.001154\n",
      "epoch 422/1000   error=0.001149\n",
      "epoch 423/1000   error=0.001145\n",
      "epoch 424/1000   error=0.001140\n",
      "epoch 425/1000   error=0.001135\n",
      "epoch 426/1000   error=0.001130\n",
      "epoch 427/1000   error=0.001126\n",
      "epoch 428/1000   error=0.001121\n",
      "epoch 429/1000   error=0.001116\n",
      "epoch 430/1000   error=0.001112\n",
      "epoch 431/1000   error=0.001107\n",
      "epoch 432/1000   error=0.001103\n",
      "epoch 433/1000   error=0.001098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 434/1000   error=0.001094\n",
      "epoch 435/1000   error=0.001089\n",
      "epoch 436/1000   error=0.001085\n",
      "epoch 437/1000   error=0.001081\n",
      "epoch 438/1000   error=0.001076\n",
      "epoch 439/1000   error=0.001072\n",
      "epoch 440/1000   error=0.001068\n",
      "epoch 441/1000   error=0.001064\n",
      "epoch 442/1000   error=0.001060\n",
      "epoch 443/1000   error=0.001055\n",
      "epoch 444/1000   error=0.001051\n",
      "epoch 445/1000   error=0.001047\n",
      "epoch 446/1000   error=0.001043\n",
      "epoch 447/1000   error=0.001039\n",
      "epoch 448/1000   error=0.001035\n",
      "epoch 449/1000   error=0.001031\n",
      "epoch 450/1000   error=0.001027\n",
      "epoch 451/1000   error=0.001023\n",
      "epoch 452/1000   error=0.001019\n",
      "epoch 453/1000   error=0.001016\n",
      "epoch 454/1000   error=0.001012\n",
      "epoch 455/1000   error=0.001008\n",
      "epoch 456/1000   error=0.001004\n",
      "epoch 457/1000   error=0.001000\n",
      "epoch 458/1000   error=0.000997\n",
      "epoch 459/1000   error=0.000993\n",
      "epoch 460/1000   error=0.000989\n",
      "epoch 461/1000   error=0.000986\n",
      "epoch 462/1000   error=0.000982\n",
      "epoch 463/1000   error=0.000978\n",
      "epoch 464/1000   error=0.000975\n",
      "epoch 465/1000   error=0.000971\n",
      "epoch 466/1000   error=0.000968\n",
      "epoch 467/1000   error=0.000964\n",
      "epoch 468/1000   error=0.000961\n",
      "epoch 469/1000   error=0.000957\n",
      "epoch 470/1000   error=0.000954\n",
      "epoch 471/1000   error=0.000950\n",
      "epoch 472/1000   error=0.000947\n",
      "epoch 473/1000   error=0.000944\n",
      "epoch 474/1000   error=0.000940\n",
      "epoch 475/1000   error=0.000937\n",
      "epoch 476/1000   error=0.000934\n",
      "epoch 477/1000   error=0.000930\n",
      "epoch 478/1000   error=0.000927\n",
      "epoch 479/1000   error=0.000924\n",
      "epoch 480/1000   error=0.000921\n",
      "epoch 481/1000   error=0.000917\n",
      "epoch 482/1000   error=0.000914\n",
      "epoch 483/1000   error=0.000911\n",
      "epoch 484/1000   error=0.000908\n",
      "epoch 485/1000   error=0.000905\n",
      "epoch 486/1000   error=0.000902\n",
      "epoch 487/1000   error=0.000899\n",
      "epoch 488/1000   error=0.000896\n",
      "epoch 489/1000   error=0.000893\n",
      "epoch 490/1000   error=0.000890\n",
      "epoch 491/1000   error=0.000887\n",
      "epoch 492/1000   error=0.000884\n",
      "epoch 493/1000   error=0.000881\n",
      "epoch 494/1000   error=0.000878\n",
      "epoch 495/1000   error=0.000875\n",
      "epoch 496/1000   error=0.000872\n",
      "epoch 497/1000   error=0.000869\n",
      "epoch 498/1000   error=0.000866\n",
      "epoch 499/1000   error=0.000863\n",
      "epoch 500/1000   error=0.000861\n",
      "epoch 501/1000   error=0.000858\n",
      "epoch 502/1000   error=0.000855\n",
      "epoch 503/1000   error=0.000852\n",
      "epoch 504/1000   error=0.000849\n",
      "epoch 505/1000   error=0.000847\n",
      "epoch 506/1000   error=0.000844\n",
      "epoch 507/1000   error=0.000841\n",
      "epoch 508/1000   error=0.000838\n",
      "epoch 509/1000   error=0.000836\n",
      "epoch 510/1000   error=0.000833\n",
      "epoch 511/1000   error=0.000830\n",
      "epoch 512/1000   error=0.000828\n",
      "epoch 513/1000   error=0.000825\n",
      "epoch 514/1000   error=0.000823\n",
      "epoch 515/1000   error=0.000820\n",
      "epoch 516/1000   error=0.000817\n",
      "epoch 517/1000   error=0.000815\n",
      "epoch 518/1000   error=0.000812\n",
      "epoch 519/1000   error=0.000810\n",
      "epoch 520/1000   error=0.000807\n",
      "epoch 521/1000   error=0.000805\n",
      "epoch 522/1000   error=0.000802\n",
      "epoch 523/1000   error=0.000800\n",
      "epoch 524/1000   error=0.000797\n",
      "epoch 525/1000   error=0.000795\n",
      "epoch 526/1000   error=0.000793\n",
      "epoch 527/1000   error=0.000790\n",
      "epoch 528/1000   error=0.000788\n",
      "epoch 529/1000   error=0.000785\n",
      "epoch 530/1000   error=0.000783\n",
      "epoch 531/1000   error=0.000781\n",
      "epoch 532/1000   error=0.000778\n",
      "epoch 533/1000   error=0.000776\n",
      "epoch 534/1000   error=0.000774\n",
      "epoch 535/1000   error=0.000771\n",
      "epoch 536/1000   error=0.000769\n",
      "epoch 537/1000   error=0.000767\n",
      "epoch 538/1000   error=0.000764\n",
      "epoch 539/1000   error=0.000762\n",
      "epoch 540/1000   error=0.000760\n",
      "epoch 541/1000   error=0.000758\n",
      "epoch 542/1000   error=0.000755\n",
      "epoch 543/1000   error=0.000753\n",
      "epoch 544/1000   error=0.000751\n",
      "epoch 545/1000   error=0.000749\n",
      "epoch 546/1000   error=0.000747\n",
      "epoch 547/1000   error=0.000745\n",
      "epoch 548/1000   error=0.000742\n",
      "epoch 549/1000   error=0.000740\n",
      "epoch 550/1000   error=0.000738\n",
      "epoch 551/1000   error=0.000736\n",
      "epoch 552/1000   error=0.000734\n",
      "epoch 553/1000   error=0.000732\n",
      "epoch 554/1000   error=0.000730\n",
      "epoch 555/1000   error=0.000728\n",
      "epoch 556/1000   error=0.000726\n",
      "epoch 557/1000   error=0.000724\n",
      "epoch 558/1000   error=0.000722\n",
      "epoch 559/1000   error=0.000720\n",
      "epoch 560/1000   error=0.000718\n",
      "epoch 561/1000   error=0.000716\n",
      "epoch 562/1000   error=0.000714\n",
      "epoch 563/1000   error=0.000712\n",
      "epoch 564/1000   error=0.000710\n",
      "epoch 565/1000   error=0.000708\n",
      "epoch 566/1000   error=0.000706\n",
      "epoch 567/1000   error=0.000704\n",
      "epoch 568/1000   error=0.000702\n",
      "epoch 569/1000   error=0.000700\n",
      "epoch 570/1000   error=0.000698\n",
      "epoch 571/1000   error=0.000696\n",
      "epoch 572/1000   error=0.000694\n",
      "epoch 573/1000   error=0.000692\n",
      "epoch 574/1000   error=0.000690\n",
      "epoch 575/1000   error=0.000689\n",
      "epoch 576/1000   error=0.000687\n",
      "epoch 577/1000   error=0.000685\n",
      "epoch 578/1000   error=0.000683\n",
      "epoch 579/1000   error=0.000681\n",
      "epoch 580/1000   error=0.000679\n",
      "epoch 581/1000   error=0.000678\n",
      "epoch 582/1000   error=0.000676\n",
      "epoch 583/1000   error=0.000674\n",
      "epoch 584/1000   error=0.000672\n",
      "epoch 585/1000   error=0.000670\n",
      "epoch 586/1000   error=0.000669\n",
      "epoch 587/1000   error=0.000667\n",
      "epoch 588/1000   error=0.000665\n",
      "epoch 589/1000   error=0.000663\n",
      "epoch 590/1000   error=0.000662\n",
      "epoch 591/1000   error=0.000660\n",
      "epoch 592/1000   error=0.000658\n",
      "epoch 593/1000   error=0.000656\n",
      "epoch 594/1000   error=0.000655\n",
      "epoch 595/1000   error=0.000653\n",
      "epoch 596/1000   error=0.000651\n",
      "epoch 597/1000   error=0.000650\n",
      "epoch 598/1000   error=0.000648\n",
      "epoch 599/1000   error=0.000646\n",
      "epoch 600/1000   error=0.000645\n",
      "epoch 601/1000   error=0.000643\n",
      "epoch 602/1000   error=0.000642\n",
      "epoch 603/1000   error=0.000640\n",
      "epoch 604/1000   error=0.000638\n",
      "epoch 605/1000   error=0.000637\n",
      "epoch 606/1000   error=0.000635\n",
      "epoch 607/1000   error=0.000633\n",
      "epoch 608/1000   error=0.000632\n",
      "epoch 609/1000   error=0.000630\n",
      "epoch 610/1000   error=0.000629\n",
      "epoch 611/1000   error=0.000627\n",
      "epoch 612/1000   error=0.000626\n",
      "epoch 613/1000   error=0.000624\n",
      "epoch 614/1000   error=0.000622\n",
      "epoch 615/1000   error=0.000621\n",
      "epoch 616/1000   error=0.000619\n",
      "epoch 617/1000   error=0.000618\n",
      "epoch 618/1000   error=0.000616\n",
      "epoch 619/1000   error=0.000615\n",
      "epoch 620/1000   error=0.000613\n",
      "epoch 621/1000   error=0.000612\n",
      "epoch 622/1000   error=0.000610\n",
      "epoch 623/1000   error=0.000609\n",
      "epoch 624/1000   error=0.000607\n",
      "epoch 625/1000   error=0.000606\n",
      "epoch 626/1000   error=0.000605\n",
      "epoch 627/1000   error=0.000603\n",
      "epoch 628/1000   error=0.000602\n",
      "epoch 629/1000   error=0.000600\n",
      "epoch 630/1000   error=0.000599\n",
      "epoch 631/1000   error=0.000597\n",
      "epoch 632/1000   error=0.000596\n",
      "epoch 633/1000   error=0.000594\n",
      "epoch 634/1000   error=0.000593\n",
      "epoch 635/1000   error=0.000592\n",
      "epoch 636/1000   error=0.000590\n",
      "epoch 637/1000   error=0.000589\n",
      "epoch 638/1000   error=0.000587\n",
      "epoch 639/1000   error=0.000586\n",
      "epoch 640/1000   error=0.000585\n",
      "epoch 641/1000   error=0.000583\n",
      "epoch 642/1000   error=0.000582\n",
      "epoch 643/1000   error=0.000581\n",
      "epoch 644/1000   error=0.000579\n",
      "epoch 645/1000   error=0.000578\n",
      "epoch 646/1000   error=0.000577\n",
      "epoch 647/1000   error=0.000575\n",
      "epoch 648/1000   error=0.000574\n",
      "epoch 649/1000   error=0.000573\n",
      "epoch 650/1000   error=0.000571\n",
      "epoch 651/1000   error=0.000570\n",
      "epoch 652/1000   error=0.000569\n",
      "epoch 653/1000   error=0.000567\n",
      "epoch 654/1000   error=0.000566\n",
      "epoch 655/1000   error=0.000565\n",
      "epoch 656/1000   error=0.000564\n",
      "epoch 657/1000   error=0.000562\n",
      "epoch 658/1000   error=0.000561\n",
      "epoch 659/1000   error=0.000560\n",
      "epoch 660/1000   error=0.000559\n",
      "epoch 661/1000   error=0.000557\n",
      "epoch 662/1000   error=0.000556\n",
      "epoch 663/1000   error=0.000555\n",
      "epoch 664/1000   error=0.000554\n",
      "epoch 665/1000   error=0.000552\n",
      "epoch 666/1000   error=0.000551\n",
      "epoch 667/1000   error=0.000550\n",
      "epoch 668/1000   error=0.000549\n",
      "epoch 669/1000   error=0.000547\n",
      "epoch 670/1000   error=0.000546\n",
      "epoch 671/1000   error=0.000545\n",
      "epoch 672/1000   error=0.000544\n",
      "epoch 673/1000   error=0.000543\n",
      "epoch 674/1000   error=0.000541\n",
      "epoch 675/1000   error=0.000540\n",
      "epoch 676/1000   error=0.000539\n",
      "epoch 677/1000   error=0.000538\n",
      "epoch 678/1000   error=0.000537\n",
      "epoch 679/1000   error=0.000536\n",
      "epoch 680/1000   error=0.000534\n",
      "epoch 681/1000   error=0.000533\n",
      "epoch 682/1000   error=0.000532\n",
      "epoch 683/1000   error=0.000531\n",
      "epoch 684/1000   error=0.000530\n",
      "epoch 685/1000   error=0.000529\n",
      "epoch 686/1000   error=0.000528\n",
      "epoch 687/1000   error=0.000526\n",
      "epoch 688/1000   error=0.000525\n",
      "epoch 689/1000   error=0.000524\n",
      "epoch 690/1000   error=0.000523\n",
      "epoch 691/1000   error=0.000522\n",
      "epoch 692/1000   error=0.000521\n",
      "epoch 693/1000   error=0.000520\n",
      "epoch 694/1000   error=0.000519\n",
      "epoch 695/1000   error=0.000518\n",
      "epoch 696/1000   error=0.000516\n",
      "epoch 697/1000   error=0.000515\n",
      "epoch 698/1000   error=0.000514\n",
      "epoch 699/1000   error=0.000513\n",
      "epoch 700/1000   error=0.000512\n",
      "epoch 701/1000   error=0.000511\n",
      "epoch 702/1000   error=0.000510\n",
      "epoch 703/1000   error=0.000509\n",
      "epoch 704/1000   error=0.000508\n",
      "epoch 705/1000   error=0.000507\n",
      "epoch 706/1000   error=0.000506\n",
      "epoch 707/1000   error=0.000505\n",
      "epoch 708/1000   error=0.000504\n",
      "epoch 709/1000   error=0.000503\n",
      "epoch 710/1000   error=0.000502\n",
      "epoch 711/1000   error=0.000501\n",
      "epoch 712/1000   error=0.000500\n",
      "epoch 713/1000   error=0.000499\n",
      "epoch 714/1000   error=0.000498\n",
      "epoch 715/1000   error=0.000497\n",
      "epoch 716/1000   error=0.000496\n",
      "epoch 717/1000   error=0.000495\n",
      "epoch 718/1000   error=0.000494\n",
      "epoch 719/1000   error=0.000493\n",
      "epoch 720/1000   error=0.000492\n",
      "epoch 721/1000   error=0.000491\n",
      "epoch 722/1000   error=0.000490\n",
      "epoch 723/1000   error=0.000489\n",
      "epoch 724/1000   error=0.000488\n",
      "epoch 725/1000   error=0.000487\n",
      "epoch 726/1000   error=0.000486\n",
      "epoch 727/1000   error=0.000485\n",
      "epoch 728/1000   error=0.000484\n",
      "epoch 729/1000   error=0.000483\n",
      "epoch 730/1000   error=0.000482\n",
      "epoch 731/1000   error=0.000481\n",
      "epoch 732/1000   error=0.000480\n",
      "epoch 733/1000   error=0.000479\n",
      "epoch 734/1000   error=0.000478\n",
      "epoch 735/1000   error=0.000477\n",
      "epoch 736/1000   error=0.000476\n",
      "epoch 737/1000   error=0.000475\n",
      "epoch 738/1000   error=0.000474\n",
      "epoch 739/1000   error=0.000473\n",
      "epoch 740/1000   error=0.000473\n",
      "epoch 741/1000   error=0.000472\n",
      "epoch 742/1000   error=0.000471\n",
      "epoch 743/1000   error=0.000470\n",
      "epoch 744/1000   error=0.000469\n",
      "epoch 745/1000   error=0.000468\n",
      "epoch 746/1000   error=0.000467\n",
      "epoch 747/1000   error=0.000466\n",
      "epoch 748/1000   error=0.000465\n",
      "epoch 749/1000   error=0.000464\n",
      "epoch 750/1000   error=0.000464\n",
      "epoch 751/1000   error=0.000463\n",
      "epoch 752/1000   error=0.000462\n",
      "epoch 753/1000   error=0.000461\n",
      "epoch 754/1000   error=0.000460\n",
      "epoch 755/1000   error=0.000459\n",
      "epoch 756/1000   error=0.000458\n",
      "epoch 757/1000   error=0.000457\n",
      "epoch 758/1000   error=0.000457\n",
      "epoch 759/1000   error=0.000456\n",
      "epoch 760/1000   error=0.000455\n",
      "epoch 761/1000   error=0.000454\n",
      "epoch 762/1000   error=0.000453\n",
      "epoch 763/1000   error=0.000452\n",
      "epoch 764/1000   error=0.000451\n",
      "epoch 765/1000   error=0.000451\n",
      "epoch 766/1000   error=0.000450\n",
      "epoch 767/1000   error=0.000449\n",
      "epoch 768/1000   error=0.000448\n",
      "epoch 769/1000   error=0.000447\n",
      "epoch 770/1000   error=0.000446\n",
      "epoch 771/1000   error=0.000446\n",
      "epoch 772/1000   error=0.000445\n",
      "epoch 773/1000   error=0.000444\n",
      "epoch 774/1000   error=0.000443\n",
      "epoch 775/1000   error=0.000442\n",
      "epoch 776/1000   error=0.000442\n",
      "epoch 777/1000   error=0.000441\n",
      "epoch 778/1000   error=0.000440\n",
      "epoch 779/1000   error=0.000439\n",
      "epoch 780/1000   error=0.000438\n",
      "epoch 781/1000   error=0.000438\n",
      "epoch 782/1000   error=0.000437\n",
      "epoch 783/1000   error=0.000436\n",
      "epoch 784/1000   error=0.000435\n",
      "epoch 785/1000   error=0.000434\n",
      "epoch 786/1000   error=0.000434\n",
      "epoch 787/1000   error=0.000433\n",
      "epoch 788/1000   error=0.000432\n",
      "epoch 789/1000   error=0.000431\n",
      "epoch 790/1000   error=0.000431\n",
      "epoch 791/1000   error=0.000430\n",
      "epoch 792/1000   error=0.000429\n",
      "epoch 793/1000   error=0.000428\n",
      "epoch 794/1000   error=0.000427\n",
      "epoch 795/1000   error=0.000427\n",
      "epoch 796/1000   error=0.000426\n",
      "epoch 797/1000   error=0.000425\n",
      "epoch 798/1000   error=0.000424\n",
      "epoch 799/1000   error=0.000424\n",
      "epoch 800/1000   error=0.000423\n",
      "epoch 801/1000   error=0.000422\n",
      "epoch 802/1000   error=0.000421\n",
      "epoch 803/1000   error=0.000421\n",
      "epoch 804/1000   error=0.000420\n",
      "epoch 805/1000   error=0.000419\n",
      "epoch 806/1000   error=0.000419\n",
      "epoch 807/1000   error=0.000418\n",
      "epoch 808/1000   error=0.000417\n",
      "epoch 809/1000   error=0.000416\n",
      "epoch 810/1000   error=0.000416\n",
      "epoch 811/1000   error=0.000415\n",
      "epoch 812/1000   error=0.000414\n",
      "epoch 813/1000   error=0.000413\n",
      "epoch 814/1000   error=0.000413\n",
      "epoch 815/1000   error=0.000412\n",
      "epoch 816/1000   error=0.000411\n",
      "epoch 817/1000   error=0.000411\n",
      "epoch 818/1000   error=0.000410\n",
      "epoch 819/1000   error=0.000409\n",
      "epoch 820/1000   error=0.000409\n",
      "epoch 821/1000   error=0.000408\n",
      "epoch 822/1000   error=0.000407\n",
      "epoch 823/1000   error=0.000406\n",
      "epoch 824/1000   error=0.000406\n",
      "epoch 825/1000   error=0.000405\n",
      "epoch 826/1000   error=0.000404\n",
      "epoch 827/1000   error=0.000404\n",
      "epoch 828/1000   error=0.000403\n",
      "epoch 829/1000   error=0.000402\n",
      "epoch 830/1000   error=0.000402\n",
      "epoch 831/1000   error=0.000401\n",
      "epoch 832/1000   error=0.000400\n",
      "epoch 833/1000   error=0.000400\n",
      "epoch 834/1000   error=0.000399\n",
      "epoch 835/1000   error=0.000398\n",
      "epoch 836/1000   error=0.000398\n",
      "epoch 837/1000   error=0.000397\n",
      "epoch 838/1000   error=0.000396\n",
      "epoch 839/1000   error=0.000396\n",
      "epoch 840/1000   error=0.000395\n",
      "epoch 841/1000   error=0.000394\n",
      "epoch 842/1000   error=0.000394\n",
      "epoch 843/1000   error=0.000393\n",
      "epoch 844/1000   error=0.000392\n",
      "epoch 845/1000   error=0.000392\n",
      "epoch 846/1000   error=0.000391\n",
      "epoch 847/1000   error=0.000391\n",
      "epoch 848/1000   error=0.000390\n",
      "epoch 849/1000   error=0.000389\n",
      "epoch 850/1000   error=0.000388\n",
      "epoch 851/1000   error=0.000388\n",
      "epoch 852/1000   error=0.000387\n",
      "epoch 853/1000   error=0.000387\n",
      "epoch 854/1000   error=0.000386\n",
      "epoch 855/1000   error=0.000386\n",
      "epoch 856/1000   error=0.000385\n",
      "epoch 857/1000   error=0.000385\n",
      "epoch 858/1000   error=0.000383\n",
      "epoch 859/1000   error=0.000384\n",
      "epoch 860/1000   error=0.000382\n",
      "epoch 861/1000   error=0.000384\n",
      "epoch 862/1000   error=0.000382\n",
      "epoch 863/1000   error=0.000384\n",
      "epoch 864/1000   error=0.000381\n",
      "epoch 865/1000   error=0.000385\n",
      "epoch 866/1000   error=0.000382\n",
      "epoch 867/1000   error=0.000388\n",
      "epoch 868/1000   error=0.000385\n",
      "epoch 869/1000   error=0.000393\n",
      "epoch 870/1000   error=0.000391\n",
      "epoch 871/1000   error=0.000404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 872/1000   error=0.000403\n",
      "epoch 873/1000   error=0.000425\n",
      "epoch 874/1000   error=0.000428\n",
      "epoch 875/1000   error=0.000463\n",
      "epoch 876/1000   error=0.000474\n",
      "epoch 877/1000   error=0.000533\n",
      "epoch 878/1000   error=0.000563\n",
      "epoch 879/1000   error=0.000661\n",
      "epoch 880/1000   error=0.000727\n",
      "epoch 881/1000   error=0.000893\n",
      "epoch 882/1000   error=0.001028\n",
      "epoch 883/1000   error=0.001311\n",
      "epoch 884/1000   error=0.001569\n",
      "epoch 885/1000   error=0.002044\n",
      "epoch 886/1000   error=0.002511\n",
      "epoch 887/1000   error=0.003279\n",
      "epoch 888/1000   error=0.004057\n",
      "epoch 889/1000   error=0.005208\n",
      "epoch 890/1000   error=0.006357\n",
      "epoch 891/1000   error=0.007871\n",
      "epoch 892/1000   error=0.009294\n",
      "epoch 893/1000   error=0.010928\n",
      "epoch 894/1000   error=0.012311\n",
      "epoch 895/1000   error=0.013658\n",
      "epoch 896/1000   error=0.014652\n",
      "epoch 897/1000   error=0.015438\n",
      "epoch 898/1000   error=0.015911\n",
      "epoch 899/1000   error=0.016153\n",
      "epoch 900/1000   error=0.016202\n",
      "epoch 901/1000   error=0.016080\n",
      "epoch 902/1000   error=0.015866\n",
      "epoch 903/1000   error=0.015555\n",
      "epoch 904/1000   error=0.015204\n",
      "epoch 905/1000   error=0.014816\n",
      "epoch 906/1000   error=0.014402\n",
      "epoch 907/1000   error=0.013998\n",
      "epoch 908/1000   error=0.013560\n",
      "epoch 909/1000   error=0.013169\n",
      "epoch 910/1000   error=0.012728\n",
      "epoch 911/1000   error=0.012363\n",
      "epoch 912/1000   error=0.011930\n",
      "epoch 913/1000   error=0.011595\n",
      "epoch 914/1000   error=0.011175\n",
      "epoch 915/1000   error=0.010872\n",
      "epoch 916/1000   error=0.010467\n",
      "epoch 917/1000   error=0.010193\n",
      "epoch 918/1000   error=0.009805\n",
      "epoch 919/1000   error=0.009560\n",
      "epoch 920/1000   error=0.009188\n",
      "epoch 921/1000   error=0.008969\n",
      "epoch 922/1000   error=0.008614\n",
      "epoch 923/1000   error=0.008420\n",
      "epoch 924/1000   error=0.008082\n",
      "epoch 925/1000   error=0.007909\n",
      "epoch 926/1000   error=0.007587\n",
      "epoch 927/1000   error=0.007434\n",
      "epoch 928/1000   error=0.007127\n",
      "epoch 929/1000   error=0.006993\n",
      "epoch 930/1000   error=0.006701\n",
      "epoch 931/1000   error=0.006583\n",
      "epoch 932/1000   error=0.006305\n",
      "epoch 933/1000   error=0.006202\n",
      "epoch 934/1000   error=0.005938\n",
      "epoch 935/1000   error=0.005849\n",
      "epoch 936/1000   error=0.005597\n",
      "epoch 937/1000   error=0.005520\n",
      "epoch 938/1000   error=0.005280\n",
      "epoch 939/1000   error=0.005215\n",
      "epoch 940/1000   error=0.004987\n",
      "epoch 941/1000   error=0.004931\n",
      "epoch 942/1000   error=0.004714\n",
      "epoch 943/1000   error=0.004668\n",
      "epoch 944/1000   error=0.004461\n",
      "epoch 945/1000   error=0.004423\n",
      "epoch 946/1000   error=0.004226\n",
      "epoch 947/1000   error=0.004195\n",
      "epoch 948/1000   error=0.004007\n",
      "epoch 949/1000   error=0.003983\n",
      "epoch 950/1000   error=0.003804\n",
      "epoch 951/1000   error=0.003786\n",
      "epoch 952/1000   error=0.003615\n",
      "epoch 953/1000   error=0.003603\n",
      "epoch 954/1000   error=0.003440\n",
      "epoch 955/1000   error=0.003433\n",
      "epoch 956/1000   error=0.003277\n",
      "epoch 957/1000   error=0.003274\n",
      "epoch 958/1000   error=0.003125\n",
      "epoch 959/1000   error=0.003126\n",
      "epoch 960/1000   error=0.002984\n",
      "epoch 961/1000   error=0.002989\n",
      "epoch 962/1000   error=0.002853\n",
      "epoch 963/1000   error=0.002861\n",
      "epoch 964/1000   error=0.002730\n",
      "epoch 965/1000   error=0.002741\n",
      "epoch 966/1000   error=0.002617\n",
      "epoch 967/1000   error=0.002630\n",
      "epoch 968/1000   error=0.002511\n",
      "epoch 969/1000   error=0.002526\n",
      "epoch 970/1000   error=0.002412\n",
      "epoch 971/1000   error=0.002430\n",
      "epoch 972/1000   error=0.002320\n",
      "epoch 973/1000   error=0.002339\n",
      "epoch 974/1000   error=0.002234\n",
      "epoch 975/1000   error=0.002255\n",
      "epoch 976/1000   error=0.002154\n",
      "epoch 977/1000   error=0.002177\n",
      "epoch 978/1000   error=0.002079\n",
      "epoch 979/1000   error=0.002104\n",
      "epoch 980/1000   error=0.002010\n",
      "epoch 981/1000   error=0.002036\n",
      "epoch 982/1000   error=0.001945\n",
      "epoch 983/1000   error=0.001972\n",
      "epoch 984/1000   error=0.001884\n",
      "epoch 985/1000   error=0.001912\n",
      "epoch 986/1000   error=0.001828\n",
      "epoch 987/1000   error=0.001857\n",
      "epoch 988/1000   error=0.001775\n",
      "epoch 989/1000   error=0.001805\n",
      "epoch 990/1000   error=0.001726\n",
      "epoch 991/1000   error=0.001757\n",
      "epoch 992/1000   error=0.001680\n",
      "epoch 993/1000   error=0.001711\n",
      "epoch 994/1000   error=0.001638\n",
      "epoch 995/1000   error=0.001669\n",
      "epoch 996/1000   error=0.001598\n",
      "epoch 997/1000   error=0.001630\n",
      "epoch 998/1000   error=0.001561\n",
      "epoch 999/1000   error=0.001593\n",
      "epoch 1000/1000   error=0.001526\n",
      "[array([[0.03260926]]), array([[0.97613032]]), array([[0.97583729]]), array([[0.06747269]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# training data\n",
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "# network\n",
    "net = Network()\n",
    "net.add(FCLayer(2, 8))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(8, 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# test\n",
    "out = net.predict(x_train)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should produce a result close to the training set output such as:\n",
    "\n",
    "```\n",
    "  [array([[0.00069917]]), array([[0.97479752]]), array([[0.97443034]]), array([[-0.0002348]])]\n",
    "```\n",
    "\n",
    "If this works (which it really, really, should, great!\n",
    "We can now solve something more interesting, let’s solve [MNIST](https://en.wikipedia.org/wiki/MNIST_database) [LeCunn](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve MNIST\n",
    "\n",
    "We didn’t implemented the Convolutional Layer but this is not a problem. All we need to do is to reshape our data so that it \n",
    "can fit into a Fully Connected Layer.\n",
    "\n",
    "*MNIST Dataset consists of images of digits from 0 to 9, of shape 28x28x1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/35   error=0.230068\n",
      "epoch 2/35   error=0.104294\n",
      "epoch 3/35   error=0.082683\n",
      "epoch 4/35   error=0.069881\n",
      "epoch 5/35   error=0.061271\n",
      "epoch 6/35   error=0.054990\n",
      "epoch 7/35   error=0.049950\n",
      "epoch 8/35   error=0.045619\n",
      "epoch 9/35   error=0.041889\n",
      "epoch 10/35   error=0.038703\n",
      "epoch 11/35   error=0.035972\n",
      "epoch 12/35   error=0.033722\n",
      "epoch 13/35   error=0.031819\n",
      "epoch 14/35   error=0.030116\n",
      "epoch 15/35   error=0.028334\n",
      "epoch 16/35   error=0.026719\n",
      "epoch 17/35   error=0.025025\n",
      "epoch 18/35   error=0.023666\n",
      "epoch 19/35   error=0.022509\n",
      "epoch 20/35   error=0.021495\n",
      "epoch 21/35   error=0.020548\n",
      "epoch 22/35   error=0.019778\n",
      "epoch 23/35   error=0.018968\n",
      "epoch 24/35   error=0.018303\n",
      "epoch 25/35   error=0.017514\n",
      "epoch 26/35   error=0.016839\n",
      "epoch 27/35   error=0.016053\n",
      "epoch 28/35   error=0.015541\n",
      "epoch 29/35   error=0.014954\n",
      "epoch 30/35   error=0.014533\n",
      "epoch 31/35   error=0.013952\n",
      "epoch 32/35   error=0.013571\n",
      "epoch 33/35   error=0.013171\n",
      "epoch 34/35   error=0.012891\n",
      "epoch 35/35   error=0.012395\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(x_test[0:3])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- predicted values : \n",
      "[[ 0.016 -0.004 -0.050 -0.037  0.048 -0.073 -0.011  0.962  0.048 -0.187]]\n",
      "[[ 0.131 -0.124 -0.277  0.447  0.151  0.748  0.865  0.057 -0.051 -0.595]]\n",
      "[[-0.327  0.981 -0.079  0.044  0.000  0.106 -0.103 -0.062 -0.288 -0.048]]\n",
      "--- true values : \n",
      "[[ 0.000  0.000  0.000  0.000  0.000  0.000  0.000  1.000  0.000  0.000]]\n",
      "[[ 0.000  0.000  1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]]\n",
      "[[ 0.000  1.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000]]\n"
     ]
    }
   ],
   "source": [
    "print(\"--- predicted values : \")\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:6.3f}\".format(x)})\n",
    "for v in out: print(v)\n",
    "\n",
    "print(\"--- true values : \")\n",
    "for v in y_test[0:3]: print(f\"[{v}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in epoch 1, the `error=0.241182` was an error rate of 25%.  After 35 epocks this shoud be close to 1% (0.01).\n",
    "\n",
    "This is working perfectly! \n",
    "\n",
    "**Amazing `:)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apClhi66ISja"
   },
   "source": [
    "### End of notebook."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "binary_functions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
