{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mAP and IoU Evaluation Metrics\n",
    "\n",
    "## Evaluating the quality of computer vision object detection models\n",
    "\n",
    "- From [Understanding the mAP Evaluation Metric for Object Detection](https://medium.com/@timothycarlen/understanding-the-map-evaluation-metric-for-object-detection-a07fe6962cf3) by Timothy C Arlen [medium.com](https://medium.com/)\n",
    "\n",
    "Updated by [John Fogarty](https://github.com/jfogarty) for Python 3.6 and [Base2 MLI](https://github.com/base2solutions/mli) and [colab](https://colab.research.google.com) standalone evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’ve evaluated models in object detection or you’ve read papers in this area, you may have encountered the mean average precision or “mAP score” (for example [here](https://arxiv.org/abs/1512.03385) or [here](https://arxiv.org/abs/1703.06870) or [here](https://arxiv.org/abs/1506.01497)).\n",
    "\n",
    "It has become the accepted way to evaluate object detection competitions, such as for the [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/), [ImageNet](http://image-net.org/), and [COCO](http://cocodataset.org/#home) challenges. In this article, I will explain:\n",
    "\n",
    "- what the mean average precision (mAP) metric is,\n",
    "\n",
    "- why it is a useful metric in object detection,\n",
    "\n",
    "- how to calculate it with example data for a particular class of object.\n",
    "\n",
    "Additionally, I will provide some code (link at the end of the article) to compute this metric for use in your own projects or work if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Object Detectors\n",
    "\n",
    "In object detection, evaluation is non trivial, because there are two distinct tasks to measure:\n",
    "\n",
    "1. Determining whether an object exists in the image (classification)\n",
    "\n",
    "2. Determining the location of the object (localization, a regression task).\n",
    "\n",
    "Furthermore, in a typical data set there will be many classes and their distribution is non-uniform (for example there might be many more dogs than ice cream cones). So a simple accuracy-based metric will introduce biases. It is also important to assess the risk of misclassifications. Thus, there is the need to associate a “confidence score” or **model score** with each bounding box detected and to assess the model at various level of confidence.\n",
    "\n",
    "In order to address these needs, the Average Precision (AP) was introduced. To understand the AP, it is necessary to understand the precision and recall of a classifier. For a more comprehensive explanation of these terms, the [wikipedia article](https://en.wikipedia.org/wiki/Precision_and_recall) is a nice place to start.\n",
    "\n",
    "Briefly, in this context:\n",
    "\n",
    "> ***Precision measures the “false positive rate” or the ratio of true object detections to the total number of objects that the classifier predicted.***\n",
    "\n",
    "If you have a precision score of close to 1.0 then there is a high likelihood that whatever the classifier predicts as a positive detection is in fact a correct prediction. \n",
    "\n",
    "> ***Recall measures the “false negative rate” or the ratio of true object detections to the total number of objects in the data set.***\n",
    "\n",
    "If you have a recall score close to 1.0 then almost all objects that are in your dataset will be positively detected by the model. Finally, it is very important to note that the there is an inverse relationship between precision and recall and that *these metrics are dependent on the model score threshold that you set* (as well as of course, the quality of the model). For example, in [this image](https://github.com/tensorflow/models/tree/master/research/object_detection#tensorflow-object-detection-api) from the TensorFlow Object Detection API, if we set the model score threshold at 50 % for the “kite” object, we get 7 positive class detections, but if we set our model score threshold at 90 %, there are 4 positive class detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data to test the mAP calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "URL='https://gist.githubusercontent.com/tarlen5/008809c3decf19313de216b9208f3734/raw/7e6fea8908a3b0adb332530f8b52e398feef8582/ground_truth_boxes.json'\n",
    "# URl was shortened with https://git.io/ to the one below.\n",
    "URL='https://git.io/fjAbx'\n",
    "filename = 'ground_truth_boxes.json'\n",
    "TMPDATA  = './tmpData'\n",
    "if not os.path.exists(TMPDATA) : os.makedirs(TMPDATA)\n",
    "ground_truth_filepath = os.path.join(TMPDATA, filename)\n",
    "!curl $URL -o $ground_truth_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "URL='https://gist.githubusercontent.com/tarlen5/008809c3decf19313de216b9208f3734/raw/7e6fea8908a3b0adb332530f8b52e398feef8582/predicted_boxes.json'\n",
    "# URl was shortened with https://git.io/ to the one below.\n",
    "URL='https://git.io/fjAb7'\n",
    "filename = 'predicted_boxes.json'\n",
    "TMPDATA  = './tmpData'\n",
    "if not os.path.exists(TMPDATA) : os.makedirs(TMPDATA)\n",
    "predicted_filepath = os.path.join(TMPDATA, filename)\n",
    "!curl $URL -o $predicted_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tim Arlen's mAP calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "author: Timothy C. Arlen\n",
    "date: 28 Feb 2018\n",
    "\n",
    "Calculate Mean Average Precision (mAP) for a set of bounding boxes corresponding to specific\n",
    "image Ids. Usage:\n",
    "\n",
    "> python calculate_mean_ap.py\n",
    "\n",
    "Will display a plot of precision vs recall curves at 10 distinct IoU thresholds as well as output\n",
    "summary information regarding the average precision and mAP scores.\n",
    "\n",
    "NOTE: Requires the files `ground_truth_boxes.json` and `predicted_boxes.json` which can be\n",
    "downloaded fromt this gist.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('poster')\n",
    "\n",
    "COLORS = [\n",
    "    '#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c',\n",
    "    '#98df8a', '#d62728', '#ff9896', '#9467bd', '#c5b0d5',\n",
    "    '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#7f7f7f',\n",
    "    '#c7c7c7', '#bcbd22', '#dbdb8d', '#17becf', '#9edae5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou_individual(pred_box, gt_box):\n",
    "    \"\"\"Calculate IoU of single predicted and ground truth box\n",
    "\n",
    "    Args:\n",
    "        pred_box (list of floats): location of predicted object as\n",
    "            [xmin, ymin, xmax, ymax]\n",
    "        gt_box (list of floats): location of ground truth object as\n",
    "            [xmin, ymin, xmax, ymax]\n",
    "\n",
    "    Returns:\n",
    "        float: value of the IoU for the two boxes.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: if the box is obviously malformed\n",
    "    \"\"\"\n",
    "    x1_t, y1_t, x2_t, y2_t = gt_box\n",
    "    x1_p, y1_p, x2_p, y2_p = pred_box\n",
    "\n",
    "    if (x1_p > x2_p) or (y1_p > y2_p):\n",
    "        raise AssertionError(\n",
    "            \"Prediction box is malformed? pred box: {}\".format(pred_box))\n",
    "    if (x1_t > x2_t) or (y1_t > y2_t):\n",
    "        raise AssertionError(\n",
    "            \"Ground Truth box is malformed? true box: {}\".format(gt_box))\n",
    "\n",
    "    if (x2_t < x1_p or x2_p < x1_t or y2_t < y1_p or y2_p < y1_t):\n",
    "        return 0.0\n",
    "\n",
    "    far_x = np.min([x2_t, x2_p])\n",
    "    near_x = np.max([x1_t, x1_p])\n",
    "    far_y = np.min([y2_t, y2_p])\n",
    "    near_y = np.max([y1_t, y1_p])\n",
    "\n",
    "    inter_area = (far_x - near_x + 1) * (far_y - near_y + 1)\n",
    "    true_box_area = (x2_t - x1_t + 1) * (y2_t - y1_t + 1)\n",
    "    pred_box_area = (x2_p - x1_p + 1) * (y2_p - y1_p + 1)\n",
    "    iou = inter_area / (true_box_area + pred_box_area - inter_area)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_image_results(gt_boxes, pred_boxes, iou_thr):\n",
    "    \"\"\"Calculates number of true_pos, false_pos, false_neg from single batch of boxes.\n",
    "\n",
    "    Args:\n",
    "        gt_boxes (list of list of floats): list of locations of ground truth\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        pred_boxes (dict): dict of dicts of 'boxes' (formatted like `gt_boxes`)\n",
    "            and 'scores'\n",
    "        iou_thr (float): value of IoU to consider as threshold for a\n",
    "            true prediction.\n",
    "\n",
    "    Returns:\n",
    "        dict: true positives (int), false positives (int), false negatives (int)\n",
    "    \"\"\"\n",
    "\n",
    "    all_pred_indices = range(len(pred_boxes))\n",
    "    all_gt_indices = range(len(gt_boxes))\n",
    "    if len(all_pred_indices) == 0:\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = len(gt_boxes)\n",
    "        return {'true_pos': tp, 'false_pos': fp, 'false_neg': fn}\n",
    "    if len(all_gt_indices) == 0:\n",
    "        tp = 0\n",
    "        fp = len(pred_boxes)\n",
    "        fn = 0\n",
    "        return {'true_pos': tp, 'false_pos': fp, 'false_neg': fn}\n",
    "\n",
    "    gt_idx_thr = []\n",
    "    pred_idx_thr = []\n",
    "    ious = []\n",
    "    for ipb, pred_box in enumerate(pred_boxes):\n",
    "        for igb, gt_box in enumerate(gt_boxes):\n",
    "            iou = calc_iou_individual(pred_box, gt_box)\n",
    "            if iou > iou_thr:\n",
    "                gt_idx_thr.append(igb)\n",
    "                pred_idx_thr.append(ipb)\n",
    "                ious.append(iou)\n",
    "\n",
    "    args_desc = np.argsort(ious)[::-1]\n",
    "    if len(args_desc) == 0:\n",
    "        # No matches\n",
    "        tp = 0\n",
    "        fp = len(pred_boxes)\n",
    "        fn = len(gt_boxes)\n",
    "    else:\n",
    "        gt_match_idx = []\n",
    "        pred_match_idx = []\n",
    "        for idx in args_desc:\n",
    "            gt_idx = gt_idx_thr[idx]\n",
    "            pr_idx = pred_idx_thr[idx]\n",
    "            # If the boxes are unmatched, add them to matches\n",
    "            if (gt_idx not in gt_match_idx) and (pr_idx not in pred_match_idx):\n",
    "                gt_match_idx.append(gt_idx)\n",
    "                pred_match_idx.append(pr_idx)\n",
    "        tp = len(gt_match_idx)\n",
    "        fp = len(pred_boxes) - len(pred_match_idx)\n",
    "        fn = len(gt_boxes) - len(gt_match_idx)\n",
    "\n",
    "    return {'true_pos': tp, 'false_pos': fp, 'false_neg': fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision_recall(img_results):\n",
    "    \"\"\"Calculates precision and recall from the set of images\n",
    "\n",
    "    Args:\n",
    "        img_results (dict): dictionary formatted like:\n",
    "            {\n",
    "                'img_id1': {'true_pos': int, 'false_pos': int, 'false_neg': int},\n",
    "                'img_id2': ...\n",
    "                ...\n",
    "            }\n",
    "\n",
    "    Returns:\n",
    "        tuple: of floats of (precision, recall)\n",
    "    \"\"\"\n",
    "    true_pos = 0; false_pos = 0; false_neg = 0\n",
    "    for _, res in img_results.items():\n",
    "        true_pos += res['true_pos']\n",
    "        false_pos += res['false_pos']\n",
    "        false_neg += res['false_neg']\n",
    "\n",
    "    try:\n",
    "        precision = true_pos/(true_pos + false_pos)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0.0\n",
    "    try:\n",
    "        recall = true_pos/(true_pos + false_neg)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0.0\n",
    "\n",
    "    return (precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_scores_map(pred_boxes):\n",
    "    \"\"\"Creates a dictionary of from model_scores to image ids.\n",
    "\n",
    "    Args:\n",
    "        pred_boxes (dict): dict of dicts of 'boxes' and 'scores'\n",
    "\n",
    "    Returns:\n",
    "        dict: keys are model_scores and values are image ids (usually filenames)\n",
    "\n",
    "    \"\"\"\n",
    "    model_scores_map = {}\n",
    "    for img_id, val in pred_boxes.items():\n",
    "        for score in val['scores']:\n",
    "            if score not in model_scores_map.keys():\n",
    "                model_scores_map[score] = [img_id]\n",
    "            else:\n",
    "                model_scores_map[score].append(img_id)\n",
    "    return model_scores_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_precision_at_iou(gt_boxes, pred_boxes, iou_thr=0.5):\n",
    "    \"\"\"Calculates average precision at given IoU threshold.\n",
    "\n",
    "    Args:\n",
    "        gt_boxes (list of list of floats): list of locations of ground truth\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        pred_boxes (list of list of floats): list of locations of predicted\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        iou_thr (float): value of IoU to consider as threshold for a\n",
    "            true prediction.\n",
    "\n",
    "    Returns:\n",
    "        dict: avg precision as well as summary info about the PR curve\n",
    "\n",
    "        Keys:\n",
    "            'avg_prec' (float): average precision for this IoU threshold\n",
    "            'precisions' (list of floats): precision value for the given\n",
    "                model_threshold\n",
    "            'recall' (list of floats): recall value for given\n",
    "                model_threshold\n",
    "            'models_thrs' (list of floats): model threshold value that\n",
    "                precision and recall were computed for.\n",
    "    \"\"\"\n",
    "    model_scores_map = get_model_scores_map(pred_boxes)\n",
    "    sorted_model_scores = sorted(model_scores_map.keys())\n",
    "\n",
    "    # Sort the predicted boxes in descending order (lowest scoring boxes first):\n",
    "    for img_id in pred_boxes.keys():\n",
    "        arg_sort = np.argsort(pred_boxes[img_id]['scores'])\n",
    "        pred_boxes[img_id]['scores'] = np.array(pred_boxes[img_id]['scores'])[arg_sort].tolist()\n",
    "        pred_boxes[img_id]['boxes'] = np.array(pred_boxes[img_id]['boxes'])[arg_sort].tolist()\n",
    "\n",
    "    pred_boxes_pruned = deepcopy(pred_boxes)\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    model_thrs = []\n",
    "    img_results = {}\n",
    "    # Loop over model score thresholds and calculate precision, recall\n",
    "    for ithr, model_score_thr in enumerate(sorted_model_scores[:-1]):\n",
    "        # On first iteration, define img_results for the first time:\n",
    "        img_ids = gt_boxes.keys() if ithr == 0 else model_scores_map[model_score_thr]\n",
    "        for img_id in img_ids:\n",
    "            gt_boxes_img = gt_boxes[img_id]\n",
    "            box_scores = pred_boxes_pruned[img_id]['scores']\n",
    "            start_idx = 0\n",
    "            for score in box_scores:\n",
    "                if score <= model_score_thr:\n",
    "                    pred_boxes_pruned[img_id]\n",
    "                    start_idx += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Remove boxes, scores of lower than threshold scores:\n",
    "            pred_boxes_pruned[img_id]['scores'] = pred_boxes_pruned[img_id]['scores'][start_idx:]\n",
    "            pred_boxes_pruned[img_id]['boxes'] = pred_boxes_pruned[img_id]['boxes'][start_idx:]\n",
    "\n",
    "            # Recalculate image results for this image\n",
    "            img_results[img_id] = get_single_image_results(\n",
    "                gt_boxes_img, pred_boxes_pruned[img_id]['boxes'], iou_thr)\n",
    "\n",
    "        prec, rec = calc_precision_recall(img_results)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        model_thrs.append(model_score_thr)\n",
    "\n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    prec_at_rec = []\n",
    "    for recall_level in np.linspace(0.0, 1.0, 11):\n",
    "        try:\n",
    "            args = np.argwhere(recalls >= recall_level).flatten()\n",
    "            prec = max(precisions[args])\n",
    "        except ValueError:\n",
    "            prec = 0.0\n",
    "        prec_at_rec.append(prec)\n",
    "    avg_prec = np.mean(prec_at_rec)\n",
    "\n",
    "    return {\n",
    "        'avg_prec': avg_prec,\n",
    "        'precisions': precisions,\n",
    "        'recalls': recalls,\n",
    "        'model_thrs': model_thrs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(\n",
    "    precisions, recalls, category='Person', label=None, color=None, ax=None):\n",
    "    \"\"\"Simple plotting helper function\"\"\"\n",
    "\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(10,8))\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if color is None:\n",
    "        color = COLORS[0]\n",
    "    ax.scatter(recalls, precisions, label=label, s=20, color=color)\n",
    "    ax.set_xlabel('recall')\n",
    "    ax.set_ylabel('precision')\n",
    "    ax.set_title('Precision-Recall curve for {}'.format(category))\n",
    "    ax.set_xlim([0.0,1.3])\n",
    "    ax.set_ylim([0.0,1.2])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with open(ground_truth_filepath) as infile:\n",
    "        gt_boxes = json.load(infile)\n",
    "\n",
    "    with open(predicted_filepath) as infile:\n",
    "        pred_boxes = json.load(infile)\n",
    "\n",
    "    # Runs it for one IoU threshold\n",
    "    iou_thr = 0.7\n",
    "    start_time = time.time()\n",
    "    data = get_avg_precision_at_iou(gt_boxes, pred_boxes, iou_thr=iou_thr)\n",
    "    end_time = time.time()\n",
    "    print('Single IoU calculation took {:.4f} secs'.format(end_time - start_time))\n",
    "    print('avg precision: {:.4f}'.format(data['avg_prec']))\n",
    "\n",
    "    start_time = time.time()\n",
    "    ax = None\n",
    "    avg_precs = []\n",
    "    iou_thrs = []\n",
    "    for idx, iou_thr in enumerate(np.linspace(0.5, 0.95, 10)):\n",
    "        data = get_avg_precision_at_iou(gt_boxes, pred_boxes, iou_thr=iou_thr)\n",
    "        avg_precs.append(data['avg_prec'])\n",
    "        iou_thrs.append(iou_thr)\n",
    "\n",
    "        precisions = data['precisions']\n",
    "        recalls = data['recalls']\n",
    "        ax = plot_pr_curve(\n",
    "            precisions, recalls, label='{:.2f}'.format(iou_thr), color=COLORS[idx*2], ax=ax)\n",
    "\n",
    "    # prettify for printing:\n",
    "    avg_precs = [float('{:.4f}'.format(ap)) for ap in avg_precs]\n",
    "    iou_thrs = [float('{:.4f}'.format(thr)) for thr in iou_thrs]\n",
    "    print('map: {:.2f}'.format(100*np.mean(avg_precs)))\n",
    "    print('avg precs: ', avg_precs)\n",
    "    print('iou_thrs:  ', iou_thrs)\n",
    "    plt.legend(loc='upper right', title='IOU Thr', frameon=True)\n",
    "    for xval in np.linspace(0.0, 1.0, 11):\n",
    "        plt.vlines(xval, 0.0, 1.1, color='gray', alpha=0.3, linestyles='dashed')\n",
    "    end_time = time.time()\n",
    "    print('\\nPlotting and calculating mAP takes {:.4f} secs'.format(end_time - start_time))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
