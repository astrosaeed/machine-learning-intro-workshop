# Intro to ML General Flow

## Jupyter, Numpy, Pandas

- 1.1 Python Numpy Tutorial: this is the bare minimum to get you started with Python and the Numpy arrays.
- 1.2 Numpy Vectors, Arrays, and Matrices: More specific information on the data structures used in ML

- 1.3 Intro to Pandas: the required library for data analysis and modeling : Pandas
- 1.4 More Pandas: more useful pandas snippets applied to the Titanic dataset

## Look at some ML Data

- 2.1 Exploring/visualizing Data - 1: The classic Iris dataset visualized with seaborn and pandas
- Chris Albon's Notes : The unreasonably useful notes of Chris Albon (host of the Partially Derivative podcast)
- 2.3 Exploring/visualizing Data - 3: The Darwin's Finches dataset with some nice EDA - Exploratory Data Analysis
- Python Machine Learning - 2nd Edition : Another good ML book created with Github notebooks

## Look at some ML Models

- 3.1 Gradient Descent: The basics of gradient descent
- 3.2 Least squares : The simplest fitting method : Linear regression
- 3.4 ML Workflow : A full end to end project on the diabetes dataset; workflows: Google Amazon Azure Watson

## Beyond basic 'accuracy'

- 6.1 Confusion Matrix : Using confusion matrices to evaluate classifiers : Confusion_Matrix

## Intro to NN models

- 5.1 Bias and Weights: Fundamental concepts: bias, and weights
- 4.1 Training a Model 1: Exploring MNIST

## Detail on a Conventional Model

- Scikit-learn : Excellent documentation and notebooks
- 3.5 Decision Trees : An introduction to decision trees : Decision Tree Learning

## Back to NN models

- 5.2 Activation Functions: Fundamental concepts: activation functions
- 4.3 Binary functions: computing the simplest binary functions (not, xor, or, and) with neural networks.

- 5.4 Math of Neural Networks: general NN designed using just Python and Numpy.
- 4.2 Training a Model 2: Exploring FashionMNIST

## More Metrics

- 6.2 Precision Recall Curves : Precision-Recall to evaluate classifiers : Precision-Recall
- 6.3 Receiver Operating Characteristics : ROC and AUC (Area Under the Curve) to evaluate classifiers : AUC_ROC

- 6.4 Minimizing Loss and Validating Models: Making better models through hyperparameter tuning

- 3.3 Beyond Gradient Descent: Adaptive techniques that improve on ordinary gradient descent

## Advanced Visualization

- 2.4 Visualization of MLP weights on MNIST : Looking at the weight matrices; reading the tea leaves
- 2.5 Visualizing MNIST: An Exploration of Dimensionality Reduction : Another classic Colahl)) more analysis with t-SNE visualization

- 5.3 Exploding and Vanishing Gradients: Gradient problems; Fundamental concepts: gradients

## A bit of Advanced Unsupervised Learning

- 5.5 Training an AutoEncoder : Using a NN for Credit Fraud detection based on an AutoEncoder

# End.