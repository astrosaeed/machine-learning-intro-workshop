{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "master.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfogarty/machine-learning-intro-workshop/blob/master/syllabus-sep-2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaIRWSzHYlOK"
      },
      "source": [
        "# Base2 Solutions Machine Learning Workshop\n",
        "\n",
        "## September 6th and 13th\n",
        "\n",
        "The workshop is two Friday afternoon/evenings over a two week period.  Please bring your own laptop, with power cord.  You should make sure your Google account is working and that you can access [Google's Colab](https://colab.research.google.com) before you get to class.\n",
        "\n",
        "This hands on workshop will be at Base2 in Bellevue, although the Zoom sessions will be live-streamed and the contents will be available afterwards for those who couldn't attend one or both sessions.\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MQuYZTAjlqjR"
      },
      "source": [
        "\n",
        "# Session 1 - Friday September 6th\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbZAoyYVvN89",
        "colab_type": "text"
      },
      "source": [
        "- Learning to manipulate and represent data\n",
        "  - numpy arrays\n",
        "  - using pandas and other tools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVBJ3wSWva4p",
        "colab_type": "text"
      },
      "source": [
        "-\tImporting and Running a model\n",
        "  - Linear Regression (Or maybe something with MNIST?)\n",
        "  - High level overview of basic syntax and concepts\n",
        "  - Splitting data into training and test\n",
        "  - Training the model\n",
        "  - Test/Validate the model\n",
        "  - Graph the model w/data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AwF4F_mvrGj",
        "colab_type": "text"
      },
      "source": [
        "- Learning more about data\n",
        "  - Intro: Linear, Logistic Regression and Perceptrons\n",
        "  - Randomize Data\n",
        "  - Normalizing Data\n",
        "    - Scaling Feature values\n",
        "       - We want to prevent some values having more weight unnecessarily because it is recorded on a much greater scale (i.e. 0 to 500000)\n",
        "       - Scaling our values all to a similar range (-1 to +1 or -3 to +3 etc)\n",
        "       - We can use the Z score of the value\n",
        "         - scaledvalue = (value - mean) / stddev\n",
        "         - Usually ends up between -3 and +3\n",
        "\n",
        "       - Handling Extreme Outliers\n",
        "         - Can take the log of the values, better but still leaves a tail\n",
        "         - We can also clip feature values at a number and any number higher or lower just become the value decided for clipping (if we clip at 4, any values >4 become 4)\n",
        "       - Mapping Categorical Values\n",
        "\n",
        "       - We can map a list of string names to a map of values, which will allow us to multiply the weights appropriately\n",
        "       - This mapping often is not the right solution however, we often don't want our categories directly linked, importance of street names shouldn't have the same weight across the board as there is usually no direct linkage between any two streets and home prices. Homes on park place may all be expensive, but a street like baltic ave may have a completely different affect on price.\n",
        "       - It is better instead to make each category it's own binary feature. All houses where isOnParkPlace=True should have a higher weight for house price, but houses on Baltic Avenue should not be modified by the Park Place weights, instead it will be weighted based off of it's own feature, isOnBalticAvenue\n",
        "       - Separating our data out like this also allows us to have homes in multiple categories (I.E Corner of a cross street)\n",
        "\n",
        "       - Binning\n",
        "       - Add values into a bin, and make each bin a boolean value\n",
        "       - Things like latitude and longitude don't make sense as floating point values as there is no good linear relationship (higher the latitude != larger house price)\n",
        "       - Group the latitude into bins (latitude 31-33 may correspond to a higher latitude, especially when crossed with longitude 52-54) (more on feature crosses later?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCQ_VFD8vrJS",
        "colab_type": "text"
      },
      "source": [
        "- Cleaning Data\n",
        "  - Search for duplicates, missing values, outliers, compare training/validation to confirm they are different/similar enough\n",
        "  - \tShould appear with non-zero values more than a small handful of times in the dataset\n",
        "  - Features should have a clear obvious meaning\n",
        "  - Features shouldn't have magic values (-1 denoting special values/undefined, instead add a separate boolean to show undefined)\n",
        "  - Definition of a feature shouldn't change over time\n",
        "  - Should not have outlier values\n",
        "  - Unique identifiers are bad feature set because they appear only once, and add no comparable information for the model\n",
        "  - Scrubbing\n",
        "    - We need to scrub our data for inconsistencies and errors\n",
        "    - Omitted Values: Someone forgot to enter a field\n",
        "    - Duplicate examples: we received a value twice\n",
        "    - Bad labels: A person mislabeled a picture of a dog as a cat\n",
        "    - Bad feature values: Someone typed in an extra digit accidentally\n",
        "  -\tHistograms can help us realize when things are wrong\n",
        "    - Maximum and minimum\n",
        "    - Mean and median\n",
        "    - Standard deviation\n",
        "\n",
        "  - Feature Crosses/Synthetic Features\n",
        "    - Using feature crosses to get even better features\n",
        "    - Cross multiple features to link the features together\n",
        "      - \\# of bedrooms in one city could mean something different than # of bedrooms in another city, so if we cross them we have that data linked\n",
        "      - Be careful with cross products, some crosses can cause unnecessary complexity to the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg22yy6ox8xd",
        "colab_type": "text"
      },
      "source": [
        "-\tExercise: Look at the dataset histogram and see if we find any data that needs cleaning/scrubbing (use a dataset with omitted or outlier values)\n",
        "- Exercise: Take a feature represented by a floating point and bin them, creating a new feature\n",
        "- Exercise: Play with normalization functions and look at the data output\n",
        "- Exercise/Discussion: What would a good feature cross look like over our sample dataset? Implement it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EszMccC8tiRD",
        "colab_type": "text"
      },
      "source": [
        "- Importing and training our model (logistic regression? Maybe classification?)\n",
        "  - Import model\n",
        "- Hyperparameters\n",
        "  - Step Size\n",
        "- Gradient Descent\n",
        "  - Backpropagation\n",
        "  - How weights change\n",
        "- Batch size\n",
        "  -\tHow much data do we train on?\n",
        "    - Why don't we train on the entire set?\n",
        "  - \\# of epochs\n",
        "  - How long do we train?\n",
        "  - Mention of early stopping/regularization\n",
        "- Understanding of overfitting\n",
        "  - How do we optimize our model to limit loss and complexity (regularization)\n",
        "  -\tRegularization is a way to measure (and then penalize) model complexity\n",
        "  - Need to be careful about overfitting, do not overtrain to your training data\n",
        "  - Either stop early (not the best but often used) orâ€¦\n",
        "  - Penalize model complexity\n",
        "    - Avoid complexity where possible, structural risk minimization\n",
        "    - Try to prefer smaller weights on our model\n",
        "  -\tL2 Regularization\n",
        "    -\tPenalize the sum of the squared values of the weights\n",
        "  -\tModel complexity can be seen as the following\n",
        "  - Function of the weights of all the features of the model\n",
        "  - Function of the total number of features with nonzero weights\n",
        "-\tDefines the regularization term as the sum of the squares of all the feature weights\n",
        "    - Encourages weight values towards 0 but not exactly 0\n",
        "    - Encourages the mean of the weights toward 0 with a normal distribution\n",
        "    - Lambda (Regularization Rate)\n",
        "    - Gives model developers the ability to tune the overall impact of the regularization term by multiplying its value by a scalar known as lambda\n",
        "  - Increasing lambda strengthens the regularization effect\n",
        "- Touch on other types of regularization as well?\n",
        " \n",
        " \n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcBiO7E9y_Ff",
        "colab_type": "text"
      },
      "source": [
        "-\tLoss\n",
        "  - Loss functions provide more than just a static representation of how your model is performingâ€“theyâ€™re how your algorithms fit data in the first place. Most machine learning algorithms use some sort of loss function in the process of optimization, or finding the best parameters (weights) for your data.\n",
        "  - Penalty between a models predictions and the truth\n",
        "  - Helps us understand how well a model fits our data\n",
        "  - Explanation on different ways loss is measured and what they mean\n",
        "  - Key to training our model predictions\n",
        "  - Mean Squared Error (MSE): is the workhorse of basic loss functions: itâ€™s easy to understand and implement and generally works pretty well. To calculate MSE, you take the difference between your predictions and the ground truth, square it, and average it out across the whole dataset.\n",
        "  - The likelihood function: is also relatively simple, and is commonly used in classification problems. The function takes the predicted probability for each input example and multiplies them.\n",
        "  - For example, consider a model that outputs probabilities of $[0.4, 0.6, 0.9, 0.1]$ for the ground truth labels of $[0, 1, 1, 0]$.\n",
        "    - The likelihood loss would be computed as $(0.6) * (0.6) * (0.9) * (0.9) = 0.2916$.\n",
        "    - Since the model outputs probabilities for TRUE (or 1) only, when the ground truth label is 0 we take (1-p) as the probability. In other words, we multiply the modelâ€™s outputted probabilities together for the actual outcomes.\n",
        "  - LogLoss:\n",
        "    - (equation)\t \n",
        "  - $(x,y)$ exists in $D$ is the data set containing many labeled examples\n",
        "  - $y$ is the label in the labeled example, must be 0 or 1\n",
        "  - $y'$ is the predicted value (somewhere between 0 and 1) given the set of features in $x$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_25tnhhyHR5",
        "colab_type": "text"
      },
      "source": [
        "- Optimizers\n",
        "  - Talk on different optimizers, how to choose them.\n",
        "    - Optimization functions usually calculate the gradient i.e. the partial derivative of loss function with respect to weights, and the weights are modified in the opposite direction of the calculated gradient. This cycle is repeated until we reach the minima of loss function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fs_nqTft8XP",
        "colab_type": "text"
      },
      "source": [
        "- Logging/illustrating the training process\n",
        "- Understanding of precision/recall\n",
        "- Understanding of ROC Curve and AUC\n",
        "- Explain the cycle of training->testing->training->testing\n",
        "- Exercise: Adjust hyperparameters to try and get the lowest loss\n",
        "- Exercise: Use a different loss function and see how things change\n",
        "  - Does training take longer? What happens to the weights with different loss functions?\n",
        "- Exercise: Use different optimizers?\n",
        "- Other topics to cover (need to find a good place for these)\n",
        "- Activation functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfekix6Bt8dR",
        "colab_type": "text"
      },
      "source": [
        "# Session 2 - Friday September 13th"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_MAHSc1t8hw",
        "colab_type": "text"
      },
      "source": [
        "- CNN? Or another Deep Learning/NN example.\n",
        "- RNNs\n",
        "- GANS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwibT8XMq797",
        "colab_type": "text"
      },
      "source": [
        "# Other Workshop Resources\n",
        "\n",
        "- [The Workshop Github Repository](https://github.com/jfogarty/machine-learning-intro-workshop)\n",
        "\n",
        "- [Python Machine Learning - 2nd Edition](https://github.com/rasbt/python-machine-learning-book-2nd-edition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYkyBvLktla3",
        "colab_type": "text"
      },
      "source": [
        "### End of notes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFtztwkUtnBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}